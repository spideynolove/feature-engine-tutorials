Directory structure:
└── encoding/
    ├── CountFrequencyEncoder.py
    ├── DecisionTreeEncoder.py
    ├── MeanEncoder.py
    ├── OneHotEncoder.py
    ├── OrdinalEncoder.py
    ├── PRatioEncoder.py
    ├── RareLabelEncoder.py
    ├── StringSimilarityEncoder.py
    └── WoEEncoder.py

================================================
File: CountFrequencyEncoder.py
================================================
"""
# CountFrequencyEncoder
<p>The CountFrequencyEncoder() replaces categories by the count of
observations per category or by the percentage of observations per category.<br>
For example in the variable colour, if 10 observations are blue, blue will
be replaced by 10. Alternatively, if 10% of the observations are blue, blue
will be replaced by 0.1.</p>
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from feature_engine.encoding import CountFrequencyEncoder


def load_titanic(filepath='titanic.csv'):
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    return data


data = load_titanic('../data/titanic-2/Titanic-Dataset.csv')
data.head()
X = data.drop(['survived', 'name', 'ticket'], axis=1)
y = data.survived
X[['cabin', 'pclass', 'embarked']].isnull().sum()
""" Make sure that the variables are type (object).
if not, cast it as object , otherwise the transformer will either send an error (if we pass it as argument) 
or not pick it up (if we leave variables=None). """
X[['cabin', 'pclass', 'embarked']].dtypes
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
    random_state=0)
X_train.shape, X_test.shape
"""
Parameters
----------

encoding_method : str, default='count' 
                Desired method of encoding.

        'count': number of observations per category
        
        'frequency': percentage of observations per category

variables : list
          The list of categorical variables that will be encoded. If None, the 
          encoder will find and transform all object type variables.
"""
count_encoder = CountFrequencyEncoder(encoding_method='frequency',
    variables=['cabin', 'pclass', 'embarked'])
count_encoder.fit(X_train)
count_encoder.encoder_dict_
train_t = count_encoder.transform(X_train)
test_t = count_encoder.transform(X_test)
test_t.head()
test_t['pclass'].value_counts().plot.bar()
plt.show()
test_orig = count_encoder.inverse_transform(test_t)
test_orig.head()
count_enc = CountFrequencyEncoder(encoding_method='count', variables='cabin')
count_enc.fit(X_train)
count_enc.encoder_dict_
train_t = count_enc.transform(X_train)
test_t = count_enc.transform(X_test)
test_t.head()
count_enc = CountFrequencyEncoder(encoding_method='count')
count_enc.fit(X_train)
count_enc.variables
train_t = count_enc.transform(X_train)
test_t = count_enc.transform(X_test)
test_t.head()



================================================
File: DecisionTreeEncoder.py
================================================
"""
# DecisionTreeEncoder

The DecisionTreeEncoder() encodes categorical variables with predictions of a decision tree model.
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from feature_engine.encoding import DecisionTreeEncoder


def load_titanic(filepath='titanic.csv'):
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    return data


data = load_titanic('../data/titanic-2/Titanic-Dataset.csv')
data.head()
X = data.drop(['survived', 'name', 'ticket'], axis=1)
y = data.survived
X[['cabin', 'pclass', 'embarked']].isnull().sum()
""" Make sure that the variables are type (object).
if not, cast it as object , otherwise the transformer will either send an error (if we pass it as argument) 
or not pick it up (if we leave variables=None). """
X[['cabin', 'pclass', 'embarked']].dtypes
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
    random_state=0)
X_train.shape, X_test.shape
"""
Parameters
    ----------

    encoding_method: str, default='arbitrary'
        The categorical encoding method that will be used to encode the original
        categories to numerical values.

        'ordered': the categories are numbered in ascending order according to
        the target mean value per category.

        'arbitrary' : categories are numbered arbitrarily.

    cv : int, default=3
        Desired number of cross-validation fold to be used to fit the decision
        tree.

    scoring: str, default='neg_mean_squared_error'
        Desired metric to optimise the performance for the tree. Comes from
        sklearn metrics. See the DecisionTreeRegressor or DecisionTreeClassifier
        model evaluation documentation for more options:
        https://scikit-learn.org/stable/modules/model_evaluation.html

    regression : boolean, default=True
        Indicates whether the encoder should train a regression or a classification
        decision tree.

    param_grid : dictionary, default=None
        The list of parameters over which the decision tree should be optimised
        during the grid search. The param_grid can contain any of the permitted
        parameters for Scikit-learn's DecisionTreeRegressor() or
        DecisionTreeClassifier().

        If None, then param_grid = {'max_depth': [1, 2, 3, 4]}.

    random_state : int, default=None
        The random_state to initialise the training of the decision tree. It is one
        of the parameters of the Scikit-learn's DecisionTreeRegressor() or
        DecisionTreeClassifier(). For reproducibility it is recommended to set
        the random_state to an integer.

    variables : list, default=None
        The list of categorical variables that will be encoded. If None, the
        encoder will find and select all object type variables.
"""
tree_enc = DecisionTreeEncoder(encoding_method='arbitrary', cv=3, scoring=
    'roc_auc', param_grid={'max_depth': [1, 2, 3, 4]}, regression=False,
    variables=['cabin', 'pclass', 'embarked'])
tree_enc.fit(X_train, y_train)
tree_enc.encoder_
train_t = tree_enc.transform(X_train)
test_t = tree_enc.transform(X_test)
test_t.sample(5)
tree_enc = DecisionTreeEncoder(encoding_method='arbitrary', cv=3, scoring=
    'roc_auc', param_grid={'max_depth': [1, 2, 3, 4]}, regression=False)
tree_enc.fit(X_train, y_train)
tree_enc.encoder_
train_t = tree_enc.transform(X_train)
test_t = tree_enc.transform(X_test)
test_t.sample(5)



================================================
File: MeanEncoder.py
================================================
"""
# MeanEncoder

The MeanEncoder() replaces the labels of the variables by the mean value of the target for that label. <br>For example, in the variable colour, if the mean value of the binary target is 0.5 for the label blue, then blue is replaced by 0.5
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from feature_engine.encoding import MeanEncoder


def load_titanic(filepath='titanic.csv'):
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    return data


data = load_titanic('../data/titanic-2/Titanic-Dataset.csv')
data.head()
X = data.drop(['survived', 'name', 'ticket'], axis=1)
y = data.survived
X[['cabin', 'pclass', 'embarked']].isnull().sum()
""" Make sure that the variables are type (object).
if not, cast it as object , otherwise the transformer will either send an error (if we pass it as argument) 
or not pick it up (if we leave variables=None). """
X[['cabin', 'pclass', 'embarked']].dtypes
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
    random_state=0)
X_train.shape, X_test.shape
"""
Parameters
----------  
variables : list, default=None
    The list of categorical variables that will be encoded. If None, the 
    encoder will find and select all object type variables.
"""
mean_enc = MeanEncoder(variables=['cabin', 'pclass', 'embarked'])
mean_enc.fit(X_train, y_train)
mean_enc.encoder_dict_
train_t = mean_enc.transform(X_train)
test_t = mean_enc.transform(X_test)
test_t.head()
""" The MeanEncoder has the characteristic that return monotonic
 variables, that is, encoded variables which values increase as the target increases"""
plt.figure(figsize=(7, 5))
pd.concat([test_t, y_test], axis=1).groupby('pclass')['survived'].mean().plot()
plt.yticks(np.arange(0, 1.1, 0.1))
plt.title('Relationship between pclass and target')
plt.xlabel('Pclass')
plt.ylabel('Mean of target')
plt.show()
mean_enc = MeanEncoder()
mean_enc.fit(X_train, y_train)
mean_enc.variables
train_t = mean_enc.transform(X_train)
test_t = mean_enc.transform(X_test)
test_t.head()



================================================
File: OneHotEncoder.py
================================================
"""
# OneHotEncoder
Performs One Hot Encoding.

The encoder can select how many different labels per variable to encode into binaries. When top_categories is set to None, all the categories will be transformed in binary variables. 

However, when top_categories is set to an integer, for example 10, then only the 10 most popular categories will be transformed into binary, and the rest will be discarded.

The encoder has also the possibility to create binary variables from all categories (drop_last = False), or remove the binary for the last category (drop_last = True), for use in linear models.

Finally, the encoder has the option to drop the second dummy variable for binary variables. That is, if a categorical variable has 2 unique values, for example colour = ['black', 'white'], setting the parameter drop_last_binary=True, will automatically create only 1 binary for this variable, for example colour_black.
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from feature_engine.encoding import OneHotEncoder


def load_titanic(filepath='titanic.csv'):
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    return data


data = load_titanic('../data/titanic-2/Titanic-Dataset.csv')
data.head()
X = data.drop(['survived', 'name', 'ticket'], axis=1)
y = data.survived
X[['cabin', 'pclass', 'embarked']].isnull().sum()
""" Make sure that the variables are type (object).
if not, cast it as object , otherwise the transformer will either send an error (if we pass it as argument) 
or not pick it up (if we leave variables=None). """
X[['cabin', 'pclass', 'embarked']].dtypes
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
    random_state=0)
X_train.shape, X_test.shape
"""
Parameters
----------

top_categories: int, default=None
    If None, a dummy variable will be created for each category of the variable.
    Alternatively, top_categories indicates the number of most frequent categories
    to encode. Dummy variables will be created only for those popular categories
    and the rest will be ignored. Note that this is equivalent to grouping all the
    remaining categories in one group.
    
variables : list
    The list of categorical variables that will be encoded. If None, the  
    encoder will find and select all object type variables.
    
drop_last: boolean, default=False
    Only used if top_categories = None. It indicates whether to create dummy
    variables for all the categories (k dummies), or if set to True, it will
    ignore the last variable of the list (k-1 dummies).
"""
ohe_enc = OneHotEncoder(top_categories=None, variables=['pclass', 'cabin',
    'embarked'], drop_last=False)
ohe_enc.fit(X_train)
ohe_enc.encoder_dict_
train_t = ohe_enc.transform(X_train)
test_t = ohe_enc.transform(X_train)
test_t.head()
ohe_enc = OneHotEncoder(top_categories=2, variables=['pclass', 'cabin',
    'embarked'], drop_last=False)
ohe_enc.fit(X_train)
ohe_enc.encoder_dict_
train_t = ohe_enc.transform(X_train)
test_t = ohe_enc.transform(X_train)
test_t.head()
ohe_enc = OneHotEncoder(top_categories=None, variables=['pclass', 'cabin',
    'embarked'], drop_last=True)
ohe_enc.fit(X_train)
ohe_enc.encoder_dict_
train_t = ohe_enc.transform(X_train)
test_t = ohe_enc.transform(X_train)
test_t.head()
ohe_enc = OneHotEncoder(top_categories=None, drop_last=True)
ohe_enc.fit(X_train)
ohe_enc.variables
ohe_enc.variables_
ohe_enc.variables_binary_
train_t = ohe_enc.transform(X_train)
test_t = ohe_enc.transform(X_train)
test_t.head()
ohe_enc = OneHotEncoder(top_categories=None, drop_last=False,
    drop_last_binary=True)
ohe_enc.fit(X_train)
ohe_enc.encoder_dict_
ohe_enc.variables_binary_
train_t = ohe_enc.transform(X_train)
test_t = ohe_enc.transform(X_train)
test_t.head()



================================================
File: OrdinalEncoder.py
================================================
"""
# OrdinalEncoder
The OrdinalEncoder() will replace the variable labels by digits, from 1 to the number of different labels. 

If we select "arbitrary", then the encoder will assign numbers as the labels appear in the variable (first come first served).

If we select "ordered", the encoder will assign numbers following the mean of the target value for that label. So labels for which the mean of the target is higher will get the number 1, and those where the mean of the target is smallest will get the number n.
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from feature_engine.encoding import OrdinalEncoder


def load_titanic(filepath='titanic.csv'):
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    return data


data = load_titanic('../data/titanic-2/Titanic-Dataset.csv')
data.head()
X = data.drop(['survived', 'name', 'ticket'], axis=1)
y = data.survived
X[['cabin', 'pclass', 'embarked']].isnull().sum()
""" Make sure that the variables are type (object).
if not, cast it as object , otherwise the transformer will either send an error (if we pass it as argument) 
or not pick it up (if we leave variables=None). """
X[['cabin', 'pclass', 'embarked']].dtypes
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
    random_state=0)
X_train.shape, X_test.shape
"""
Parameters
----------

encoding_method : str, default='ordered' 
    Desired method of encoding.

    'ordered': the categories are numbered in ascending order according to
    the target mean value per category.

    'arbitrary' : categories are numbered arbitrarily.
    
variables : list, default=None
    The list of categorical variables that will be encoded. If None, the 
    encoder will find and select all object type variables.
"""
ordinal_enc = OrdinalEncoder(encoding_method='ordered', variables=['pclass',
    'cabin', 'embarked'])
ordinal_enc.fit(X_train, y_train)
ordinal_enc.encoder_dict_
train_t = ordinal_enc.transform(X_train)
test_t = ordinal_enc.transform(X_test)
test_t.sample(5)
""" The OrdinalEncoder with encoding_method='order' has the characteristic that return monotonic
 variables,that is, encoded variables which values increase as the target increases"""
plt.figure(figsize=(7, 5))
pd.concat([test_t, y_test], axis=1).groupby('pclass')['survived'].mean().plot()
plt.xticks([0, 1, 2])
plt.yticks(np.arange(0, 1.1, 0.1))
plt.title('Relationship between pclass and target')
plt.xlabel('Pclass')
plt.ylabel('Mean of target')
plt.show()
ordinal_enc = OrdinalEncoder(encoding_method='arbitrary', variables=[
    'pclass', 'cabin', 'embarked'])
ordinal_enc.fit(X_train)
ordinal_enc.encoder_dict_
train_t = ordinal_enc.transform(X_train)
test_t = ordinal_enc.transform(X_test)
test_t.sample(5)
ordinal_enc = OrdinalEncoder(encoding_method='arbitrary')
ordinal_enc.fit(X_train)
ordinal_enc.variables
train_t = ordinal_enc.transform(X_train)
test_t = ordinal_enc.transform(X_test)
test_t.sample(5)



================================================
File: PRatioEncoder.py
================================================
"""
# PRatioEncoder

The PRatioEncoder() replaces categories by the ratio of the probability of the
target = 1 and the probability of the target = 0.<br>

The target probability ratio is given by: p(1) / p(0).

The log of the target probability ratio is: np.log( p(1) / p(0) )
#### It only works for binary classification.
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from feature_engine.encoding import PRatioEncoder
from feature_engine.encoding import RareLabelEncoder


def load_titanic(filepath='titanic.csv'):
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    return data


data = load_titanic('../data/titanic-2/Titanic-Dataset.csv')
data.head()
X = data.drop(['survived', 'name', 'ticket'], axis=1)
y = data.survived
X[['cabin', 'pclass', 'embarked']].isnull().sum()
""" Make sure that the variables are type (object).
if not, cast it as object , otherwise the transformer will either send an error (if we pass it as argument) 
or not pick it up (if we leave variables=None). """
X[['cabin', 'pclass', 'embarked']].dtypes
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
    random_state=0)
X_train.shape, X_test.shape
rare_encoder = RareLabelEncoder(tol=0.03, n_categories=2, variables=[
    'cabin', 'pclass', 'embarked'])
rare_encoder.fit(X_train)
train_t = rare_encoder.transform(X_train)
test_t = rare_encoder.transform(X_test)
"""
Parameters
----------

encoding_method : str, default=woe
    Desired method of encoding.

    'ratio' : probability ratio

    'log_ratio' : log probability ratio

variables : list, default=None
    The list of categorical variables that will be encoded. If None, the
    encoder will find and select all object type variables.
"""
Ratio_enc = PRatioEncoder(encoding_method='ratio', variables=['cabin',
    'pclass', 'embarked'])
Ratio_enc.fit(train_t, y_train)
Ratio_enc.encoder_dict_
train_t = Ratio_enc.transform(train_t)
test_t = Ratio_enc.transform(test_t)
test_t.sample(5)
train_t = rare_encoder.transform(X_train)
test_t = rare_encoder.transform(X_test)
logRatio_enc = PRatioEncoder(encoding_method='log_ratio', variables=[
    'cabin', 'pclass', 'embarked'])
logRatio_enc.fit(train_t, y_train)
logRatio_enc.encoder_dict_
train_t = logRatio_enc.transform(train_t)
test_t = logRatio_enc.transform(test_t)
test_t.sample(5)
""" The PRatioEncoder(encoding_method='ratio' or 'log_ratio') has the characteristic that return monotonic
 variables, that is, encoded variables which values increase as the target increases"""
plt.figure(figsize=(7, 5))
pd.concat([test_t, y_test], axis=1).groupby('pclass')['survived'].mean().plot()
plt.yticks(np.arange(0, 1.1, 0.1))
plt.title('Relationship between pclass and target')
plt.xlabel('Pclass')
plt.ylabel('Mean of target')
plt.show()
train_t = rare_encoder.transform(X_train)
test_t = rare_encoder.transform(X_test)
logRatio_enc = PRatioEncoder(encoding_method='log_ratio')
logRatio_enc.fit(train_t, y_train)
train_t = logRatio_enc.transform(train_t)
test_t = logRatio_enc.transform(test_t)
test_t.sample(5)



================================================
File: RareLabelEncoder.py
================================================
"""
# RareLabelEncoder

The RareLabelEncoder() groups labels that show a small number of observations in the dataset into a new category called 'Rare'. This helps to avoid overfitting.

The argument ' tol ' indicates the percentage of observations that the label needs to have in order not to be re-grouped into the "Rare" label.<br> The argument n_categories indicates the minimum number of distinct categories that a variable needs to have for any of the labels to be re-grouped into 'Rare'.<br><br>
#### Note
If the number of labels is smaller than n_categories, then the encoder will not group the labels for that variable.
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from feature_engine.encoding import RareLabelEncoder


def load_titanic(filepath='titanic.csv'):
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    return data


data = load_titanic('../data/titanic-2/Titanic-Dataset.csv')
data.head()
X = data.drop(['survived', 'name', 'ticket'], axis=1)
y = data.survived
X[['cabin', 'pclass', 'embarked']].isnull().sum()
""" Make sure that the variables are type (object).
if not, cast it as object , otherwise the transformer will either send an error (if we pass it as argument) 
or not pick it up (if we leave variables=None). """
X[['cabin', 'pclass', 'embarked']].dtypes
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
    random_state=0)
X_train.shape, X_test.shape
"""
Parameters
----------

tol: float, default=0.05
    the minimum frequency a label should have to be considered frequent.
    Categories with frequencies lower than tol will be grouped.

n_categories: int, default=10
    the minimum number of categories a variable should have for the encoder
    to find frequent labels. If the variable contains less categories, all
    of them will be considered frequent.

max_n_categories: int, default=None
    the maximum number of categories that should be considered frequent.
    If None, all categories with frequency above the tolerance (tol) will be
    considered.

variables : list, default=None
    The list of categorical variables that will be encoded. If None, the 
    encoder will find and select all object type variables.

replace_with : string, default='Rare'
    The category name that will be used to replace infrequent categories.
"""
rare_encoder = RareLabelEncoder(tol=0.05, n_categories=5, variables=[
    'cabin', 'pclass', 'embarked'])
rare_encoder.fit(X_train)
rare_encoder.encoder_dict_
train_t = rare_encoder.transform(X_train)
test_t = rare_encoder.transform(X_train)
test_t.head()
test_t.cabin.value_counts()
rare_encoder = RareLabelEncoder(tol=0.03, replace_with='Other', variables=[
    'cabin', 'pclass', 'embarked'], n_categories=2)
rare_encoder.fit(X_train)
train_t = rare_encoder.transform(X_train)
test_t = rare_encoder.transform(X_train)
test_t.sample(5)
rare_encoder.encoder_dict_
test_t.cabin.value_counts()
rare_encoder = RareLabelEncoder(tol=0.03, variables=['cabin', 'pclass',
    'embarked'], n_categories=2, max_n_categories=3)
rare_encoder.fit(X_train)
train_t = rare_encoder.transform(X_train)
test_t = rare_encoder.transform(X_train)
test_t.sample(5)
rare_encoder.encoder_dict_
len(X_train['pclass'].unique()), len(X_train['sex'].unique()), len(X_train[
    'embarked'].unique())
rare_encoder = RareLabelEncoder(tol=0.03, n_categories=3)
rare_encoder.fit(X_train)
rare_encoder.encoder_dict_
train_t = rare_encoder.transform(X_train)
test_t = rare_encoder.transform(X_train)
test_t.sample(5)



================================================
File: StringSimilarityEncoder.py
================================================
import string
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from feature_engine.encoding import StringSimilarityEncoder


def load_titanic(filepath='titanic.csv'):
    translate_table = str.maketrans('', '', string.punctuation)
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['name'] = data['name'].str.strip().str.translate(translate_table
        ).str.replace('  ', ' ').str.lower()
    data['ticket'] = data['ticket'].str.strip().str.translate(translate_table
        ).str.replace('  ', ' ').str.lower()
    return data


data = load_titanic('../data/titanic-2/Titanic-Dataset.csv')
data.head()
X_train, X_test, y_train, y_test = train_test_split(data.drop(['survived',
    'sex', 'cabin', 'embarked'], axis=1), data['survived'], test_size=0.3,
    random_state=0)
encoder = StringSimilarityEncoder(top_categories=2, variables=['name',
    'ticket'])
encoder.fit(X_train)
encoder.encoder_dict_
train_t = encoder.transform(X_train)
test_t = encoder.transform(X_test)
train_t.head(5)
test_t.head(5)
fig, ax = plt.subplots(2, 1)
train_t.plot(kind='scatter', x='ticket_ca 2343', y='ticket_347082', sharex=
    True, title='Ticket encoding in train', ax=ax[0])
test_t.plot(kind='scatter', x='ticket_ca 2343', y='ticket_347082', sharex=
    True, title='Ticket encoding in test', ax=ax[1])
encoder = StringSimilarityEncoder(top_categories=2, missing_values='ignore',
    variables=['name', 'ticket'])
encoder.fit(X_train)
encoder.encoder_dict_
train_t = encoder.transform(X_train)
test_t = encoder.transform(X_test)
train_t.head(5)
test_t.head(5)
fig, ax = plt.subplots(2, 1)
train_t.plot(kind='scatter', x='home.dest_new york ny', y=
    'home.dest_london', sharex=True, title=
    'Home destination encoding in train', ax=ax[0])
test_t.plot(kind='scatter', x='home.dest_new york ny', y='home.dest_london',
    sharex=True, title='Home destination encoding in test', ax=ax[1])
from sklearn.decomposition import PCA
encoder = StringSimilarityEncoder(top_categories=None, handle_missing=
    'impute', variables=['home.dest'])
encoder.fit(X_train)
train_t = encoder.transform(X_train)
train_t.shape
home_encoded = train_t.filter(like='home.dest')
pca = PCA(n_components=0.9)
pca.fit(home_encoded)
train_compressed = pca.transform(home_encoded)
train_compressed.shape



================================================
File: WoEEncoder.py
================================================
"""
## WoEEncoder (weight of evidence)

This encoder replaces the labels by the weight of evidence 
#### It only works for binary classification.

The weight of evidence is given by: log( p(1) / p(0) )
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from feature_engine.encoding import WoEEncoder
from feature_engine.encoding import RareLabelEncoder


def load_titanic(filepath='titanic.csv'):
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    return data


data = load_titanic('../data/titanic-2/Titanic-Dataset.csv')
data.head()
X = data.drop(['survived', 'name', 'ticket'], axis=1)
y = data.survived
X[['cabin', 'pclass', 'embarked']].isnull().sum()
""" Make sure that the variables are type (object).
if not, cast it as object , otherwise the transformer will either send an error (if we pass it as argument) 
or not pick it up (if we leave variables=None). """
X[['cabin', 'pclass', 'embarked']].dtypes
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
    random_state=0)
X_train.shape, X_test.shape
rare_encoder = RareLabelEncoder(tol=0.03, n_categories=2, variables=[
    'cabin', 'pclass', 'embarked'])
rare_encoder.fit(X_train)
train_t = rare_encoder.transform(X_train)
test_t = rare_encoder.transform(X_test)
woe_enc = WoEEncoder(variables=['cabin', 'pclass', 'embarked'])
woe_enc.fit(train_t, y_train)
woe_enc.encoder_dict_
train_t = woe_enc.transform(train_t)
test_t = woe_enc.transform(test_t)
test_t.sample(5)
""" The WoEEncoder has the characteristic that return monotonic
 variables, that is, encoded variables which values increase as the target increases"""
plt.figure(figsize=(7, 5))
pd.concat([test_t, y_test], axis=1).groupby('pclass')['survived'].mean().plot()
plt.yticks(np.arange(0, 1.1, 0.1))
plt.title('Relationship between pclass and target')
plt.xlabel('Pclass')
plt.ylabel('Mean of target')
plt.show()
ratio_enc = WoEEncoder()
ratio_enc.fit(train_t, y_train)
train_t = ratio_enc.transform(train_t)
test_t = ratio_enc.transform(test_t)
test_t.head()


