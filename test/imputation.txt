Directory structure:
└── imputation/
    ├── AddMissingIndicator.py
    ├── ArbitraryNumberImputer.py
    ├── CategoricalImputer.py
    ├── DropMissingData.py
    ├── EndTailImputer.py
    ├── MeanMedianImputer.py
    └── RandomSampleImputer.py

================================================
File: AddMissingIndicator.py
================================================
"""
# AddMissingIndicator

AddMissingIndicator adds additional binary variables indicating missing data (thus, called missing indicators). The binary variables take the value 1 if the observation's value is missing, or 0 otherwise. AddMissingIndicator adds 1 binary variable per variable.
"""
import feature_engine
feature_engine.__version__
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from feature_engine.imputation import AddMissingIndicator, MeanMedianImputer, CategoricalImputer
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']
X_test = test_df.drop(['Id'], axis=1)
print('X_train :', X_train.shape)
print('X_test :', X_test.shape)
X_train[['Alley', 'MasVnrType', 'LotFrontage', 'MasVnrArea']].isnull().mean()
imputer = AddMissingIndicator(variables=['Alley', 'MasVnrType',
    'LotFrontage', 'MasVnrArea'])
imputer.fit(X_train)
imputer.variables_
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
train_t[['Alley_na', 'MasVnrType_na', 'LotFrontage_na', 'MasVnrArea_na']].head(
    )
train_t[['Alley_na', 'MasVnrType_na', 'LotFrontage_na', 'MasVnrArea_na']].mean(
    )
X_train[['Alley', 'MasVnrType', 'LotFrontage', 'MasVnrArea']].dtypes
pipe = Pipeline([('indicators', AddMissingIndicator(variables=['Alley',
    'MasVnrType', 'LotFrontage', 'MasVnrArea'])), ('imputer_num',
    MeanMedianImputer(imputation_method='median', variables=['LotFrontage',
    'MasVnrArea'])), ('imputer_cat', CategoricalImputer(imputation_method=
    'frequent', variables=['Alley', 'MasVnrType']))])
pipe.fit(X_train)
pipe.named_steps['indicators'].variables_
pipe.named_steps['imputer_num'].imputer_dict_
pipe.named_steps['imputer_cat'].imputer_dict_
train_t = pipe.transform(X_train)
test_t = pipe.transform(X_test)
vars_ = ['Alley', 'MasVnrType', 'LotFrontage', 'MasVnrArea', 'Alley_na',
    'MasVnrType_na', 'LotFrontage_na', 'MasVnrArea_na']
train_t[vars_].head()
train_t[vars_].isnull().sum()
imputer = AddMissingIndicator(variables=None, missing_only=True)
imputer.fit(X_train)
imputer.variables
imputer.variables_
len(imputer.variables_)
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
X_train.shape, train_t.shape
train_t.head()
imputer = AddMissingIndicator(variables=None, missing_only=False)
imputer.fit(X_train)
len(imputer.variables_)
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
X_train.shape, train_t.shape
pipe = Pipeline([('indicators', AddMissingIndicator(missing_only=True)), (
    'imputer_num', MeanMedianImputer(imputation_method='median')), (
    'imputer_cat', CategoricalImputer(imputation_method='frequent'))])
pipe.fit(X_train)
pipe.named_steps['indicators'].variables_
pipe.named_steps['imputer_num'].imputer_dict_
pipe.named_steps['imputer_cat'].imputer_dict_
train_t = pipe.transform(X_train)
test_t = pipe.transform(X_test)
train_t.isnull().sum()
[v for v in train_t.columns if train_t[v].isnull().sum() > 1]



================================================
File: ArbitraryNumberImputer.py
================================================
"""
# ArbitraryNumberImputer

ArbitraryNumberImputer replaces NA by an arbitrary value. It works for numerical variables. The arbitrary value needs to be defined by the user.
"""
import feature_engine
feature_engine.__version__
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from feature_engine.imputation import ArbitraryNumberImputer
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']
X_test = test_df.drop(['Id'], axis=1)
print('X_train :', X_train.shape)
print('X_test :', X_test.shape)
X_train[['LotFrontage', 'MasVnrArea']].isnull().mean()
imputer = ArbitraryNumberImputer(arbitrary_number=-999, variables=[
    'LotFrontage', 'MasVnrArea'])
imputer.fit(X_train)
imputer.arbitrary_number
imputer.imputer_dict_
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
train_t[['LotFrontage', 'MasVnrArea']].min()
fig = plt.figure()
ax = fig.add_subplot(111)
X_train['LotFrontage'].plot(kind='kde', ax=ax)
train_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')
imputer = ArbitraryNumberImputer(imputer_dict={'LotFrontage': -678,
    'MasVnrArea': -789})
imputer.fit(X_train)
imputer.imputer_dict_
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
train_t[['LotFrontage', 'MasVnrArea']].min()
fig = plt.figure()
ax = fig.add_subplot(111)
X_train['LotFrontage'].plot(kind='kde', ax=ax)
train_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')
imputer = ArbitraryNumberImputer(arbitrary_number=-1)
imputer.fit(X_train)
imputer.variables_
imputer.imputer_dict_
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
[v for v in train_t.columns if train_t[v].dtypes != 'O' and train_t[v].
    isnull().sum() > 1]
imputer.get_feature_names_out()



================================================
File: CategoricalImputer.py
================================================
"""
# Missing value imputation: CategoricalImputer


CategoricalImputer performs imputation of categorical variables. It replaces missing values by an arbitrary label "Missing" (default) or any other label entered by the user. Alternatively, it imputes missing data with the most frequent category.
"""
import feature_engine
feature_engine.__version__
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from feature_engine.imputation import CategoricalImputer
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']
X_test = test_df.drop(['Id'], axis=1)
print('X_train :', X_train.shape)
print('X_test :', X_test.shape)
X_train[['Alley', 'MasVnrType']].isnull().mean()
X_train['MasVnrType'].value_counts().plot.bar()
plt.ylabel('Number of observations')
plt.title('MasVnrType')
imputer = CategoricalImputer(imputation_method='missing', variables=[
    'Alley', 'MasVnrType'])
imputer.fit(X_train)
imputer.imputer_dict_
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
test_t['MasVnrType'].value_counts().plot.bar()
plt.ylabel('Number of observations')
plt.title('Imputed MasVnrType')
test_t['Alley'].value_counts().plot.bar()
plt.ylabel('Number of observations')
plt.title('Imputed Alley')
imputer = CategoricalImputer(variables='MasVnrType', fill_value=
    'this_is_missing')
train_t = imputer.fit_transform(X_train)
test_t = imputer.transform(X_test)
imputer.imputer_dict_
test_t['MasVnrType'].value_counts().plot.bar()
plt.ylabel('Number of observations')
plt.title('Imputed MasVnrType')
imputer = CategoricalImputer(imputation_method='frequent', variables=[
    'Alley', 'MasVnrType'])
imputer.fit(X_train)
imputer.imputer_dict_
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
X_train['MasVnrType'].value_counts()
train_t['MasVnrType'].value_counts()
imputer = CategoricalImputer(imputation_method='frequent')
imputer.fit(X_train)
imputer.imputer_dict_
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
[v for v in train_t.columns if train_t[v].dtypes == 'O' and train_t[v].
    isnull().sum() > 1]
imputer.get_feature_names_out()



================================================
File: DropMissingData.py
================================================
"""
# Missing value imputation: DropMissingData

Deletes rows with missing values. DropMissingData works both with numerical and categorical variables.
"""
import feature_engine
feature_engine.__version__
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from feature_engine.imputation import DropMissingData
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']
X_test = test_df.drop(['Id'], axis=1)
print('X_train :', X_train.shape)
print('X_test :', X_test.shape)
imputer = DropMissingData(variables=['Alley', 'MasVnrType', 'LotFrontage',
    'MasVnrArea'], missing_only=False)
imputer.fit(X_train)
imputer.variables_
X_train[imputer.variables].isna().sum()
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
train_t[imputer.variables].isna().sum()
X_train.shape
train_t.shape
tmp = imputer.return_na_data(X_train)
tmp.shape
1022 - 963
imputer = DropMissingData(variables=['Alley', 'MasVnrType', 'LotFrontage',
    'MasVnrArea'], missing_only=False, threshold=0.5)
imputer.fit(X_train)
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
train_t[imputer.variables].isna().sum()
imputer = DropMissingData(missing_only=True)
imputer.fit(X_train)
imputer.variables_
X_train[imputer.variables_].isna().sum()
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
train_t[imputer.variables_].isna().sum()
train_t.shape
imputer = DropMissingData(missing_only=True, threshold=0.75)
imputer.fit(X_train)
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
train_t.shape



================================================
File: EndTailImputer.py
================================================
"""
# EndTailImputer

The EndTailImputer() replaces missing data by a value at either tail of the distribution. It automatically determines the value to be used in the imputation using the mean plus or minus a factor of the standard deviation, or using the inter-quartile range proximity rule. Alternatively, it can use a factor of the maximum value.
The EndTailImputer() is in essence, very similar to the ArbitraryNumberImputer, but it selects the value to use fr the imputation automatically, instead of having the user pre-define them.
It works only with numerical variables.
"""

import feature_engine
feature_engine.__version__
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from feature_engine.imputation import EndTailImputer
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']
X_test = test_df.drop(['Id'], axis=1)
print('X_train :', X_train.shape)
print('X_test :', X_test.shape)
X_train[['LotFrontage', 'MasVnrArea']].isnull().mean()
imputer = EndTailImputer(imputation_method='gaussian', tail='right', fold=3,
    variables=['LotFrontage', 'MasVnrArea'])
imputer.fit(X_train)
imputer.imputer_dict_
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
train_t['LotFrontage'].isnull().sum()
fig = plt.figure()
ax = fig.add_subplot(111)
X_train['LotFrontage'].plot(kind='kde', ax=ax)
train_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')
imputer = EndTailImputer(imputation_method='iqr', tail='left', fold=3,
    variables=['LotFrontage', 'MasVnrArea'])
imputer.fit(X_train)
imputer.imputer_dict_
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
train_t[['LotFrontage', 'MasVnrArea']].isnull().sum()
fig = plt.figure()
ax = fig.add_subplot(111)
X_train['LotFrontage'].plot(kind='kde', ax=ax)
train_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')
imputer = EndTailImputer(imputation_method='max', fold=3, variables=[
    'LotFrontage', 'MasVnrArea'])
imputer.fit(X_train)
imputer.imputer_dict_
X_train[imputer.variables_].max()
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
train_t[['LotFrontage', 'MasVnrArea']].isnull().sum()
fig = plt.figure()
ax = fig.add_subplot(111)
X_train['LotFrontage'].plot(kind='kde', ax=ax)
train_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')
imputer = EndTailImputer()
imputer.imputation_method
imputer.tail
imputer.fold
imputer.fit(X_train)
imputer.variables_
imputer.imputer_dict_
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
[v for v in train_t.columns if train_t[v].dtypes != 'O' and train_t[v].
    isnull().sum() > 1]



================================================
File: MeanMedianImputer.py
================================================
"""
# Missing value imputation: MeanMedianImputer

The MeanMedianImputer() replaces missing data by the mean or median value of the variable. It works only with numerical variables.
"""
import feature_engine
feature_engine.__version__
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from feature_engine.imputation import MeanMedianImputer
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']
X_test = test_df.drop(['Id'], axis=1)
print('X_train :', X_train.shape)
print('X_test :', X_test.shape)
X_train[['LotFrontage', 'MasVnrArea']].isnull().mean()
imputer = MeanMedianImputer(imputation_method='median', variables=[
    'LotFrontage', 'MasVnrArea'])
imputer.fit(X_train)
imputer.imputer_dict_
X_train[['LotFrontage', 'MasVnrArea']].median()
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
train_t[['LotFrontage', 'MasVnrArea']].isnull().sum()
fig = plt.figure()
ax = fig.add_subplot(111)
X_train['LotFrontage'].plot(kind='kde', ax=ax)
train_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')
imputer = MeanMedianImputer(imputation_method='mean')
imputer.fit(X_train)
imputer.variables_
imputer.imputer_dict_
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
test_t[imputer.variables_].isnull().sum()



================================================
File: RandomSampleImputer.py
================================================
"""
# Missing value imputation: RandomSampleImputer

The RandomSampleImputer extracts a random sample of observations where data is available, and uses it to replace the NA. It is suitable for numerical and categorical variables.
To control the random sample extraction, there are various ways to set a seed and ensure or maximize reproducibility.
"""
import feature_engine
feature_engine.__version__
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from feature_engine.imputation import RandomSampleImputer
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']
X_test = test_df.drop(['Id'], axis=1)
print('X_train :', X_train.shape)
print('X_test :', X_test.shape)
imputer = RandomSampleImputer(variables=['Alley', 'MasVnrType',
    'LotFrontage', 'MasVnrArea'], random_state=10, seed='general')
imputer.fit(X_train)
imputer.X_.head()
X_train[['Alley', 'MasVnrType', 'LotFrontage', 'MasVnrArea']].isnull().mean()
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
train_t[['Alley', 'MasVnrType', 'LotFrontage', 'MasVnrArea']].isnull().mean()
fig = plt.figure()
ax = fig.add_subplot(111)
X_train['LotFrontage'].plot(kind='kde', ax=ax)
train_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')
imputer = RandomSampleImputer(random_state=['MSSubClass', 'YrSold'], seed=
    'observation', seeding_method='add', variables=None)
imputer.fit(X_train)
imputer.X_
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)
test_t.isnull().sum()
fig = plt.figure()
ax = fig.add_subplot(111)
X_train['LotFrontage'].plot(kind='kde', ax=ax)
train_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')


