// Sklearn-wrapper-plus-SimpleImputer.py
// Sklearn-wrapper-plus-SimpleImputer.py
# Generated from: Sklearn-wrapper-plus-SimpleImputer.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

from feature_engine.wrappers import SklearnTransformerWrapper


# # load house prices data set from Kaggle

# data = pd.read_csv('houseprice.csv')
# data.head()

# # let's separate into training and testing set

# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1),
#     data['SalePrice'],
#     test_size=0.3,
#     random_state=0)

# X_train.shape, X_test.shape



# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


X_train[['LotFrontage', 'MasVnrArea']].isnull().mean()


# ## SimpleImputer
#
# ### Mean imputation


imputer = SklearnTransformerWrapper(
    transformer = SimpleImputer(strategy='mean'),
    variables = ['LotFrontage', 'MasVnrArea'],
)

imputer.fit(X_train)


# we can find the mean values within the parameters of the
# simple imputer

imputer.transformer_.statistics_


# remove NA

X_train = imputer.transform(X_train)
X_test = imputer.transform(X_test)

X_train[['LotFrontage', 'MasVnrArea']].isnull().mean()


# ### Frequent category imputation


cols = [c for c in train_df.columns if train_df[c].dtypes=='O' and train_df[c].isnull().sum()>0]
train_df[cols].head()


imputer = SklearnTransformerWrapper(
    transformer=SimpleImputer(strategy='most_frequent'),
    variables=cols,
)

# find the most frequent category
imputer.fit(X_train)


# we can find the most frequent values within the parameters of the
# simple imputer

imputer.transformer_.statistics_


# remove NA

X_train = imputer.transform(X_train)
X_test = imputer.transform(X_test)

X_train[cols].isnull().mean()


X_test[cols].head()



// ---------------------------------------------------

// Sklearn-wrapper-plus-Categorical-Encoding.py
// Sklearn-wrapper-plus-Categorical-Encoding.py
# Generated from: Sklearn-wrapper-plus-Categorical-Encoding.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder

from feature_engine.wrappers import SklearnTransformerWrapper
from feature_engine.encoding import RareLabelEncoder


# # load the dataset from Kaggle

# data = pd.read_csv('houseprice.csv')
# data.head()

# # let's separate into training and testing set

# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1),
#     data['SalePrice'],
#     test_size=0.3,
#     random_state=0,
# )

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# ## OrdinalEncoder


cols = ['Alley',
        'MasVnrType',
        'BsmtQual',
        'BsmtCond',
        'BsmtExposure',
        'BsmtFinType1',
        'BsmtFinType2',
        'Electrical',
        'FireplaceQu',
        'GarageType',
        'GarageFinish',
        'GarageQual',
        ]


# let's remove rare labels to avoid errors when encoding

rare_label_enc = RareLabelEncoder(n_categories=2, variables=cols)

X_train = rare_label_enc.fit_transform(X_train.fillna('Missing'))
X_test = rare_label_enc.transform(X_test.fillna('Missing'))


# now let's replace categories by integers

encoder = SklearnTransformerWrapper(
    transformer = OrdinalEncoder(),
    variables = cols,
)

encoder.fit(X_train)


# we can navigate to the parameters of the sklearn transformer
# like this:

encoder.transformer_.categories_


# encode categories

X_train = encoder.transform(X_train)
X_test = encoder.transform(X_test)

X_train[cols].isnull().mean()


X_test[cols].head()



// ---------------------------------------------------

// Sklearn-wrapper-plus-scalers.py
// Sklearn-wrapper-plus-scalers.py
# Generated from: Sklearn-wrapper-plus-scalers.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from feature_engine.wrappers import SklearnTransformerWrapper


# data = pd.read_csv('houseprice.csv')
# data.head()

# # let's separate into training and testing set

# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1), data['SalePrice'], test_size=0.3, random_state=0)

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# ## Scaling


cols = [var for var in X_train.columns if X_train[var].dtypes !='O']

cols


# let's apply the standard scaler on the above variables

scaler = SklearnTransformerWrapper(transformer = StandardScaler(),
                                    variables = cols)

scaler.fit(X_train.fillna(0))


X_train = scaler.transform(X_train.fillna(0))
X_test = scaler.transform(X_test.fillna(0))


# mean values, learnt by the StandardScaler
scaler.transformer_.mean_


# std values, learnt by the StandardScaler
scaler.transformer_.scale_


# the mean of the scaled variables is 0
X_train[cols].mean()


# the std of the scaled variables is ~1

X_train[cols].std()



// ---------------------------------------------------

// Sklearn-wrapper-plus-KBinsDiscretizer.py
// Sklearn-wrapper-plus-KBinsDiscretizer.py
# Generated from: Sklearn-wrapper-plus-KBinsDiscretizer.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import KBinsDiscretizer

from feature_engine.wrappers import SklearnTransformerWrapper


# # load house prices data set from Kaggle

# data = pd.read_csv('houseprice.csv')
# data.head()

# # let's separate into training and testing set

# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1),
#     data['SalePrice'],
#     test_size=0.3,
#     random_state=0)

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


cols = [var for var in X_train.columns if X_train[var].dtypes !='O']

cols


X_train[cols].hist(bins=50, figsize=(15,15))
plt.show()


# ## KBinsDiscretizer
#
# ### Equal-frequency discretization


variables = ['GrLivArea','GarageArea']

X_train[variables].isnull().mean()


# # at the moment it only works if the encoding in kbinsdiscretizer
# # is set to 'ordinal'

# discretizer = SklearnTransformerWrapper(
#     transformer = KBinsDiscretizer(
#         n_bins=5, strategy='quantile', encode='ordinal'),
#     variables = variables,
# )

# discretizer.fit(X_train)

from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

# Create pipeline with imputer and discretizer
discretizer = Pipeline([
    ('imputer', SklearnTransformerWrapper(
        transformer=SimpleImputer(strategy='median'),
        variables=variables
    )),
    ('discretizer', SklearnTransformerWrapper(
        transformer=KBinsDiscretizer(n_bins=5, strategy='quantile', encode='ordinal'),
        variables=variables
    ))
])

# Fit and transform
discretizer.fit(X_train)



# discretizer.variables_


# discretizer.transformer_


# we can find the mean values within the parameters of the
# simple imputer

# discretizer.transformer_.bin_edges_


# remove NA

X_train = discretizer.transform(X_train)
X_test = discretizer.transform(X_test)


X_test['GrLivArea'].value_counts(normalize=True)


X_test['GarageArea'].value_counts(normalize=True)


X_test[variables].hist()
plt.show()



// ---------------------------------------------------

// Sklearn-wrapper-plus-feature-selection.py
// Sklearn-wrapper-plus-feature-selection.py
# Generated from: Sklearn-wrapper-plus-feature-selection.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import (
    f_regression,
    SelectKBest,
    SelectFromModel,
)

from sklearn.linear_model import Lasso

from feature_engine.wrappers import SklearnTransformerWrapper


# # load dataset

# data = pd.read_csv('houseprice.csv')
# data.head()

# # let's separate into training and testing set

# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1),
#     data['SalePrice'],
#     test_size=0.3,
#     random_state=0,
# )

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# ## Select K Best


# variables to evaluate:

cols = [var for var in X_train.columns if X_train[var].dtypes !='O']

cols


# let's use select K best to select the best k variables

selector = SklearnTransformerWrapper(
    transformer = SelectKBest(f_regression, k=5),
    variables = cols)

selector.fit(X_train.fillna(0), y_train)


selector.transformer_.get_support(indices=True)


# selecteed features

X_train.columns[selector.transformer_.get_support(indices=True)]


# the transformer returns the selected variables from the list
# we passed to the transformer PLUS the remaining variables 
# in the dataframe that were not examined

X_train_t = selector.transform(X_train.fillna(0))
X_test_t = selector.transform(X_test.fillna(0))


X_test_t.head()


# ## SelectFromModel


# let's select the best variables according to Lasso

lasso = Lasso(alpha=10000, random_state=0)

sfm = SelectFromModel(lasso, prefit=False)

selector = SklearnTransformerWrapper(
    transformer = sfm,
    variables = cols)

selector.fit(X_train.fillna(0), y_train)


selector.transformer_.get_support(indices=True)


len(selector.transformer_.get_support(indices=True))


len(cols)


# the transformer returns the selected variables from the list
# we passed to the transformer PLUS the remaining variables 
# in the dataframe that were not examined

X_train_t = selector.transform(X_train.fillna(0))
X_test_t = selector.transform(X_test.fillna(0))


X_test_t.head()



// ---------------------------------------------------

