Directory structure:
└── pipelines/
    ├── adult-income-with-feature-engine.py
    ├── create-new-features-with-feature-engine.py
    └── predict-house-price-with-feature-engine.py

================================================
File: adult-income-with-feature-engine.py
================================================
import feature_engine
feature_engine.__version__
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, f1_score
from sklearn.preprocessing import StandardScaler
from feature_engine import imputation as mdi
from feature_engine import discretisation as dsc
from feature_engine import encoding as ce
filename = '../data/adult/adult.data'
col_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',
    'marital-status', 'occupation', 'relationship', 'race', 'sex',
    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',
    'income']
data = pd.read_csv(filename, sep=',', names=col_names)
print(data.shape)
data.head()
data.info()
categorical = [var for var in data.columns if data[var].dtype == 'O']
discrete = [var for var in data.columns if data[var].dtype != 'O']
categorical
discrete
data[discrete].hist(bins=30, figsize=(15, 15))
plt.show()
for var in categorical:
    sns.catplot(data=data, y=var, hue=var, kind='count', palette='ch:.25',
        legend=False)
data['income'] = data.income.apply(lambda x: x.replace('<=50K', '0'))
data['income'] = data.income.apply(lambda x: x.replace('>50K', '1'))
data['income'] = data.income.apply(lambda x: int(x))
data.head()
X_train, X_test, y_train, y_test = train_test_split(data.drop(['income'],
    axis=1), data['income'], test_size=0.1, random_state=42)
X_train.shape, X_test.shape
categorical.pop()
income_pipe = Pipeline([('rare_label_enc', ce.RareLabelEncoder(tol=0.1,
    n_categories=1)), ('categorical_enc', ce.DecisionTreeEncoder(regression
    =False, param_grid={'max_depth': [1, 2, 3]}, random_state=2909,
    variables=categorical)), ('discretisation', dsc.DecisionTreeDiscretiser
    (regression=False, param_grid={'max_depth': [1, 2, 3]}, random_state=
    2909, variables=discrete)), ('gbm', GradientBoostingClassifier(
    random_state=42))])
income_pipe.fit(X_train, y_train)
X_train_preds = income_pipe.predict(X_train)
X_test_preds = income_pipe.predict(X_test)
print('train accuracy: {}'.format(accuracy_score(y_train, X_train_preds)))
print()
print('test accuracy: {}'.format(accuracy_score(y_test, X_test_preds)))



================================================
File: create-new-features-with-feature-engine.py
================================================
import feature_engine
feature_engine.__version__
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.pipeline import Pipeline
from feature_engine.creation import RelativeFeatures, MathFeatures
data = pd.read_csv('../data/winequality-red.csv', sep=';')
print(data.shape)
data.head()
(data['quality'].value_counts() / len(data)).sort_index().plot.bar()
plt.title('Wine Quality')
plt.ylabel('Percentage of wines in the data')
plt.xlabel('Wine Quality')
plt.show()
data['quality'] = np.where(data['quality'] <= 6, 0, 1)
(data['quality'].value_counts() / len(data)).plot.bar()
plt.title('Wine Quality')
plt.ylabel('Percentage of wines in the data')
plt.xlabel('Wine Quality')
plt.show()
data.hist(bins=50, figsize=(10, 10))
plt.show()
g = sns.PairGrid(data, x_vars=['quality'], y_vars=data.columns[0:-1])
g.map(sns.barplot)
plt.show()
df = data.melt(id_vars=['quality'])
cols = df.variable.unique()
g = sns.axisgrid.FacetGrid(df[df.variable.isin(cols[0:6])], col='variable',
    sharey=False)
g.map(sns.boxplot, 'quality', 'value')
plt.show()
g = sns.axisgrid.FacetGrid(df[df.variable.isin(cols[6:])], col='variable',
    sharey=False)
g.map(sns.boxplot, 'quality', 'value')
plt.show()
data.head()
plt.scatter(data['citric acid'], data['pH'], c=data['quality'])
plt.xlabel('Citric acid')
plt.ylabel('pH')
plt.show()
plt.scatter(data['sulphates'], data['pH'], c=data['quality'])
plt.xlabel('sulphates')
plt.ylabel('pH')
plt.show()
plt.scatter(data['sulphates'], data['citric acid'], c=data['quality'])
plt.xlabel('sulphates')
plt.ylabel('citric acid')
plt.show()
g = sns.PairGrid(data, y_vars=['density'], x_vars=['chlorides', 'sulphates',
    'residual sugar', 'alcohol'])
g.map(sns.regplot)
plt.show()
combinator = MathFeatures(variables=['fixed acidity', 'volatile acidity'],
    func=['sum', 'mean'], new_variables_names=['total_acidity',
    'average_acidity'])
data = combinator.fit_transform(data)
data.head()
combinator = MathFeatures(variables=['chlorides', 'sulphates'], func=['sum',
    'mean'], new_variables_names=['total_minerals', 'average_minerals'])
data = combinator.fit_transform(data)
data.head()
combinator = RelativeFeatures(variables=['total sulfur dioxide'], reference
    =['free sulfur dioxide'], func=['sub'])
data = combinator.fit_transform(data)
data.head()
combinator = RelativeFeatures(variables=['free sulfur dioxide'], reference=
    ['total sulfur dioxide'], func=['div'])
data = combinator.fit_transform(data)
data.head()
combinator = RelativeFeatures(variables=['sulphates'], reference=[
    'free sulfur dioxide'], func=['div'])
data = combinator.fit_transform(data)
data.head()
data.columns
new_vars = ['total_acidity', 'average_acidity', 'total_minerals',
    'average_minerals', 'total sulfur dioxide_sub_free sulfur dioxide',
    'free sulfur dioxide_div_total sulfur dioxide',
    'free sulfur dioxide_div_total sulfur dioxide']
df = data[new_vars + ['quality']].melt(id_vars=['quality'])
cols = df.variable.unique()
g = sns.axisgrid.FacetGrid(df[df.variable.isin(cols)], col='variable',
    sharey=False)
g.map(sns.boxplot, 'quality', 'value')
plt.show()
data = pd.read_csv('../data/winequality-red.csv', sep=';')
data['quality'] = np.where(data['quality'] <= 6, 0, 1)
X_train, X_test, y_train, y_test = train_test_split(data.drop(labels=[
    'quality'], axis=1), data['quality'], test_size=0.2, random_state=0)
X_train.shape, X_test.shape
pipe = Pipeline([('acidity', MathFeatures(variables=['fixed acidity',
    'volatile acidity'], func=['sum', 'mean'], new_variables_names=[
    'total_acidity', 'average_acidity'])), ('total_minerals', MathFeatures(
    variables=['chlorides', 'sulphates'], func=['sum', 'mean'],
    new_variables_names=['total_minerals', 'average_minearals'])), (
    'non_free_sulfur', RelativeFeatures(variables=['total sulfur dioxide'],
    reference=['free sulfur dioxide'], func=['sub'])), ('perc_free_sulfur',
    RelativeFeatures(variables=['free sulfur dioxide'], reference=[
    'total sulfur dioxide'], func=['div'])), ('perc_salt_sulfur',
    RelativeFeatures(variables=['sulphates'], reference=[
    'free sulfur dioxide'], func=['div'])), ('gbm',
    GradientBoostingClassifier(n_estimators=10, max_depth=2, random_state=1))])
pipe.fit(X_train, y_train)
pred = pipe.predict_proba(X_train)
print('Train roc-auc: {}'.format(roc_auc_score(y_train, pred[:, 1])))
pred = pipe.predict_proba(X_test)
print('Test roc-auc: {}'.format(roc_auc_score(y_test, pred[:, 1])))
new_vars = ['total_acidity', 'average_acidity', 'total_minerals',
    'average_minearals', 'non_free_sulfur_dioxide',
    'percentage_free_sulfur', 'percentage_salt_sulfur']
importance = pd.Series(pipe.named_steps['gbm'].feature_importances_)
importance.index = list(X_train.columns) + new_vars
importance.sort_values(ascending=False).plot.bar(figsize=(15, 5))
plt.ylabel('Feature importance')
plt.show()



================================================
File: predict-house-price-with-feature-engine.py
================================================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score, log_loss
from sklearn.preprocessing import StandardScaler
from feature_engine import imputation as mdi
from feature_engine import discretisation as dsc
from feature_engine import encoding as ce
data = pd.read_csv('../data/house-prices/train.csv')
categorical = [var for var in data.columns if data[var].dtype == 'O']
year_vars = [var for var in data.columns if 'Yr' in var or 'Year' in var]
discrete = [var for var in data.columns if data[var].dtype != 'O' and len(
    data[var].unique()) < 20 and var not in year_vars]
numerical = [var for var in data.columns if data[var].dtype != 'O' if var
     not in discrete and var not in ['Id', 'SalePrice'] and var not in
    year_vars]
sns.pairplot(data=data, y_vars=['SalePrice'], x_vars=['LotFrontage',
    'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2'])
sns.pairplot(data=data, y_vars=['SalePrice'], x_vars=['BsmtUnfSF',
    'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF'])
sns.pairplot(data=data, y_vars=['SalePrice'], x_vars=['GrLivArea',
    'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch'])
sns.pairplot(data=data, y_vars=['SalePrice'], x_vars=['3SsnPorch',
    'ScreenPorch', 'MiscVal'])
data[discrete] = data[discrete].astype('O')
X_train, X_test, y_train, y_test = train_test_split(data.drop(['Id',
    'SalePrice'], axis=1), data['SalePrice'], test_size=0.1, random_state=0)
X_train.shape, X_test.shape


def elapsed_years(df, var):
    df[var] = df['YrSold'] - df[var]
    return df


for var in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:
    X_train = elapsed_years(X_train, var)
    X_test = elapsed_years(X_test, var)
X_train.drop('YrSold', axis=1, inplace=True)
X_test.drop('YrSold', axis=1, inplace=True)
house_pipe = Pipeline([('missing_ind', mdi.AddMissingIndicator(missing_only
    =True)), ('imputer_num', mdi.MeanMedianImputer(imputation_method=
    'median')), ('imputer_cat', mdi.CategoricalImputer(return_object=True)),
    ('rare_label_enc', ce.RareLabelEncoder(tol=0.1, n_categories=1)), (
    'categorical_enc', ce.DecisionTreeEncoder(param_grid={'max_depth': [1, 
    2, 3]}, random_state=2909)), ('discretisation', dsc.
    DecisionTreeDiscretiser(param_grid={'max_depth': [1, 2, 3]},
    random_state=2909, variables=numerical)), ('scaler', StandardScaler()),
    ('lasso', Lasso(alpha=100, random_state=0, max_iter=1000))])
house_pipe.fit(X_train, y_train)
X_train_preds = house_pipe.predict(X_train)
X_test_preds = house_pipe.predict(X_test)
print('train mse: {}'.format(mean_squared_error(y_train, X_train_preds,
    squared=True)))
print('train rmse: {}'.format(mean_squared_error(y_train, X_train_preds,
    squared=False)))
print('train r2: {}'.format(r2_score(y_train, X_train_preds)))
print()
print('test mse: {}'.format(mean_squared_error(y_test, X_test_preds,
    squared=True)))
print('test rmse: {}'.format(mean_squared_error(y_test, X_test_preds,
    squared=False)))
print('test r2: {}'.format(r2_score(y_test, X_test_preds)))


