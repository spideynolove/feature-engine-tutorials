// MatchVariables.py
// preprocessing/MatchVariables.py
# Generated from: MatchVariables.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # MatchVariables
#
#
# MatchVariables() ensures that the columns in the test set are identical to those
# in the train set.
#
# If the test set contains additional columns, they are dropped. Alternatively, if the
# test set lacks columns that were present in the train set, they will be added with a
# value determined by the user, for example np.nan.


import numpy as np
import pandas as pd

from feature_engine.preprocessing import MatchVariables


# # Load titanic dataset from OpenML

# def load_titanic(filepath='../data/titanic.csv'):
#     # data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
#     data = pd.read_csv(filepath)
#     data = data.replace('?', np.nan)
#     data['cabin'] = data['cabin'].astype(str).str[0]
#     data['pclass'] = data['pclass'].astype('O')
#     data['age'] = data['age'].astype('float')
#     data['fare'] = data['fare'].astype('float')
#     data['embarked'].fillna('C', inplace=True)
#     data.drop(
#         # labels=['name', 'ticket', 'boat', 'body', 'home.dest'],
#         labels=['name', 'ticket'],
#         axis=1, inplace=True,
#     )
#     return data

# data = load_titanic()
# # data.head()
# # data.shape

# # separate the dataset into train and test

# train = data.iloc[0:1000, :]
# test = data.iloc[1000:, :]

# train.shape, test.shape


def load_titanic(train_path='../data/titanic-3/train.csv', test_path='../data/titanic-3/test.csv'):
    # Read both train and test datasets
    train = pd.read_csv(train_path)
    test = pd.read_csv(test_path)
    
    # Common preprocessing for both datasets
    def preprocess_df(df):
        df = df.replace('?', np.nan)
        df['cabin'] = df['cabin'].astype(str).str[0]
        df['pclass'] = df['pclass'].astype('O')
        df['age'] = df['age'].astype('float')
        df['fare'] = df['fare'].astype('float')
        df['embarked'].fillna('C', inplace=True)
        df.drop(
            labels=['name', 'ticket'],
            axis=1, inplace=True,
        )
        return df
    
    # Apply preprocessing to both datasets
    train = preprocess_df(train)
    test = preprocess_df(test)
    
    return train, test


train, test = load_titanic()
print("Train shape:", train.shape)
print("Test shape:", test.shape)


# set up the transformer
match_cols = MatchVariables(missing_values="ignore")

# learn the variables in the train set
match_cols.fit(train)


# the transformer stores the input variables
# match_cols.input_features_
match_cols.feature_names_in_


# ## 1 - Some columns are missing in the test set


match_cols


# Let's drop some columns in the test set for the demo
test_t = test.drop(["sex", "age"], axis=1)


# test.columns
test_t.shape


test_t.head()


# the transformer adds the columns back
test_tt = match_cols.transform(test_t)

print()
test_tt.head()


# Note how the missing columns were added back to the transformed test set, with
# missing values, in the position (i.e., order) in which they were in the train set.
#
# Similarly, if the test set contained additional columns, those would be removed:


# ## Test set contains variables not present in train set


test_t.loc[:, "new_col1"] = 5
test_t.loc[:, "new_col2"] = "test"

test_t.head()


# set up the transformer with different
# fill value
match_cols = MatchVariables(
    fill_value=0, missing_values="ignore",
)

# learn the variables in the train set
match_cols.fit(train)


test_tt = match_cols.transform(test_t)

print()
test_tt.head()


# Note how the columns that were present in the test set but not in train set were dropped. And now, the missing variables were added back into the dataset with the value 0.



// ---------------------------------------------------

// create-new-features-with-feature-engine.py
// pipelines/create-new-features-with-feature-engine.py
# Generated from: create-new-features-with-feature-engine.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Create new features within a Pipeline
#
# In this notebook, I show how easy and practical is to create new features Feature-engine and the scikit-learn pipeline.
#
# For this demonstration, we use the UCI Wine Quality Dataset.
#
# The data is publicly available on [UCI repository](https://archive.ics.uci.edu/ml/datasets/Wine+Quality)
#
# P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.


import feature_engine
feature_engine.__version__


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.pipeline import Pipeline


# import classes from Feature-engine
from feature_engine.creation import RelativeFeatures, MathFeatures
# from feature_engine.creation import MathFeatures, RelativeFeatures


# Load dataset

data = pd.read_csv('../data/winequality-red.csv', sep=';')

print(data.shape)

data.head()


# ## Exploratory Data Analysis
#
# Let's have a look at the variables and their relationships.


# check how many wines of different qualities there are

# percentage of wines of each quality
(data['quality'].value_counts() / len(data)).sort_index().plot.bar()

# plot
plt.title('Wine Quality')
plt.ylabel('Percentage of wines in the data')
plt.xlabel('Wine Quality')
plt.show()


# Most wines are medium to low quality. Only a few of high quality (>6)


# let's transform the target into binary

# wines with quality below 6 will be considered low quality (0)
data['quality'] = np.where(data['quality'] <= 6, 0, 1)

(data['quality'].value_counts() / len(data)).plot.bar()

plt.title('Wine Quality')
plt.ylabel('Percentage of wines in the data')
plt.xlabel('Wine Quality')
plt.show()


# let's explore variable distributions with histograms

data.hist(bins=50, figsize=(10,10))

plt.show()


# All variables are continuous.


# let's evaluate the mean variable value per wine quality

g = sns.PairGrid(data, x_vars=["quality"], y_vars=data.columns[0:-1])
g.map(sns.barplot)
plt.show()


# There doesn't seem to be a difference in pH between wines of low and high quality, but high quality wines tend to have more alcohol, for example.
#
# Similarly, good quality wines tend to have more sulphates but less free and total sulfur, a molecule that is part of the sulphates.
#
# Good quality wines tend to have more citric acid, yet surprisingly, the pH in good quality wines is not lower. So the pH must be equilibrated through something else, for example the sulphates.


# now let's explore the data with boxplots

# reorganise for plotting
df = data.melt(id_vars=['quality'])

# capture variables
cols = df.variable.unique()

# plot first 6 columns
g = sns.axisgrid.FacetGrid(df[df.variable.isin(cols[0:6])], col='variable', sharey=False)
g.map(sns.boxplot, 'quality','value')
plt.show()


# plot remaining columns
g = sns.axisgrid.FacetGrid(df[df.variable.isin(cols[6:])], col='variable', sharey=False)
g.map(sns.boxplot, 'quality','value')
plt.show()


data.head()


# the citric acid affects the pH of the wine

plt.scatter(data['citric acid'], data['pH'], c=data['quality'])
plt.xlabel('Citric acid')
plt.ylabel('pH')
plt.show()


# the sulphates may affect the pH of the wine

plt.scatter(data['sulphates'], data['pH'], c=data['quality'])
plt.xlabel('sulphates')
plt.ylabel('pH')
plt.show()


plt.scatter(data['sulphates'], data['citric acid'], c=data['quality'])
plt.xlabel('sulphates')
plt.ylabel('citric acid')
plt.show()


# Good quality wine tend to have more citric acid and more sulphate, thus similar pH.


# let's evaluate the relationship between some molecules and the density of the wine

g = sns.PairGrid(data, y_vars=["density"], x_vars=['chlorides','sulphates', 'residual sugar', 'alcohol'])
g.map(sns.regplot)
plt.show()


# ## Create additional variables
#
# Let's combine variables into new ones to capture additional information.


# combine fixed and volatile acidity to create total acidity
# and mean acidity

combinator = MathFeatures(
    variables=['fixed acidity', 'volatile acidity'],
    func = ['sum', 'mean'],
    new_variables_names = ['total_acidity', 'average_acidity']
)

data = combinator.fit_transform(data)

# note the new variables at the end of the dataframe
data.head()


# let's combine salts into total minerals and average minerals

combinator = MathFeatures(
    variables=['chlorides', 'sulphates'],
    func = ['sum', 'mean'],
    new_variables_names = ['total_minerals', 'average_minerals']
)

data = combinator.fit_transform(data)

# note the new variable at the end of the dataframe
data.head()


# let's determine the sulfur that is not free

combinator = RelativeFeatures(
    variables=['total sulfur dioxide'],
    reference=['free sulfur dioxide'],
    func=['sub'],
    # new_variables_names=['non_free_sulfur_dioxide']
)

data = combinator.fit_transform(data)

# note the new variable at the end of the dataframe
data.head()


# let's calculate the % of free sulfur

combinator = RelativeFeatures(
    variables=['free sulfur dioxide'],
    reference=['total sulfur dioxide'],
    func=['div'],
    # new_variables_names=['percentage_free_sulfur']
)

data = combinator.fit_transform(data)

# note the new variable at the end of the dataframe
data.head()


# let's determine from all free sulfur how much is as salt

combinator = RelativeFeatures(
    variables=['sulphates'],
    reference=['free sulfur dioxide'],
    func=['div'],
    # new_variables_names=['percentage_salt_sulfur']
)

data = combinator.fit_transform(data)

# note the new variable at the end of the dataframe
data.head()


data.columns


# now let's explore the new variables with boxplots

new_vars = [
    'total_acidity',
    'average_acidity',
    'total_minerals',
    'average_minerals',
    
    'total sulfur dioxide_sub_free sulfur dioxide',
    'free sulfur dioxide_div_total sulfur dioxide',
    'free sulfur dioxide_div_total sulfur dioxide',

    # 'non_free_sulfur_dioxide',
    # 'percentage_free_sulfur',
    # 'percentage_salt_sulfur'
]

# KeyError: "['non_free_sulfur_dioxide', 'percentage_free_sulfur', 
# 'percentage_salt_sulfur'] not in index"

# reorganise for plotting
df = data[new_vars+['quality']].melt(id_vars=['quality'])

# capture variables
cols = df.variable.unique()

# plot first 6 columns
g = sns.axisgrid.FacetGrid(df[df.variable.isin(cols)], col='variable', sharey=False)
g.map(sns.boxplot, 'quality','value')
plt.show()


# ## Machine Learning Pipeline
#
# Now we are going to carry out all variable creation within a Scikit-learn Pipeline and add a classifier at the end.


data = pd.read_csv('../data/winequality-red.csv', sep=';')

# make binary target
data['quality'] = np.where(data['quality'] <= 6, 0, 1)

# separate dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['quality'], axis=1),
    data['quality'],
    test_size=0.2,
    random_state=0)

X_train.shape, X_test.shape


pipe = Pipeline([
    # variable creation
    ('acidity', MathFeatures(
        variables=['fixed acidity', 'volatile acidity'],
        func = ['sum', 'mean'],
        new_variables_names = ['total_acidity', 'average_acidity']
        )
    ),
    
    ('total_minerals', MathFeatures(
        variables=['chlorides', 'sulphates'],
        func = ['sum', 'mean'],
        new_variables_names = ['total_minerals', 'average_minearals'],
        )
    ),
    
    ('non_free_sulfur', RelativeFeatures(
        variables=['total sulfur dioxide'],
        reference=['free sulfur dioxide'],
        func=['sub'],
        # new_variables_names=['non_free_sulfur_dioxide'],
        )
    ),
    
    ('perc_free_sulfur', RelativeFeatures(
        variables=['free sulfur dioxide'],
        reference=['total sulfur dioxide'],
        func=['div'],
        # new_variables_names=['percentage_free_sulfur'],
        )
    ),
    
    ('perc_salt_sulfur', RelativeFeatures(
        variables=['sulphates'],
        reference=['free sulfur dioxide'],
        func=['div'],
        # new_variables_names=['percentage_salt_sulfur'],
        )
    ),
    
    # =====  the machine learning model ====
    
    ('gbm', GradientBoostingClassifier(n_estimators=10, max_depth=2, random_state=1)),
])

# create new variables, and then train gradient boosting machine
# uses only the training dataset

pipe.fit(X_train, y_train)


# make predictions and determine model performance

# the pipeline takes in the raw data, creates all the new features and then
# makes the prediction with the model trained on the final subset of variables

# obtain predictions and determine model performance

pred = pipe.predict_proba(X_train)
print('Train roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))

pred = pipe.predict_proba(X_test)
print('Test roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))


# ## Feature importance


new_vars = ['total_acidity', 'average_acidity', 'total_minerals', 'average_minearals',
           'non_free_sulfur_dioxide', 'percentage_free_sulfur','percentage_salt_sulfur']


importance = pd.Series(pipe.named_steps['gbm'].feature_importances_)
importance.index = list(X_train.columns) + new_vars

importance.sort_values(ascending=False).plot.bar(figsize=(15,5))
plt.ylabel('Feature importance')
plt.show()


# We see that some of the variables that we created are somewhat important for the prediction, like average_minerals, total_minerals, and total and average acidity.
#
# That is all folks!
#
#
# ## References and further reading
#
# - [Feature-engine](https://feature-engine.readthedocs.io/en/latest/index.html), Python open-source library
# - [Python Feature Engineering Cookbook](https://www.packtpub.com/data/python-feature-engineering-cookbook)
#
# ## Other Kaggle kernels featuring Feature-engine
#
# - [Feature selection for bank customer satisfaction prediction](https://www.kaggle.com/solegalli/feature-selection-with-feature-engine)
# - [Feature engineering and selection for house price prediction](https://www.kaggle.com/solegalli/predict-house-price-with-feature-engine)



// ---------------------------------------------------

// predict-house-price-with-feature-engine.py
// pipelines/predict-house-price-with-feature-engine.py
# Generated from: predict-house-price-with-feature-engine.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# ## Regression
#
# In this lecture, I show how easy and practical is to engineer features in an entire dataset utilising Feature-engine and the scikit-learn pipeline.
#
# **We use the Ames House Prices dataset produced by Professor Dean De Cock:**
#
# Dean De Cock (2011) Ames, Iowa: Alternative to the Boston Housing
# Data as an End of Semester Regression Project, Journal of Statistics Education, Vol.19, No. 3
#
# http://jse.amstat.org/v19n3/decock.pdf
#
# https://www.tandfonline.com/doi/abs/10.1080/10691898.2011.11889627
#
# The version of the dataset used in this notebook can be obtained from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# for the model
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score, log_loss

# for feature engineering
from sklearn.preprocessing import StandardScaler
from feature_engine import imputation as mdi
from feature_engine import discretisation as dsc
from feature_engine import encoding as ce


# load training data
data = pd.read_csv('../data/house-prices/train.csv')


# make lists of variable types

categorical = [var for var in data.columns if data[var].dtype == 'O']

year_vars = [var for var in data.columns if 'Yr' in var or 'Year' in var]

discrete = [
    var for var in data.columns if data[var].dtype != 'O'
    and len(data[var].unique()) < 20 and var not in year_vars
]

numerical = [
    var for var in data.columns if data[var].dtype != 'O'
    if var not in discrete and var not in ['Id', 'SalePrice']
    and var not in year_vars
]


# some plots to get familiar with the variable distributions

sns.pairplot(data=data,
             y_vars=['SalePrice'],
             x_vars=['LotFrontage',
                     'LotArea',
                     'MasVnrArea',
                     'BsmtFinSF1',
                     'BsmtFinSF2', ])


sns.pairplot(data=data,
             y_vars=['SalePrice'],
             x_vars=['BsmtUnfSF',
                     'TotalBsmtSF',
                     '1stFlrSF',
                     '2ndFlrSF',
                     'LowQualFinSF', ])


sns.pairplot(data=data,
             y_vars=['SalePrice'],
             x_vars=['GrLivArea',
                     'GarageArea',
                     'WoodDeckSF',
                     'OpenPorchSF',
                     'EnclosedPorch', ])


sns.pairplot(data=data,
             y_vars=['SalePrice'],
             x_vars=['3SsnPorch',
                     'ScreenPorch',
                     'MiscVal'])


# we are going to treat discrete variables as categorical 
# thus, to allow Feature-engine to pick them up automatically
# we need to re-cast them as object

data[discrete] = data[discrete].astype('O')


# split training data into train and test

X_train, X_test, y_train, y_test = train_test_split(data.drop(
    ['Id', 'SalePrice'], axis=1),
    data['SalePrice'],
    test_size=0.1,
    random_state=0)

X_train.shape, X_test.shape


# transform year variables:
# calculate elapsed time

def elapsed_years(df, var):
    # capture difference between year variable and
    # year the house was sold
    
    df[var] = df['YrSold'] - df[var]
    return df

for var in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:
    X_train = elapsed_years(X_train, var)
    X_test = elapsed_years(X_test, var)


# drop YrSold

X_train.drop('YrSold', axis=1, inplace=True)
X_test.drop('YrSold', axis=1, inplace=True)


house_pipe = Pipeline([

    # ===  missing data imputation =======
    # add missing indicator to variables that show NA
    ('missing_ind', mdi.AddMissingIndicator(missing_only=True)),

    # impute numerical variables with the median - vars automatically identified
    ('imputer_num',  mdi.MeanMedianImputer(imputation_method='median')),

    # impute categorical variables with a string, vars automatically identified
    # with return_object set to true, the numerical variables are cast as object
    # so that the encoders can identify them automatically
    ('imputer_cat', mdi.CategoricalImputer(return_object=True)),


    # === categorical encoding =========
    # group infrequent labels into a group, called "Rare"
    # categorical variables automatically identified
    ('rare_label_enc', ce.RareLabelEncoder(tol=0.1, n_categories=1)),

    # encode categories with the predictions from a tree
    # categorical variables automatically identified
    ('categorical_enc', ce.DecisionTreeEncoder(
        param_grid={'max_depth': [1, 2,3]},
        random_state=2909)),

    # === discretisation =====
    # transform numerical variables into tree predictions
    # need to specify variable names, because by now, all variables
    # will be numerical. Otherwise transformer will transform all
    ('discretisation', dsc.DecisionTreeDiscretiser(
        param_grid={'max_depth': [1, 2, 3]},
        random_state=2909,
        variables=numerical)),

    # feature Scaling
    ('scaler', StandardScaler()),

    # regression
    ('lasso', Lasso(alpha=100, random_state=0, max_iter=1000)),
])


# let's fit the pipeline
house_pipe.fit(X_train, y_train)

# let's get the predictions
X_train_preds = house_pipe.predict(X_train)
X_test_preds = house_pipe.predict(X_test)


# check model performance:

print('train mse: {}'.format(mean_squared_error(y_train, X_train_preds, squared=True)))
print('train rmse: {}'.format(mean_squared_error(y_train, X_train_preds, squared=False)))
print('train r2: {}'.format(r2_score(y_train, X_train_preds)))
print()
print('test mse: {}'.format(mean_squared_error(y_test, X_test_preds,squared=True)))
print('test rmse: {}'.format(mean_squared_error(y_test, X_test_preds, squared=False)))
print('test r2: {}'.format(r2_score(y_test, X_test_preds)))



// ---------------------------------------------------

// adult-income-with-feature-engine.py
// pipelines/adult-income-with-feature-engine.py
# Generated from: adult-income-with-feature-engine.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Feature creation within a Pipeline
#
# This notebook shows the creation of new features with Feature-engine and the scikit-learn pipeline.
#
# The notebook uses data from UCI Dataset called "Adult Income"
#
# The data is publicly available on [UCI repository](https://archive.ics.uci.edu/ml/datasets/adult)
#
# Donor:
#
# Ronny Kohavi and Barry Becker
# Data Mining and Visualization
# Silicon Graphics.
# e-mail: ronnyk '@' live.com for questions.


# Import packages


import feature_engine
feature_engine.__version__


# for data processing and visualisation
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# for the model
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, f1_score

# for feature engineering
from sklearn.preprocessing import StandardScaler
from feature_engine import imputation as mdi
from feature_engine import discretisation as dsc
from feature_engine import encoding as ce


# Load dataset

filename = '../data/adult/adult.data'
col_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain','capital-loss', 'hours-per-week', 'native-country','income']
data = pd.read_csv(filename, sep=',', names=col_names)

print(data.shape)

data.head()


# inspect data types

data.info()


# make lists of variable types

categorical = [var for var in data.columns if data[var].dtype == 'O']
discrete = [var for var in data.columns if data[var].dtype != 'O']


categorical


discrete


# histograms of discrete variables

data[discrete].hist(bins=30, figsize=(15,15))
plt.show()


# plot of categoricals

for var in categorical:
    # sns.catplot(data=data, y=var, kind="count", palette="ch:.25")
    sns.catplot(data=data, y=var, hue=var,kind="count", palette="ch:.25", legend=False)


# transform the income value (target variable)


data['income'] = data.income.apply(lambda x: x.replace("<=50K","0"))


data['income'] = data.income.apply(lambda x: x.replace(">50K","1"))


data['income'] = data.income.apply(lambda x: int(x))


data.head()


# split training data into train and test

X_train, X_test, y_train, y_test = train_test_split(data.drop(
    ['income'], axis=1),
    data['income'],
    test_size=0.1,
    random_state=42)

X_train.shape, X_test.shape


# remove 'income' from the categorical list

categorical.pop()


# build the pipeline

income_pipe = Pipeline([

    # === rare label encoding =========
    ('rare_label_enc', ce.RareLabelEncoder(tol=0.1, n_categories=1)),

    # === encoding categories ===
    ('categorical_enc', ce.DecisionTreeEncoder(regression=False,
        param_grid={'max_depth': [1, 2,3]},
        random_state=2909,
        variables=categorical)),

    # === discretisation =====
    ('discretisation', dsc.DecisionTreeDiscretiser(regression=False,
        param_grid={'max_depth': [1, 2, 3]},
        random_state=2909,
        variables=discrete)),

    # classification
    ('gbm', GradientBoostingClassifier(random_state=42))
])


# fit the pipeline

income_pipe.fit(X_train, y_train)


# extract predictions

X_train_preds = income_pipe.predict(X_train)
X_test_preds = income_pipe.predict(X_test)


# show model performance:

print('train accuracy: {}'.format(accuracy_score(y_train, X_train_preds)))
print()
print('test accuracy: {}'.format(accuracy_score(y_test, X_test_preds)))



// ---------------------------------------------------

// CategoricalImputer.py
// imputation/CategoricalImputer.py
# Generated from: CategoricalImputer.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Missing value imputation: CategoricalImputer
#
#
# CategoricalImputer performs imputation of categorical variables. It replaces missing values by an arbitrary label "Missing" (default) or any other label entered by the user. Alternatively, it imputes missing data with the most frequent category.
#
# **For this demonstration, we use the Ames House Prices dataset produced by Professor Dean De Cock:**
#
# [Dean De Cock (2011) Ames, Iowa: Alternative to the Boston Housing
# Data as an End of Semester Regression Project, Journal of Statistics Education, Vol.19, No. 3](http://jse.amstat.org/v19n3/decock.pdf)
#
# The version of the dataset used in this notebook can be obtained from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)


# ## Version


# Make sure you are using this 
# Feature-engine version.

import feature_engine

feature_engine.__version__


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from  feature_engine.imputation import CategoricalImputer


# ## Load data


# # Download the data from Kaggle and store it in the same folder as this notebook.
# data = pd.read_csv('../data/housing.csv')
# data.head()

# # Separate the data into train and test sets.
# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1),
#     data['SalePrice'],
#     test_size=0.3,
#     random_state=0,
# )

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# ## Check missing data


# These are categorical variables with missing data

X_train[['Alley', 'MasVnrType']].isnull().mean()


# Number of observations per category

X_train['MasVnrType'].value_counts().plot.bar()
plt.ylabel('Number of observations')
plt.title('MasVnrType')


# ## Imputat with string missing
#
# We replace missing data with the string "Missing".


imputer = CategoricalImputer(
    imputation_method='missing',
    variables=['Alley', 'MasVnrType'])

imputer.fit(X_train)


# We impute all variables with the
# string 'Missing'

imputer.imputer_dict_


# Perform imputation.

train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)


# Observe the new category 'Missing'

test_t['MasVnrType'].value_counts().plot.bar()

plt.ylabel('Number of observations')
plt.title('Imputed MasVnrType')


test_t['Alley'].value_counts().plot.bar()

plt.ylabel('Number of observations')
plt.title('Imputed Alley')


# ## Impute with another string
#
# We can also enter a specific string for the imputation instead of the default 'Missing'.


imputer = CategoricalImputer(
    variables='MasVnrType',
    fill_value="this_is_missing",
)


# We can also fit and transform the train set
# in one line of code
train_t = imputer.fit_transform(X_train)


# and then transform the test set
test_t = imputer.transform(X_test)


# let's check the current imputation
# dictionary

imputer.imputer_dict_


# After the imputation we see the new category

test_t['MasVnrType'].value_counts().plot.bar()

plt.ylabel('Number of observations')
plt.title('Imputed MasVnrType')


# ## Frequent Category Imputation
#
# We can also replace missing values with the most frequent category.


imputer = CategoricalImputer(
    imputation_method='frequent',
    variables=['Alley', 'MasVnrType'],
)


# Find most frequent category

imputer.fit(X_train)


# In this attribute we find the most frequent category
# per variable to impute.

imputer.imputer_dict_


# Impute variables
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)


# Let's count the number of observations per category
# in the original variable.

X_train['MasVnrType'].value_counts()


# note that we have a few more observations in the 
# most frequent category, which for this variable
# is 'None', after the transformation.

train_t['MasVnrType'].value_counts()


# The number of observations for `None` in `MasVnrType` increased from 609 to 614, thanks to replacing the NA with this label.


# ## Automatically select categorical variables
#
# We can impute all catetgorical variables automatically, either with a string or with the most frequent category.
#
# To do so, we need to leave the parameter `variables` to `None`.


# Impute all categorical variables with 
# the most frequent category

imputer = CategoricalImputer(imputation_method='frequent')


# with fit, the transformer identifies the categorical variables
# in the train set, and their most frequent category.
imputer.fit(X_train)

# Here we find the imputation values for each
# categorical variable.

imputer.imputer_dict_


# With transform we replace missing data.

train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)


# Sanity check:

# No categorical variable with NA is left in the
# transformed data.

[v for v in train_t.columns if train_t[v].dtypes ==
    'O' and train_t[v].isnull().sum() > 1]


# We can also return the name of the final features in
# the transformed data
imputer.get_feature_names_out()



// ---------------------------------------------------

// DropMissingData.py
// imputation/DropMissingData.py
# Generated from: DropMissingData.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Missing value imputation: DropMissingData
#
# Deletes rows with missing values.
#
# DropMissingData works both with numerical and categorical variables.
#
# **For this demonstration, we use the Ames House Prices dataset produced by Professor Dean De Cock:**
#
# [Dean De Cock (2011) Ames, Iowa: Alternative to the Boston Housing
# Data as an End of Semester Regression Project, Journal of Statistics Education, Vol.19, No. 3](http://jse.amstat.org/v19n3/decock.pdf)
#
# The version of the dataset used in this notebook can be obtained from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)


# ## Version


# Make sure you are using this 
# Feature-engine version.

import feature_engine

feature_engine.__version__


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from feature_engine.imputation import DropMissingData


# # Download the data from Kaggle and store it in the same folder as this notebook.
# data = pd.read_csv('../data/housing.csv')
# data.head()

# # Separate the data into train and test sets.
# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1),
#     data['SalePrice'],
#     test_size=0.3,
#     random_state=0,
# )

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# ## Drop data based on specific variables.
#
# We can drop observations that show NA in any of a subset of variables.


# Drop data when there are NA in any of the indicated variables

imputer = DropMissingData(
    variables=['Alley', 'MasVnrType', 'LotFrontage', 'MasVnrArea'],
    missing_only=False,
)


imputer.fit(X_train)


# variables from which observations with NA will be deleted

imputer.variables_


# Number of observations with NA before the transformation

X_train[imputer.variables].isna().sum()


# After the transformation the rows with NA values are 
# deleted form the dataframe

train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)


# Number of observations with NA after transformation

train_t[imputer.variables].isna().sum()


# Shape of dataframe before transformation

X_train.shape


# Shape of dataframe after transformation

train_t.shape


# The "return_na_data()" method, returns a dataframe that contains
# the observations with NA. 

# That is, the portion of the data that is dropped when
# we apply the transform() method.

tmp = imputer.return_na_data(X_train)

tmp.shape


# total obs - obs with NA = final dataframe shape
#  after the transformation

1022-963


# Sometimes, it is useful to retain the observation with NA in the production environment, to log which
# observations are not being scored by the model for example.


# ## Drop data when variables contain %  of NA
#
# We can drop observations if they contain less than a required percentage of values in a subset of observations.


# Drop data if an observation contains NA in 
# 2 of the 4 indicated variables (50%).

imputer = DropMissingData(
    variables=['Alley', 'MasVnrType', 'LotFrontage', 'MasVnrArea'],
    missing_only=False,
    threshold=0.5,
)


imputer.fit(X_train)


# After the transformation the rows with NA values are 
# deleted form the dataframe

train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)


# Number of observations with NA after transformation

train_t[imputer.variables].isna().sum()


# We see that not all missing observations were dropped, because we required the observation to have NA in more than 1 of the variables at the time. 


# ## Automatically select all variables
#
# We can drop obserations if they show NA in any variable in the dataset.
#
# When the parameter `variables` is left to None and the parameter `missing_only` is left to True, the imputer will evaluate observations based of all variables with missing data.
#
# When the parameter `variables` is left to None and the parameter `missing_only` is switched to False, the imputer will evaluate observations based of all variables.
#
# It is good practice to use `missing_only=True` when we set `variables=None`, so that the transformer handles the imputation automatically in a meaningful way.
#
# ### Automatically find variables with NA


# Find variables with NA

imputer = DropMissingData(missing_only=True)

imputer.fit(X_train)


# variables with NA in the train set

imputer.variables_


# Number of observations with NA

X_train[imputer.variables_].isna().sum()


# After the transformation the rows with NA are deleted form the dataframe

train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)


# Number of observations with NA after the transformation

train_t[imputer.variables_].isna().sum()


# in this case, all observations will be dropped
# because all of them show NA at least in 1 variable

train_t.shape


# ## Drop rows with % of missing data
#
# Not to end up with an empty dataframe, let's drop rows that have less than 75% of the variables with values.


# Find variables with NA

imputer = DropMissingData(
    missing_only=True,
    threshold=0.75,
)

imputer.fit(X_train)


# After the transformation the rows with NA are deleted form the dataframe

train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)


train_t.shape


# Now, we do have some data left.



// ---------------------------------------------------

// EndTailImputer.py
// imputation/EndTailImputer.py
# Generated from: EndTailImputer.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # EndTailImputer
#
# The EndTailImputer() replaces missing data by a value at either tail of the distribution. It automatically determines the value to be used in the imputation using the mean plus or minus a factor of the standard deviation, or using the inter-quartile range proximity rule. Alternatively, it can use a factor of the maximum value.
#
# The EndTailImputer() is in essence, very similar to the ArbitraryNumberImputer, but it selects the value to use fr the imputation automatically, instead of having the user pre-define them.
#
# It works only with numerical variables.
#
# **For this demonstration, we use the Ames House Prices dataset produced by Professor Dean De Cock:**
#
# [Dean De Cock (2011) Ames, Iowa: Alternative to the Boston Housing
# Data as an End of Semester Regression Project, Journal of Statistics Education, Vol.19, No. 3](http://jse.amstat.org/v19n3/decock.pdf)
#
# The version of the dataset used in this notebook can be obtained from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)


# ## Version


# Make sure you are using this 
# Feature-engine version.

import feature_engine

feature_engine.__version__


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from feature_engine.imputation import EndTailImputer


# ## Load data


# # Download the data from Kaggle and store it in the same folder as this notebook.
# data = pd.read_csv('../data/housing.csv')
# data.head()

# # Separate the data into train and test sets.
# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1),
#     data['SalePrice'],
#     test_size=0.3,
#     random_state=0,
# )

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# ## Check missing data


# numerical variables with missing data

X_train[['LotFrontage', 'MasVnrArea']].isnull().mean()


# The EndTailImputer can replace NA with a value at the left or right end of the distribution.
#
# In addition, it uses 3 different methods to identify the imputation values.
#
# In the following cells, we show how to use each method.
#
# ## Gaussian, right tail
#
# Let's begin by finding the values automatically at the right tail, by using the mean and the standard deviation.


imputer = EndTailImputer(
    # uses mean and standard deviation to determine the value
    imputation_method='gaussian',
    # value at right tail of distribution
    tail='right',
    # multiply the std by 3
    fold=3,
    # the variables to impute
    variables=['LotFrontage', 'MasVnrArea'],
)


# find the imputation values
imputer.fit(X_train)


# The values for the imputation
imputer.imputer_dict_


# Note that we use different values for different variables.


# impute the data
train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)


# check we no longer have NA
train_t['LotFrontage'].isnull().sum()


# The variable distribution changed slightly with more values accumulating towards the right tail
fig = plt.figure()
ax = fig.add_subplot(111)
X_train['LotFrontage'].plot(kind='kde', ax=ax)
train_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')


# ## IQR, left tail
#
# Now, we will impute variables with values at the left tail. The values are identified using the inter-quartile range proximity rule. 
#
# The IQR rule is better suited for skewed variables.


imputer = EndTailImputer(
    
    # uses the inter-quartile range proximity rule
    imputation_method='iqr',
    
    # determines values at the left tail of the distribution
    tail='left',
    
    # multiplies the IQR by 3
    fold=3,
    
    # the variables to impute
    variables=['LotFrontage', 'MasVnrArea'],
)


# finds the imputation values

imputer.fit(X_train)


# imputation values per variable

imputer.imputer_dict_


# transform the data

train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)


# Check we have no NA after the transformation

train_t[['LotFrontage', 'MasVnrArea']].isnull().sum()


# The variable distribution changed with the
# transformation, with more values
# accumulating towards the left tail.

fig = plt.figure()
ax = fig.add_subplot(111)
X_train['LotFrontage'].plot(kind='kde', ax=ax)
train_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')


# ## Impute with the maximum value
#
# We can find imputation values with a factor of the maximum variable value.


imputer = EndTailImputer(
    
    # imputes beyond the maximum value
    imputation_method='max',
    
    # multiplies the maximum value by 3
    fold=3,
    
    # the variables to impute
    variables=['LotFrontage', 'MasVnrArea'],
)


# find imputation values

imputer.fit(X_train)


# The imputation values.

imputer.imputer_dict_


# the maximum values of the variables,
# note how the imputer multiplied them by 3
# to determine the imputation values.

X_train[imputer.variables_].max()


# impute the data

train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)


# Check we have no NA  in the imputed data

train_t[['LotFrontage', 'MasVnrArea']].isnull().sum()


# The variable distribution changed with the
# transformation, with now more values
# beyond the maximum.

fig = plt.figure()
ax = fig.add_subplot(111)
X_train['LotFrontage'].plot(kind='kde', ax=ax)
train_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')


# ## Automatically impute all variables
#
# As with all Feature-engine transformers, the EndTailImputer can also find and impute all numerical variables in the data.


# Start the imputer

imputer = EndTailImputer()


# Check the default parameters

# how to find the imputation value
imputer.imputation_method


# which tail to use

imputer.tail


# how far out
imputer.fold


# Find variables and imputation values

imputer.fit(X_train)


# The variables to impute

imputer.variables_


#  The imputation values

imputer.imputer_dict_


# impute the data

train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)


# Sanity check:

# No numerical variable with NA is  left in the
# transformed data.

[v for v in train_t.columns if train_t[v].dtypes !=
    'O' and train_t[v].isnull().sum() > 1]



// ---------------------------------------------------

// RandomSampleImputer.py
// imputation/RandomSampleImputer.py
# Generated from: RandomSampleImputer.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Missing value imputation: RandomSampleImputer
#
#
# The RandomSampleImputer extracts a random sample of observations where data is available, and uses it to replace the NA. It is suitable for numerical and categorical variables.
#
# To control the random sample extraction, there are various ways to set a seed and ensure or maximize reproducibility.
#
#
# **For this demonstration, we use the Ames House Prices dataset produced by Professor Dean De Cock:**
#
# [Dean De Cock (2011) Ames, Iowa: Alternative to the Boston Housing
# Data as an End of Semester Regression Project, Journal of Statistics Education, Vol.19, No. 3](http://jse.amstat.org/v19n3/decock.pdf)
#
# The version of the dataset used in this notebook can be obtained from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)


# ## Version


# Make sure you are using this 
# Feature-engine version.

import feature_engine

feature_engine.__version__


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from feature_engine.imputation import RandomSampleImputer


# # Download the data from Kaggle and store it in the same folder as this notebook.
# data = pd.read_csv('../data/housing.csv')
# data.head()

# # Separate the data into train and test sets.
# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1),
#     data['SalePrice'],
#     test_size=0.3,
#     random_state=0,
# )

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# ## Imputation in batch
#
# We can set the imputer to impute several observations in batch with a unique seed. This is the equivalent of setting the `random_state` to an integer in `pandas.sample()`.


# Start the imputer

imputer = RandomSampleImputer(

    # the variables to impute
    variables=['Alley', 'MasVnrType', 'LotFrontage', 'MasVnrArea'],

    # the random state for reproducibility
    random_state=10,

    # equialent to setting random_state in
    # pandas.sample()
    seed='general',
)


# Stores a copy of the train set variables

imputer.fit(X_train)


# the imputer saves a copy of the variables 
# from the training set to impute new data.

imputer.X_.head()


# Check missing data in train set

X_train[['Alley', 'MasVnrType', 'LotFrontage', 'MasVnrArea']].isnull().mean()


# impute data

train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)


# Check missing data after the transformation

train_t[['Alley', 'MasVnrType', 'LotFrontage', 'MasVnrArea']].isnull().mean()


# when using the random sample imputer, 
# the distribution of the variable does not change.

# This imputation method is useful for models that 
# are sensitive to changes in the variable distributions.

fig = plt.figure()
ax = fig.add_subplot(111)
X_train['LotFrontage'].plot(kind='kde', ax=ax)
train_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')


# ## Specific seeds for each observation
#
# Sometimes, we want to guarantee that the same observation is imputed with the same value, run after run. 
#
# To achieve this, we need to always use the same seed for every particular observation. 
#
# To do this, we can use the values in neighboring variables as seed.
#
# In this case, the seed will be calculated observation per observation, either by adding or multiplying the seeding variable values, and passed to the random_state of pandas.sample(), which is used under the hood by the imputer.
# Then, a value will be extracted from the train set using that seed and  used to replace the NAN in particular observation.
#
# **To know more about how the observation per seed is used check this [notebook](https://github.com/solegalli/feature-engineering-for-machine-learning/blob/master/Section-04-Missing-Data-Imputation/04.07-Random-Sample-Imputation.ipynb)** 


imputer = RandomSampleImputer(

    # the values of these variables will be used as seed
    random_state=['MSSubClass', 'YrSold'],

    # 1 seed per observation
    seed='observation',

    # how to combine the values of the seeding variables
    seeding_method='add',
    
    # impute all variables, numerical and categorical
    variables=None,
)


# Stores a copy of the train set.

imputer.fit(X_train)


# takes a copy of the entire train set

imputer.X_


# imputes all variables.

# this procedure takes a while because it is 
# done observation per observation.

train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)


# No missing data in any variable
# after the imputation.

test_t.isnull().sum()


# when using the random sample imputer, 
# the distribution of the variable does not change

fig = plt.figure()
ax = fig.add_subplot(111)
X_train['LotFrontage'].plot(kind='kde', ax=ax)
train_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')



// ---------------------------------------------------

// ArbitraryNumberImputer.py
// imputation/ArbitraryNumberImputer.py
# Generated from: ArbitraryNumberImputer.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # ArbitraryNumberImputer
#
#
# ArbitraryNumberImputer replaces NA by an arbitrary value. It works for numerical variables. The arbitrary value needs to be defined by the user.
#
# **For this demonstration, we use the Ames House Prices dataset produced by Professor Dean De Cock:**
#
# [Dean De Cock (2011) Ames, Iowa: Alternative to the Boston Housing
# Data as an End of Semester Regression Project, Journal of Statistics Education, Vol.19, No. 3](http://jse.amstat.org/v19n3/decock.pdf)
#
# The version of the dataset used in this notebook can be obtained from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)


# ## Version


# Make sure you are using this 
# Feature-engine version.

import feature_engine

feature_engine.__version__


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from feature_engine.imputation  import ArbitraryNumberImputer


# ## Load data


# # Download the data from Kaggle and store it in the same folder as this notebook.
# data = pd.read_csv('../data/housing.csv')
# data.head()

# # Separate the data into train and test sets.
# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1),
#     data['SalePrice'],
#     test_size=0.3,
#     random_state=0,
# )

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# ## Imputate variables with same number
#
# We will impute 2 numerical variables with the number 999.


# Check missing data

X_train[['LotFrontage', 'MasVnrArea']].isnull().mean()


# Let's create an instance of the imputer where we impute
# 2 variables with the same arbitraty number.

imputer = ArbitraryNumberImputer(
    arbitrary_number=-999,
    variables=['LotFrontage', 'MasVnrArea'],
)

imputer.fit(X_train)


# The number to use in the imputation
# is stored as parameter.

imputer.arbitrary_number


# The imputer will use the same value to impute
# all indicated variables.

imputer.imputer_dict_


# Impute variables

train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)

# Sanity check: the min value is the one used for 
#  the imputation

train_t[['LotFrontage', 'MasVnrArea']].min()


# The distribution of the variable
# changed with the transformation.

fig = plt.figure()
ax = fig.add_subplot(111)
X_train['LotFrontage'].plot(kind='kde', ax=ax)
train_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')


# ### Impute variables with different numbers
#
# We can also impute different variables with different values. In this case, we need to start the transformer with a dictionary of variable to value pairs.


# Impute different variables with different values

imputer = ArbitraryNumberImputer(
    imputer_dict={"LotFrontage": -678, "MasVnrArea": -789}
)

imputer.fit(X_train)


# In this case, the imputer_dict_ matches the 
# entered dictionary.

imputer.imputer_dict_


# Now we impute the missing data

train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)

# Sanity check: check minimum values

train_t[['LotFrontage', 'MasVnrArea']].min()


# The distribution of the variable changed
# after the transformation.

fig = plt.figure()
ax = fig.add_subplot(111)
X_train['LotFrontage'].plot(kind='kde', ax=ax)
train_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')


# ## Automatically select all variables
#
# We can impute all numerical variables with the same value automatically with this transformer. We need to leave the  parameter `variables` to None.


# Let's create an instance of the imputer where we impute
# 2 variables with the same arbitraty number.

imputer = ArbitraryNumberImputer(
    arbitrary_number=-1,
)

imputer.fit(X_train)


# The imputer finds all numerical variables
# automatically.

imputer.variables_


# We find the imputation value in the dictionary

imputer.imputer_dict_


# now we impute the missing data

train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)


# Sanity check:

# No numerical variable with NA is  left in the
# transformed data.

[v for v in train_t.columns if train_t[v].dtypes !=
    'O' and train_t[v].isnull().sum() > 1]


# New: we can get the name of the features in the final output
imputer.get_feature_names_out()



// ---------------------------------------------------

// AddMissingIndicator.py
// imputation/AddMissingIndicator.py
# Generated from: AddMissingIndicator.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # AddMissingIndicator
#
#
# AddMissingIndicator adds additional binary variables indicating missing data (thus, called missing indicators). The binary variables take the value 1 if the observation's value is missing, or 0 otherwise. AddMissingIndicator adds 1 binary variable per variable.
#
# **For this demonstration, we use the Ames House Prices dataset produced by Professor Dean De Cock:**
#
# [Dean De Cock (2011) Ames, Iowa: Alternative to the Boston Housing
# Data as an End of Semester Regression Project, Journal of Statistics Education, Vol.19, No. 3](http://jse.amstat.org/v19n3/decock.pdf)
#
# The version of the dataset used in this notebook can be obtained from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)


# ## Version


# Make sure you are using this 
# Feature-engine version.

import feature_engine

feature_engine.__version__


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline

from feature_engine.imputation import (
    AddMissingIndicator,
    MeanMedianImputer,
    CategoricalImputer,
)


# ## Load data


# # Download the data from Kaggle and store it in the same folder as this notebook.
# data = pd.read_csv('../data/housing.csv')
# data.head()

# # Separate the data into train and test sets.
# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1),
#     data['SalePrice'],
#     test_size=0.3,
#     random_state=0,
# )

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# ## Add indicators
#
# We will add indicators to 4 variables with missing data.


# Check missing data

X_train[['Alley', 'MasVnrType', 'LotFrontage', 'MasVnrArea']].isnull().mean()


# Start the imputer with the variables for which
# we want indicators.

imputer = AddMissingIndicator(
    variables=['Alley', 'MasVnrType', 'LotFrontage', 'MasVnrArea'],
)

imputer.fit(X_train)


# the variables for which missing 
# indicators will be added.

imputer.variables_


# Check the added indicators. They take the name of
# the variable underscore na

train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)

train_t[['Alley_na', 'MasVnrType_na', 'LotFrontage_na', 'MasVnrArea_na']].head()


# Note that the original variables still have missing data.

train_t[['Alley_na', 'MasVnrType_na', 'LotFrontage_na', 'MasVnrArea_na']].mean()


# ## Indicators plus imputation
#
# We normally add missing indicators and impute the original variables with the mean or median if the variable is numerical, or with the mode if the variable is categorical. So let's do that.


# Check variable types

X_train[['Alley', 'MasVnrType', 'LotFrontage', 'MasVnrArea']].dtypes


# The first 2 variables are categorical, so I will impute them with the most frequent category. The last variables are numerical, so I will impute with the median.


# Create a pipeline with the imputation strategy

pipe = Pipeline([
    ('indicators', AddMissingIndicator(
        variables=['Alley', 'MasVnrType',
                   'LotFrontage', 'MasVnrArea'],
    )),

    ('imputer_num', MeanMedianImputer(
        imputation_method='median',
        variables=['LotFrontage', 'MasVnrArea'],
    )),

    ('imputer_cat', CategoricalImputer(
        imputation_method='frequent',
        variables=['Alley', 'MasVnrType'],
    )),
])


# With fit() the transformers learn the 
# required parameters.

pipe.fit(X_train)


# We can look into the attributes of the
# different transformers.

# Check the variables that will take indicators.
pipe.named_steps['indicators'].variables_


# Check the median values for the imputation.

pipe.named_steps['imputer_num'].imputer_dict_


# Check the mode values for the imputation.

pipe.named_steps['imputer_cat'].imputer_dict_


# Now, we transform the data.

train_t = pipe.transform(X_train)
test_t = pipe.transform(X_test)


# Lets' look at the transformed variables.

# original variables plus indicators
vars_ = ['Alley', 'MasVnrType', 'LotFrontage', 'MasVnrArea',
         'Alley_na', 'MasVnrType_na', 'LotFrontage_na', 'MasVnrArea_na']

train_t[vars_].head()


# After the transformation, the variables do not
# show missing data

train_t[vars_].isnull().sum()


# ## Automatically select the variables
#
# We have the option to add indicators to all variables in the dataset, or to all variables with missing data. AddMissingIndicator can select which variables to transform automatically.
#
# When the parameter `variables` is left to None and the parameter `missing_only` is left to True, the imputer add indicators to all variables with missing data.
#
# When the parameter `variables` is left to None and the parameter `missing_only` is switched to False, the imputer add indicators to all variables.
#
# It is good practice to use `missing_only=True` when we set `variables=None`, so that the transformer handles the imputation automatically in a meaningful way.
#
# ### Automatically find variables with NA


# With missing_only=True, missing indicators will only be added
# to those variables with missing data found during the fit method
# in the train set


imputer = AddMissingIndicator(
    variables=None,
    missing_only=True,
)

# finds variables with missing data
imputer.fit(X_train)


# The original variables argument was None

imputer.variables


# In variables_ we find the list of variables with NA
# in the train set

imputer.variables_


len(imputer.variables_)


# We've got 19 variables with NA in the train set.


# After transforming the dataset, we see more columns
# corresponding to the missing indicators.

train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)

X_train.shape, train_t.shape


# Towards the right, we find the missing indicators.

train_t.head()


# ## Add indicators to all variables


# We can, in practice, set up the indicator to add
# missing indicators to all variables

imputer = AddMissingIndicator(
    variables=None,
    missing_only=False,
)

imputer.fit(X_train)


# the attribute variables_ now shows all variables
# in the train set.

len(imputer.variables_)


# After transforming the dataset,
# we obtain double the number of columns

train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)

X_train.shape, train_t.shape


# ## Automatic imputation
#
# We can automatically impute missing data in numerical and categorical variables, letting the imputers  find out which variables to impute.
#
# We need to set the parameter variables to None in all imputers. None is the default value, so we can simply omit the parameter when initialising the transformers.


# Create a pipeline with the imputation strategy

pipe = Pipeline([
    # add indicators to variables with NA
    ('indicators', AddMissingIndicator(
        missing_only=True,
    )),
    # impute all numerical variables with the median
    ('imputer_num', MeanMedianImputer(
        imputation_method='median',
    )),
    # impute all categorical variables with the mode
    ('imputer_cat', CategoricalImputer(
        imputation_method='frequent',
    )),
])


# With fit() the transformers learn the required parameters.
pipe.fit(X_train)


# We can look into the attributes of the different transformers.
# Check the variables that will take indicators.
pipe.named_steps['indicators'].variables_


# Check the median values for the imputation.
pipe.named_steps['imputer_num'].imputer_dict_


# Check the mode values for the imputation.
pipe.named_steps['imputer_cat'].imputer_dict_


# Now, we transform the data.
train_t = pipe.transform(X_train)
test_t = pipe.transform(X_test)


# We should see a complete case dataset
train_t.isnull().sum()


# Sanity check
[v for v in train_t.columns if train_t[v].isnull().sum() > 1]



// ---------------------------------------------------

// MeanMedianImputer.py
// imputation/MeanMedianImputer.py
# Generated from: MeanMedianImputer.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Missing value imputation: MeanMedianImputer
#
# The MeanMedianImputer() replaces missing data by the mean or median value of the variable. 
#
# It works only with numerical variables.
#
# **For this demonstration, we use the Ames House Prices dataset produced by Professor Dean De Cock:**
#
# [Dean De Cock (2011) Ames, Iowa: Alternative to the Boston Housing
# Data as an End of Semester Regression Project, Journal of Statistics Education, Vol.19, No. 3](http://jse.amstat.org/v19n3/decock.pdf)
#
# The version of the dataset used in this notebook can be obtained from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)


# ## Version


# Make sure you are using this 
# Feature-engine version.

import feature_engine

feature_engine.__version__


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from  feature_engine.imputation import MeanMedianImputer


# ## Load data


# # Download the data from Kaggle and store it in the same folder as this notebook.
# data = pd.read_csv('../data/housing.csv')
# data.head()

# # Separate the data into train and test sets.
# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1),
#     data['SalePrice'],
#     test_size=0.3,
#     random_state=0,
# )

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# ## Check missing data


# Numerical variables with missing data

X_train[['LotFrontage', 'MasVnrArea']].isnull().mean()


# ## Imputation with the median
#
# Let's start by imputing missing data in 2 variables with their median.


# Set up the imputer.

imputer = MeanMedianImputer(
    imputation_method='median',
    variables=['LotFrontage', 'MasVnrArea'],
)


# Find median values

imputer.fit(X_train)


# Dictionary with the imputation values for each variable.

imputer.imputer_dict_


# Let's corroborate that the dictionary 
# contains the median values of the variables.

X_train[['LotFrontage', 'MasVnrArea']].median()


# impute the data

train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)


# Check we no longer have NA

train_t[['LotFrontage', 'MasVnrArea']].isnull().sum()


# The variable distribution changed slightly with
# more values accumulating towards the median 
# after the imputation.

fig = plt.figure()
ax = fig.add_subplot(111)
X_train['LotFrontage'].plot(kind='kde', ax=ax)
train_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')


# ## Automatically select all numerical variables
#
# Let's now impute all numerical variables with the mean.
#
# If we leave the parameter `variables` to `None`, the transformer identifies and imputes all numerical variables.


# Set up the imputer

imputer = MeanMedianImputer(
    imputation_method='mean',
)


# Find numerical variables and their mean.

imputer.fit(X_train)


# Numerical variables identified.

imputer.variables_


# The imputation value, the mean, for each variable

imputer.imputer_dict_


# impute the data

train_t = imputer.transform(X_train)
test_t = imputer.transform(X_test)

# the numerical variables do not have NA after
# the imputation.

test_t[imputer.variables_].isnull().sum()



// ---------------------------------------------------

// ReciprocalTransformer.py
// transformation/ReciprocalTransformer.py
# Generated from: ReciprocalTransformer.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Variable transformers : ReciprocalTransformer
#
# The ReciprocalTransformer() applies the reciprocal transformation 1 / x
# to numerical variables.
#
# The ReciprocalTransformer() only works with numerical variables with non-zero
# values. If a variable contains the value  the transformer will raise an error.
#
# **For this demonstration, we use the Ames House Prices dataset produced by Professor Dean De Cock:**
#
# Dean De Cock (2011) Ames, Iowa: Alternative to the Boston Housing
# Data as an End of Semester Regression Project, Journal of Statistics Education, Vol.19, No. 3
#
# http://jse.amstat.org/v19n3/decock.pdf
#
# https://www.tandfonline.com/doi/abs/10.1080/10691898.2011.11889627
#
# The version of the dataset used in this notebook can be obtained from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

from feature_engine.imputation import ArbitraryNumberImputer
from feature_engine.transformation import ReciprocalTransformer


# # load data

# data = pd.read_csv('houseprice.csv')
# data.head()

# # let's separate into training and testing set

# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1), data['SalePrice'], test_size=0.3, random_state=0)

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# transform 2 variables

rt = ReciprocalTransformer(variables = ['LotArea', 'GrLivArea'])

rt.fit(X_train)


# variables to transform

rt.variables_


# transforming variables
train_t = rt.transform(X_train)
test_t = rt.transform(X_test)


# variable before transformation
X_train['GrLivArea'].hist(bins=50)
plt.title('Variable before transformation')
plt.xlabel('GrLivArea')


# transformed variable
train_t['GrLivArea'].hist(bins=50)
plt.title('Transformed variable')
plt.xlabel('GrLivArea')


# tvariable before transformation
X_train['LotArea'].hist(bins=50)
plt.title('Variable before transformation')
plt.xlabel('LotArea')


# transformed variable
train_t['LotArea'].hist(bins=50)
plt.title('Variable before transformation')
plt.xlabel('LotArea')


# return variables to original representation

train_orig = rt.inverse_transform(train_t)
test_orig = rt.inverse_transform(test_t)


# inverse transformed variable distribution

train_orig['LotArea'].hist(bins=50)


# inverse transformed variable distribution

train_orig['GrLivArea'].hist(bins=50)


# ## Automatically select numerical variables
#
# We cannot do reciprocal transformation when the variable values are zero so we will use only positive variables for this demo.


# load numerical variables only

variables = ['LotFrontage', 'LotArea',
             '1stFlrSF', 'GrLivArea',
             'TotRmsAbvGrd', 'SalePrice']


# data = pd.read_csv('houseprice.csv', usecols=variables)

# # let's separate into training and testing set

# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['SalePrice'], axis=1), data['SalePrice'], test_size=0.3, random_state=0)

# X_train.shape, X_test.shape




# Read the separate files - only reading the columns we need
train_df = pd.read_csv('../data/house-prices/train.csv', usecols=['Id'] + variables)
test_df = pd.read_csv('../data/house-prices/test.csv', usecols=['Id'] + variables[:-1])  # excluding SalePrice for test

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# Impute missing values

arbitrary_imputer = ArbitraryNumberImputer(arbitrary_number=2)

arbitrary_imputer.fit(X_train)

# impute variables
train_t = arbitrary_imputer.transform(X_train)
test_t = arbitrary_imputer.transform(X_test)


# reciprocal transformation

rt = ReciprocalTransformer()

rt.fit(train_t)


# variables to transform
rt.variables_


# before transforming 

train_t['GrLivArea'].hist(bins=50)


# before transforming 
train_t['LotArea'].hist(bins=50)


# transform variables
train_t = rt.transform(train_t)
test_t = rt.transform(test_t)


# transformed variable
train_t['GrLivArea'].hist(bins=50)


# transformed variable
train_t['LotArea'].hist(bins=50)


# return variables to original representation

train_orig = rt.inverse_transform(train_t)
test_orig = rt.inverse_transform(test_t)


# inverse transformed variable distribution

train_orig['LotArea'].hist(bins=50)


# inverse transformed variable distribution

train_orig['GrLivArea'].hist(bins=50)



// ---------------------------------------------------

// BoxCoxTransformer.py
// transformation/BoxCoxTransformer.py
# Generated from: BoxCoxTransformer.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Variable transformers : BoxCoxTransformer
#
# The BoxCoxTransformer() applies the BoxCox transformation to numerical
# variables.
#
# The Box-Cox transformation is defined as:
#
# - T(Y)=(Y exp(λ)−1)/λ if λ!=0
# - log(Y) otherwise
#
# where Y is the response variable and λ is the transformation parameter. λ varies,
# typically from -5 to 5. In the transformation, all values of λ are considered and
# the optimal value for a given variable is selected.
#
# **For this demonstration, we use the Ames House Prices dataset produced by Professor Dean De Cock:**
#
# Dean De Cock (2011) Ames, Iowa: Alternative to the Boston Housing
# Data as an End of Semester Regression Project, Journal of Statistics Education, Vol.19, No. 3
#
# http://jse.amstat.org/v19n3/decock.pdf
#
# https://www.tandfonline.com/doi/abs/10.1080/10691898.2011.11889627
#
# The version of the dataset used in this notebook can be obtained from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

from feature_engine.imputation import ArbitraryNumberImputer, CategoricalImputer
from feature_engine.transformation import BoxCoxTransformer


# #Read data
# data = pd.read_csv('houseprice.csv')
# data.head()

# # let's separate into training and testing set

# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1), data['SalePrice'], test_size=0.3, random_state=0)

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# let's transform 2 variables

bct = BoxCoxTransformer(variables = ['LotArea', 'GrLivArea'])

# find the optimal lambdas 
bct.fit(X_train)


# these are the exponents for the BoxCox transformation

bct.lambda_dict_


# transfor the variables

train_t = bct.transform(X_train)
test_t = bct.transform(X_test)


# variable before transformation
X_train['GrLivArea'].hist(bins=50)
plt.title('Variable before transformation')
plt.xlabel('GrLivArea')


# transformed variable
train_t['GrLivArea'].hist(bins=50)
plt.title('Transformed variable')
plt.xlabel('GrLivArea')


# tvariable before transformation
X_train['LotArea'].hist(bins=50)
plt.title('Variable before transformation')
plt.xlabel('LotArea')


# transformed variable
train_t['LotArea'].hist(bins=50)
plt.title('Variable before transformation')
plt.xlabel('LotArea')


# ## Automatically select numerical variables
#
# The transformer will transform all numerical variables if no variables are specified.


# load numerical variables only

variables = ['LotFrontage', 'LotArea',
             '1stFlrSF', 'GrLivArea',
             'TotRmsAbvGrd', 'SalePrice']


# data = pd.read_csv('houseprice.csv', usecols=variables)

# # let's separate into training and testing set
# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['SalePrice'], axis=1), data['SalePrice'], test_size=0.3, random_state=0)

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# Impute missing values

arbitrary_imputer = ArbitraryNumberImputer(arbitrary_number=2)

arbitrary_imputer.fit(X_train)

# impute variables
train_t = arbitrary_imputer.transform(X_train)
test_t = arbitrary_imputer.transform(X_test)


numeric_columns = train_t.select_dtypes(include=['int64', 'float64']).columns
numeric_columns


train_numeric = train_t[numeric_columns].copy()


for column in numeric_columns:
    min_val = train_numeric[column].min()
    if min_val <= 0:
        print(f"{column}: minimum value = {min_val}")
        shift = abs(min_val) + 1
        train_numeric[column] = train_numeric[column] + shift



train_numeric.describe()


# Check for extremely large values
for col in train_numeric.columns:
    q75 = train_numeric[col].quantile(0.75)
    q25 = train_numeric[col].quantile(0.25)
    iqr = q75 - q25
    upper_bound = q75 + 1.5 * iqr
    
    if train_numeric[col].max() > upper_bound:
        print(f"\n{col}:")
        print(f"Max value: {train_numeric[col].max()}")
        print(f"Upper bound: {upper_bound}")


# # Temp
# from feature_engine.transformation import YeoJohnsonTransformer
# from feature_engine.outliers import Winsorizer

# # First winsorize the outliers
# winsor = Winsorizer(capping_method='iqr', tail='both', fold=1.5)
# train_winsorized = winsor.fit_transform(train_numeric)

# # Then apply YeoJohnson transformation (which handles both positive and negative values)
# yjt = YeoJohnsonTransformer()
# train_transformed = yjt.fit_transform(train_winsorized)

# # Check the results
# print("\nSkewness before transformation:")
# print(train_numeric.skew())
# print("\nSkewness after transformation:")
# print(train_transformed.skew())


# let's transform all numerical variables

bct = BoxCoxTransformer()

# bct.fit(train_t)
bct.fit(train_numeric)


# variables that will be transformed

bct.variables_


# transform  variables
train_t = bct.transform(train_t)
test_t = bct.transform(test_t)


# learned parameters

bct.lambda_dict_



// ---------------------------------------------------

// LogCpTransformer.py
// transformation/LogCpTransformer.py
# Generated from: LogCpTransformer.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Variable transformers : LogCpTransformer
#
#
# The `LogCpTransformer()` applies the transformation log(x + C), where C is a positive constant, to the input variable. 
#
# It applies the natural logarithm or the base 10 logarithm, where the natural logarithm is logarithm in base e by setting the param `base="e"` or `base="10"`.
#
# The `LogCpTransformer()`  only works with numerical non-negative values after adding a constant C. If the variable contains a zero or a negative value after adding a constant C, the transformer will return an error.
#
# The transformer can automatically find the constant C to each variable by setting `C="auto"`.
#
# A list of variables can be passed as an argument. Alternatively, the transformer will automatically select and transform all variables of type numeric.
#
# In this tutorial we use the boston dataset from [sklearn.datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html)


import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_california_housing
# from sklearn.datasets import load_boston
from feature_engine.transformation import LogCpTransformer


# Load dataset
X, y = fetch_california_housing(return_X_y=True)
X = pd.DataFrame(X)

# Separate into train and test sets
X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.3, random_state=0)


# The `LogCpTransformer` automatically finds numerical variables in the dataset by setting `variables=None` or pass a list of column names as the example below shows. 
#
# Additionally notice that we define the transformer to automatically find the constant C `C="auto"`. Internally, each variable constant is calculated with the formula `C = abs(min(x)) + 1`.


print("Column names:", list(X_train.columns))
print("\nColumn positions:")
for i, col in enumerate(X_train.columns):
    print(f"{i}: {col}")


# num_feats = [7, 12]
num_feats = [6, 7]

# set up the variable transformer
tf = LogCpTransformer(variables=num_feats, C="auto")

# fit the transformer
tf.fit(X_train)

# transform the data
train_t= tf.transform(X_train)
test_t= tf.transform(X_test)


# We can now visualize the results from the transformation


plt.figure(figsize=(12, 12))

for idx, col in enumerate(num_feats, start=1):
    
    # plot un-transformed variable
    plt.subplot(2, 2, round(idx*1.4))
    plt.title(f'Untransformed variable {col}')
    X_train[col].hist()
    
    # plot transformed variable
    plt.subplot(2, 2, idx*2)
    plt.title(f'Transformed variable {col}')
    train_t[col].hist()


# One last thing, to verify the transformed variables we can access the transformer `variables_` attribute


tf.variables_


# or the constant `C` applied through the `C_` attribute.


tf.C_



// ---------------------------------------------------

// LogTransformer.py
// transformation/LogTransformer.py
# Generated from: LogTransformer.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Variable transformers : LogTransformer
#
# The LogTransformer() applies the natural logarithm or the base 10 logarithm to
# numerical variables. The natural logarithm is logarithm in base e.
#
# The LogTransformer() only works with numerical non-negative values. If the variable
# contains a zero or a negative value the transformer will return an error.
#
# **For this demonstration, we use the Ames House Prices dataset produced by Professor Dean De Cock:**
#
# Dean De Cock (2011) Ames, Iowa: Alternative to the Boston Housing
# Data as an End of Semester Regression Project, Journal of Statistics Education, Vol.19, No. 3
#
# http://jse.amstat.org/v19n3/decock.pdf
#
# https://www.tandfonline.com/doi/abs/10.1080/10691898.2011.11889627
#
# The version of the dataset used in this notebook can be obtained from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

from feature_engine.imputation import ArbitraryNumberImputer
from feature_engine.transformation import LogTransformer


# # load data

# data = pd.read_csv('houseprice.csv')
# data.head()

# # let's separate into training and testing set

# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1), data['SalePrice'], test_size=0.3, random_state=0)

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# plot distributions before transformation

X_train['LotArea'].hist(bins=50)


# plot distributions before transformation

X_train['GrLivArea'].hist(bins=50)


# ## Log base e


# Initialzing the tansformer with log base e

lt = LogTransformer(variables=['LotArea', 'GrLivArea'], base='e')

lt.fit(X_train)


# variables that will be transformed

lt.variables_


# apply the log transform

train_t = lt.transform(X_train)
test_t = lt.transform(X_test)


# transformed variable distribution

train_t['LotArea'].hist(bins=50)


# transformed variable distribution

train_t['GrLivArea'].hist(bins=50)


# return variables to original representation

train_orig = lt.inverse_transform(train_t)
test_orig = lt.inverse_transform(test_t)


# inverse transformed variable distribution

train_orig['LotArea'].hist(bins=50)


# inverse transformed variable distribution

train_orig['GrLivArea'].hist(bins=50)


# ## Automatically select numerical variables
#
# The transformer will transform all numerical variables if no variables are specified.


# load numerical variables only

variables = ['LotFrontage', 'LotArea',
             '1stFlrSF', 'GrLivArea',
             'TotRmsAbvGrd', 'SalePrice']



# data = pd.read_csv('houseprice.csv', usecols=variables)

# # let's separate into training and testing set

# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['SalePrice'], axis=1), data['SalePrice'], test_size=0.3, random_state=0)

# X_train.shape, X_test.shape



# # Read the separate files - only reading the columns we need
# train_df = pd.read_csv('../data/house-prices/train.csv', usecols=['Id'] + variables)
# test_df = pd.read_csv('../data/house-prices/test.csv', usecols=['Id'] + variables[:-1])  # excluding SalePrice for test

# # Separate features and target in training data
# X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
# y_train = train_df['SalePrice']

# # For test data, you might not have the target variable
# X_test = test_df.drop(['Id'], axis=1)

# print("X_train :", X_train.shape)
# print("X_test :", X_test.shape)

# ----------------------------------------------------------------------------------

# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# Impute missing values

arbitrary_imputer = ArbitraryNumberImputer(arbitrary_number=2)

arbitrary_imputer.fit(X_train)

# impute variables
train_t = arbitrary_imputer.transform(X_train)
test_t = arbitrary_imputer.transform(X_test)


numeric_columns = train_t.select_dtypes(include=['int64', 'float64']).columns
# numeric_columns_t = test_t.select_dtypes(include=['int64', 'float64']).columns


train_numeric = train_t[numeric_columns].copy()
# test_numeric = test_t[numeric_columns_t].copy()


train_numeric


# Define columns where zero is meaningful (count-based features)
meaningful_zeros = ['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 
                   'BedroomAbvGr', 'KitchenAbvGr', 'Fireplaces', 'GarageCars',
                   'PoolArea']

# Define area-based columns that need shifting
area_columns = ['MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 
                'TotalBsmtSF', '2ndFlrSF', 'LowQualFinSF', 'GarageArea',
                'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',
                'ScreenPorch', 'MiscVal']

# Create a copy of the training data
train_shifted = train_numeric.copy()

# Add small constant (1) only to area-based columns
for col in area_columns:
    train_shifted[col] = train_numeric[col] + 1

# Exclude meaningful zeros from log transformation
variables_to_transform = [col for col in train_numeric.columns if col not in meaningful_zeros]



# # Check which columns have zeros or negative values
# problematic_cols = []
# for col in train_numeric.columns:
#     if (train_numeric[col] <= 0).any():
#         problematic_cols.append(col)
#         print(f"{col}: Min value = {train_numeric[col].min()}")

# print("\nTotal problematic columns:", len(problematic_cols))



# Now apply log transformation only to specified variables
lt = LogTransformer(base='10', variables=variables_to_transform)
lt.fit(train_shifted)


# variables that will be transformed

lt.variables_


# before transformation
train_t['GrLivArea'].hist(bins=50)
plt.title('GrLivArea')


# Before transformation
train_t['LotArea'].hist(bins=50)
plt.title('LotArea')


train_t.columns


# transform the data

train_t = lt.transform(train_t)
test_t = lt.transform(test_t)


# transformed variable

train_t['GrLivArea'].hist(bins=50)
plt.title('GrLivArea')


# transformed variable
train_t['LotArea'].hist(bins=50)
plt.title('LotArea')


# return variables to original representation

train_orig = lt.inverse_transform(train_t)
test_orig = lt.inverse_transform(test_t)


# inverse transformed variable distribution

train_orig['LotArea'].hist(bins=50)


# inverse transformed variable distribution

train_orig['GrLivArea'].hist(bins=50)



// ---------------------------------------------------

// YeoJohnsonTransformer.py
// transformation/YeoJohnsonTransformer.py
# Generated from: YeoJohnsonTransformer.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Variable transformers : YeoJohnsonTransformer
#
# The YeoJohnsonTransformer() applies the Yeo-Johnson transformation to the
# numerical variables.
#
# **For this demonstration, we use the Ames House Prices dataset produced by Professor Dean De Cock:**
#
# Dean De Cock (2011) Ames, Iowa: Alternative to the Boston Housing
# Data as an End of Semester Regression Project, Journal of Statistics Education, Vol.19, No. 3
#
# http://jse.amstat.org/v19n3/decock.pdf
#
# https://www.tandfonline.com/doi/abs/10.1080/10691898.2011.11889627
#
# The version of the dataset used in this notebook can be obtained from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

from feature_engine.imputation import ArbitraryNumberImputer
from feature_engine.transformation import YeoJohnsonTransformer


# # load the dataset from Kaggle

# data = pd.read_csv('houseprice.csv')
# data.head()

# # let's separate into training and testing set

# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1),
#     data['SalePrice'],
#     test_size=0.3,
#     random_state=0,
# )

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# initialize transformer to transform 2 variables

yjt = YeoJohnsonTransformer(variables = ['LotArea', 'GrLivArea'])

# find otpimal lambdas for the transformation
yjt.fit(X_train)


# these are the lambdas for the YeoJohnson transformation

yjt.lambda_dict_


# transform variables

train_t = yjt.transform(X_train)
test_t = yjt.transform(X_test)


# variable before transformation
X_train['GrLivArea'].hist(bins=50)
plt.title('Variable before transformation')
plt.xlabel('GrLivArea')


# transformed variable
train_t['GrLivArea'].hist(bins=50)
plt.title('Transformed variable')
plt.xlabel('GrLivArea')


# tvariable before transformation
X_train['LotArea'].hist(bins=50)
plt.title('Variable before transformation')
plt.xlabel('LotArea')


# transformed variable
train_t['LotArea'].hist(bins=50)
plt.title('Variable before transformation')
plt.xlabel('LotArea')


# ## Automatically select numerical variables
#
# Before using YeoJohnsonTransformer we need to ensure that numerical variables do not have missing data.


# impute missing data

arbitrary_imputer = ArbitraryNumberImputer(arbitrary_number=2)

arbitrary_imputer.fit(X_train)

train_t = arbitrary_imputer.transform(X_train)
test_t = arbitrary_imputer.transform(X_test)


# intializing transformer to transform all variables

yjt = YeoJohnsonTransformer()

yjt.fit(train_t)


# Note, the run time error is because we are trying to transform integers.


# variables that will be transformed
# (these are the numerical variables in the dataset)

yjt.variables_


# these are the parameters for YeoJohnsonTransformer

yjt.lambda_dict_


# transform  variables
train_t = yjt.transform(train_t)
test_t = yjt.transform(test_t)



// ---------------------------------------------------

// PowerTransformer.py
// transformation/PowerTransformer.py
# Generated from: PowerTransformer.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Variable transformers : PowerTransformer
#
# The PowerTransformer() applies power or exponential transformations to
# numerical variables.
#
# The PowerTransformer() works only with numerical variables.
#
# **For this demonstration, we use the Ames House Prices dataset produced by Professor Dean De Cock:**
#
# Dean De Cock (2011) Ames, Iowa: Alternative to the Boston Housing
# Data as an End of Semester Regression Project, Journal of Statistics Education, Vol.19, No. 3
#
# http://jse.amstat.org/v19n3/decock.pdf
#
# https://www.tandfonline.com/doi/abs/10.1080/10691898.2011.11889627
#
# The version of the dataset used in this notebook can be obtained from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

from feature_engine.imputation import ArbitraryNumberImputer
from feature_engine.transformation import PowerTransformer


# # load data

# data = pd.read_csv('houseprice.csv')
# data.head()

# # let's separate into training and testing set

# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1), data['SalePrice'], test_size=0.3, random_state=0)

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# Initialize Transformers with exponent 1/2
# this is equivalent to square root
# we will transform only 2 variables

et_transformer = PowerTransformer(variables=['LotArea', 'GrLivArea'], exp=0.5)

et_transformer.fit(X_train)


# transform variables

train_t = et_transformer.transform(X_train)
test_t = et_transformer.transform(X_test)


# variable before transformation
X_train['GrLivArea'].hist(bins=50)
plt.title('Variable before transformation')
plt.xlabel('GrLivArea')


# transformed variable
train_t['GrLivArea'].hist(bins=50)
plt.title('Transformed variable')
plt.xlabel('GrLivArea')


# tvariable before transformation
X_train['LotArea'].hist(bins=50)
plt.title('Variable before transformation')
plt.xlabel('LotArea')


# transformed variable
train_t['LotArea'].hist(bins=50)
plt.title('Variable before transformation')
plt.xlabel('LotArea')


# return variables to original representation

train_orig = et_transformer.inverse_transform(train_t)
test_orig = et_transformer.inverse_transform(test_t)


# inverse transformed variable distribution

train_orig['LotArea'].hist(bins=50)


# inverse transformed variable distribution

train_orig['GrLivArea'].hist(bins=50)


# ## Automatically select numerical variables
#
# To use the PowerTransformer we need to ensure that numerical values don't have missing data.


# remove missing data 

arbitrary_imputer = ArbitraryNumberImputer()

arbitrary_imputer.fit(X_train)

# impute variables
train_t = arbitrary_imputer.transform(X_train)
test_t = arbitrary_imputer.transform(X_test)


# initialize transformer with exp as 2

et = PowerTransformer(exp=2, variables=None)

et.fit(train_t)


# variables to trasnform
et.variables_


# before transformation

train_t['GrLivArea'].hist(bins=50)


# transform variables

train_t = et.transform(train_t)
test_t = et.transform(test_t)


# transformed variable
train_t['GrLivArea'].hist(bins=50)


# return variables to original representation

train_orig = et_transformer.inverse_transform(train_t)
test_orig = et_transformer.inverse_transform(test_t)


# inverse transformed variable distribution

train_orig['LotArea'].hist(bins=50)


# inverse transformed variable distribution

train_orig['GrLivArea'].hist(bins=50)



// ---------------------------------------------------

// StringSimilarityEncoder.py
// encoding/StringSimilarityEncoder.py
# Generated from: StringSimilarityEncoder.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Imports


import string
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from feature_engine.encoding import StringSimilarityEncoder


# # Load and preprocess data


# Helper function for loading and preprocessing data
def load_titanic(filepath='titanic.csv'):
    translate_table = str.maketrans('' , '', string.punctuation)
    # data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    # data['home.dest'] = (
    #     data['home.dest']
    #     .str.strip()
    #     .str.translate(translate_table)
    #     .str.replace('  ', ' ')
    #     .str.lower()
    # )
    data['name'] = (
        data['name']
        .str.strip()
        .str.translate(translate_table)
        .str.replace('  ', ' ')
        .str.lower()
    )
    data['ticket'] = (
        data['ticket']
        .str.strip()
        .str.translate(translate_table)
        .str.replace('  ', ' ')
        .str.lower()
    )
    return data


# data = load_titanic("../data/titanic.csv")
data = load_titanic("../data/titanic-2/Titanic-Dataset.csv")
data.head()


# Separate into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(['survived', 'sex', 'cabin', 'embarked'], axis=1),
    data['survived'],
    test_size=0.3,
    random_state=0
)


# # StringSimilarityEncoder


# set up the encoder
# encoder = StringSimilarityEncoder(top_categories=2, variables=['name', 'home.dest', 'ticket'])
encoder = StringSimilarityEncoder(top_categories=2, variables=['name', 'ticket'])


# fit the encoder
encoder.fit(X_train)


# lets see what categories we will be comparing to others
encoder.encoder_dict_


# transform the data
train_t = encoder.transform(X_train)
test_t = encoder.transform(X_test)


# check output
train_t.head(5)


# check output
test_t.head(5)


# plot encoded column - ticket
# OHE could produce only 0, but SSE produces values in [0,1] range
fig, ax = plt.subplots(2, 1)
# train_t.plot(kind='scatter', x='ticket_ca 2343', y='ticket_ca 2144', sharex=True, title='Ticket encoding in train', ax=ax[0])
train_t.plot(kind='scatter', x='ticket_ca 2343', y='ticket_347082', sharex=True, title='Ticket encoding in train', ax=ax[0])
# test_t.plot(kind='scatter', x='ticket_ca 2343', y='ticket_ca 2144', sharex=True, title='Ticket encoding in test', ax=ax[1])
test_t.plot(kind='scatter', x='ticket_ca 2343', y='ticket_347082', sharex=True, title='Ticket encoding in test', ax=ax[1])


# defining encoder that ignores NaNs
encoder = StringSimilarityEncoder(
    top_categories=2,
    missing_values='ignore',
    # variables=['name', 'home.dest', 'ticket']
    variables=['name', 'ticket']
)


# refiting the encoder
encoder.fit(X_train)


# lets see what categories we will be comparing to others
# note - no empty strings with handle_missing='ignore'
encoder.encoder_dict_


# transform the data
train_t = encoder.transform(X_train)
test_t = encoder.transform(X_test)


# check output
train_t.head(5)


# check output
test_t.head(5)


# plot encoded column - home.dest
fig, ax = plt.subplots(2, 1);
train_t.plot(
    kind='scatter',
    x='home.dest_new york ny',
    y='home.dest_london',
    sharex=True,
    title='Home destination encoding in train',
    ax=ax[0]
);
test_t.plot(
    kind='scatter',
    x='home.dest_new york ny',
    y='home.dest_london',
    sharex=True,
    title='Home destination encoding in test',
    ax=ax[1]
);


# # Note on dimensionality reduction


# These encoded columns could also be compressed further to reduce dimensions
# since they are not boolean, but real numbers
from sklearn.decomposition import PCA


# defining encoder for home destination
encoder = StringSimilarityEncoder(
    top_categories=None,
    handle_missing='impute',
    variables=['home.dest']
)


# refiting the encoder
encoder.fit(X_train)


# transform the data
train_t = encoder.transform(X_train)


# check the shape (should be pretty big)
train_t.shape


# take home.dest encoded columns
home_encoded = train_t.filter(like='home.dest')


# defining PCA for compression
pca = PCA(n_components=0.9)


# train PCA
pca.fit(home_encoded)


# transform train and test datasets
train_compressed = pca.transform(home_encoded)


# check compressed shape (should be way smaller)
train_compressed.shape



// ---------------------------------------------------

// OrdinalEncoder.py
// encoding/OrdinalEncoder.py
# Generated from: OrdinalEncoder.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # OrdinalEncoder
# The OrdinalEncoder() will replace the variable labels by digits, from 1 to the number of different labels. 
#
# If we select "arbitrary", then the encoder will assign numbers as the labels appear in the variable (first come first served).
#
# If we select "ordered", the encoder will assign numbers following the mean of the target value for that label. So labels for which the mean of the target is higher will get the number 1, and those where the mean of the target is smallest will get the number n.


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from feature_engine.encoding import OrdinalEncoder


# Load titanic dataset from OpenML

def load_titanic(filepath='titanic.csv'):
    # data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    # data.drop(labels=['boat', 'body', 'home.dest', 'name', 'ticket'], axis=1, inplace=True)
    return data


# data = load_titanic("../data/titanic.csv")
data = load_titanic("../data/titanic-2/Titanic-Dataset.csv")
data.head()


X = data.drop(['survived', 'name', 'ticket'], axis=1)
y = data.survived


# we will encode the below variables, they have no missing values
X[['cabin', 'pclass', 'embarked']].isnull().sum()


''' Make sure that the variables are type (object).
if not, cast it as object , otherwise the transformer will either send an error (if we pass it as argument) 
or not pick it up (if we leave variables=None). '''

X[['cabin', 'pclass', 'embarked']].dtypes


# let's separate into training and testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

X_train.shape, X_test.shape


# The OrdinalEncoder() replaces categories by ordinal numbers 
# (0, 1, 2, 3, etc). The numbers can be ordered based on the mean of the target
# per category, or assigned arbitrarily.
#
# Ordered ordinal encoding:  for the variable colour, if the mean of the target
# for blue, red and grey is 0.5, 0.8 and 0.1 respectively, blue is replaced by 1,
# red by 2 and grey by 0.
#
# Arbitrary ordinal encoding: the numbers will be assigned arbitrarily to the
# categories, on a first seen first served basis.
#
# The encoder will encode only categorical variables (type 'object'). A list
# of variables can be passed as an argument. If no variables are passed, the
# encoder will find and encode all categorical variables (type 'object').


# ### Ordered


# we will encode 3 variables:
'''
Parameters
----------

encoding_method : str, default='ordered' 
    Desired method of encoding.

    'ordered': the categories are numbered in ascending order according to
    the target mean value per category.

    'arbitrary' : categories are numbered arbitrarily.
    
variables : list, default=None
    The list of categorical variables that will be encoded. If None, the 
    encoder will find and select all object type variables.
'''
ordinal_enc = OrdinalEncoder(encoding_method='ordered',
                             variables=['pclass', 'cabin', 'embarked'])

# for this encoder, we need to pass the target as argument
# if encoding_method='ordered'
ordinal_enc.fit(X_train, y_train)


ordinal_enc.encoder_dict_


# transform and visualise the data

train_t = ordinal_enc.transform(X_train)
test_t = ordinal_enc.transform(X_test)

test_t.sample(5)


''' The OrdinalEncoder with encoding_method='order' has the characteristic that return monotonic
 variables,that is, encoded variables which values increase as the target increases'''

# let's explore the monotonic relationship
plt.figure(figsize=(7,5))
pd.concat([test_t,y_test], axis=1).groupby("pclass")["survived"].mean().plot()
plt.xticks([0,1,2])
plt.yticks(np.arange(0,1.1,0.1))
plt.title("Relationship between pclass and target")
plt.xlabel("Pclass")
plt.ylabel("Mean of target")
plt.show()


# ### Arbitrary


ordinal_enc = OrdinalEncoder(encoding_method='arbitrary',
                             variables=['pclass', 'cabin', 'embarked'])

# for this encoder we don't need to add the target. You can leave it or remove it.
ordinal_enc.fit(X_train)


ordinal_enc.encoder_dict_


# Note that the ordering of the different labels is  not the same when we select "arbitrary" or "ordered"


# transform: see the numerical values in the former categorical variables

train_t = ordinal_enc.transform(X_train)
test_t = ordinal_enc.transform(X_test)

test_t.sample(5)


# ### Automatically select categorical variables
#
# This encoder selects all the categorical variables, if None is passed to the variable argument when calling the encoder.


ordinal_enc = OrdinalEncoder(encoding_method = 'arbitrary')

# for this encoder we don't need to add the target. You can leave it or remove it.
ordinal_enc.fit(X_train)


ordinal_enc.variables


train_t = ordinal_enc.transform(X_train)
test_t = ordinal_enc.transform(X_test)

test_t.sample(5)



// ---------------------------------------------------

// RareLabelEncoder.py
// encoding/RareLabelEncoder.py
# Generated from: RareLabelEncoder.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # RareLabelEncoder
#
# The RareLabelEncoder() groups labels that show a small number of observations in the dataset into a new category called 'Rare'. This helps to avoid overfitting.
#
# The argument ' tol ' indicates the percentage of observations that the label needs to have in order not to be re-grouped into the "Rare" label.<br> The argument n_categories indicates the minimum number of distinct categories that a variable needs to have for any of the labels to be re-grouped into 'Rare'.<br><br>
# #### Note
# If the number of labels is smaller than n_categories, then the encoder will not group the labels for that variable.


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from feature_engine.encoding import RareLabelEncoder


# Load titanic dataset from OpenML

def load_titanic(filepath='titanic.csv'):
    # data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    # data.drop(labels=['boat', 'body', 'home.dest', 'name', 'ticket'], axis=1, inplace=True)
    return data


# data = load_titanic("../data/titanic.csv")
data = load_titanic("../data/titanic-2/Titanic-Dataset.csv")
data.head()


X = data.drop(['survived', 'name', 'ticket'], axis=1)
y = data.survived


# we will encode the below variables, they have no missing values
X[['cabin', 'pclass', 'embarked']].isnull().sum()


''' Make sure that the variables are type (object).
if not, cast it as object , otherwise the transformer will either send an error (if we pass it as argument) 
or not pick it up (if we leave variables=None). '''

X[['cabin', 'pclass', 'embarked']].dtypes


# let's separate into training and testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
X_train.shape, X_test.shape


# The RareLabelEncoder() groups rare / infrequent categories in
# a new category called "Rare", or any other name entered by the user.
#
# For example in the variable colour,<br> if the percentage of observations
# for the categories magenta, cyan and burgundy 
# are < 5%, all those
# categories will be replaced by the new label "Rare".
#
# Note, infrequent labels can also be grouped under a user defined name, for
# example 'Other'. The name to replace infrequent categories is defined
# with the parameter replace_with.
#
# The encoder will encode only categorical variables (type 'object'). A list
# of variables can be passed as an argument. If no variables are passed as 
# argument, the encoder will find and encode all categorical variables
# (object type).


# Rare value encoder
'''
Parameters
----------

tol: float, default=0.05
    the minimum frequency a label should have to be considered frequent.
    Categories with frequencies lower than tol will be grouped.

n_categories: int, default=10
    the minimum number of categories a variable should have for the encoder
    to find frequent labels. If the variable contains less categories, all
    of them will be considered frequent.

max_n_categories: int, default=None
    the maximum number of categories that should be considered frequent.
    If None, all categories with frequency above the tolerance (tol) will be
    considered.

variables : list, default=None
    The list of categorical variables that will be encoded. If None, the 
    encoder will find and select all object type variables.

replace_with : string, default='Rare'
    The category name that will be used to replace infrequent categories.
'''

rare_encoder = RareLabelEncoder(tol=0.05,
                                n_categories=5,
                                variables=['cabin', 'pclass', 'embarked'])
rare_encoder.fit(X_train)


rare_encoder.encoder_dict_


train_t = rare_encoder.transform(X_train)
test_t = rare_encoder.transform(X_train)

test_t.head()


test_t.cabin.value_counts()


# #### The user can change the string from 'Rare' to something else.


# Rare value encoder
rare_encoder = RareLabelEncoder(tol=0.03,
                                replace_with='Other',  # replacing 'Rare' with 'Other'
                                variables=['cabin', 'pclass', 'embarked'],
                                n_categories=2
                                )

rare_encoder.fit(X_train)

train_t = rare_encoder.transform(X_train)
test_t = rare_encoder.transform(X_train)

test_t.sample(5)


rare_encoder.encoder_dict_


test_t.cabin.value_counts()


# #### The user can choose to retain only the most popular categories with the argument max_n_categories.


# Rare value encoder

rare_encoder = RareLabelEncoder(tol=0.03,
                                variables=['cabin', 'pclass', 'embarked'],
                                n_categories=2,
                                # keeps only the most popular 3 categories in every variable.
                                max_n_categories=3
                                )

rare_encoder.fit(X_train)

train_t = rare_encoder.transform(X_train)
test_t = rare_encoder.transform(X_train)

test_t.sample(5)


rare_encoder.encoder_dict_


# ### Automatically select all categorical variables
#
# If no variable list is passed as argument, it selects all the categorical variables.


len(X_train['pclass'].unique()), len(X_train['sex'].unique()), len(X_train['embarked'].unique())


# # X_train['pclass'].value_counts(dropna=False)
# pclass_encoder = RareLabelEncoder(tol=0.03, n_categories=3)
# X_train['pclass'] = pclass_encoder.fit_transform(X_train[['pclass']])


# # X_train['sex'].value_counts(dropna=False)
# sex_encoder = RareLabelEncoder(tol=0.03, n_categories=2)
# X_train['sex'] = sex_encoder.fit_transform(X_train[['sex']])


# # X_train['embarked'].value_counts(dropna=False)
# embarked_encoder = RareLabelEncoder(tol=0.03, n_categories=3)
# X_train['embarked'] = embarked_encoder.fit_transform(X_train[['embarked']])


## Rare value encoder
rare_encoder = RareLabelEncoder(tol = 0.03, n_categories=3)
rare_encoder.fit(X_train)
rare_encoder.encoder_dict_


train_t = rare_encoder.transform(X_train)
test_t = rare_encoder.transform(X_train)
test_t.sample(5)



// ---------------------------------------------------

// OneHotEncoder.py
// encoding/OneHotEncoder.py
# Generated from: OneHotEncoder.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # OneHotEncoder
# Performs One Hot Encoding.
#
# The encoder can select how many different labels per variable to encode into binaries. When top_categories is set to None, all the categories will be transformed in binary variables. 
#
# However, when top_categories is set to an integer, for example 10, then only the 10 most popular categories will be transformed into binary, and the rest will be discarded.
#
# The encoder has also the possibility to create binary variables from all categories (drop_last = False), or remove the binary for the last category (drop_last = True), for use in linear models.
#
# Finally, the encoder has the option to drop the second dummy variable for binary variables. That is, if a categorical variable has 2 unique values, for example colour = ['black', 'white'], setting the parameter drop_last_binary=True, will automatically create only 1 binary for this variable, for example colour_black.


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from feature_engine.encoding import OneHotEncoder


# Load titanic dataset from OpenML

def load_titanic(filepath='titanic.csv'):
    # data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    # data.drop(labels=['boat', 'body', 'home.dest', 'name', 'ticket'], axis=1, inplace=True)
    return data


# data = load_titanic("../data/titanic.csv")
data = load_titanic("../data/titanic-2/Titanic-Dataset.csv")
data.head()


X = data.drop(['survived', 'name', 'ticket'], axis=1)
y = data.survived


# we will encode the below variables, they have no missing values
X[['cabin', 'pclass', 'embarked']].isnull().sum()


''' Make sure that the variables are type (object).
if not, cast it as object , otherwise the transformer will either send an error (if we pass it as argument) 
or not pick it up (if we leave variables=None). '''

X[['cabin', 'pclass', 'embarked']].dtypes


# let's separate into training and testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

X_train.shape, X_test.shape


# One hot encoding consists in replacing the categorical variable by a
# combination of binary variables which take value 0 or 1, to indicate if
# a certain category is present in an observation.
#
# Each one of the binary variables are also known as dummy variables. For
# example, from the categorical variable "Gender" with categories 'female'
# and 'male', we can generate the boolean variable "female", which takes 1
# if the person is female or 0 otherwise. We can also generate the variable
# male, which takes 1 if the person is "male" and 0 otherwise.
#
# The encoder has the option to generate one dummy variable per category, or
# to create dummy variables only for the top n most popular categories, that is,
# the categories that are shown by the majority of the observations.
#
# If dummy variables are created for all the categories of a variable, you have
# the option to drop one category not to create information redundancy. That is,
# encoding into k-1 variables, where k is the number if unique categories.
#
# The encoder will encode only categorical variables (type 'object'). A list
# of variables can be passed as an argument. If no variables are passed as 
# argument, the encoder will find and encode categorical variables (object type).
#
#
# #### Note:
# New categories in the data to transform, that is, those that did not appear
# in the training set, will be ignored (no binary variable will be created for them).


# ### Create all k dummy variables, top_categories=False


'''
Parameters
----------

top_categories: int, default=None
    If None, a dummy variable will be created for each category of the variable.
    Alternatively, top_categories indicates the number of most frequent categories
    to encode. Dummy variables will be created only for those popular categories
    and the rest will be ignored. Note that this is equivalent to grouping all the
    remaining categories in one group.
    
variables : list
    The list of categorical variables that will be encoded. If None, the  
    encoder will find and select all object type variables.
    
drop_last: boolean, default=False
    Only used if top_categories = None. It indicates whether to create dummy
    variables for all the categories (k dummies), or if set to True, it will
    ignore the last variable of the list (k-1 dummies).
'''

ohe_enc = OneHotEncoder(top_categories=None,
                        variables=['pclass', 'cabin', 'embarked'],
                        drop_last=False)
ohe_enc.fit(X_train)


ohe_enc.encoder_dict_


train_t = ohe_enc.transform(X_train)
test_t = ohe_enc.transform(X_train)

test_t.head()


# ### Selecting top_categories to encode


ohe_enc = OneHotEncoder(top_categories=2,
                        variables=['pclass', 'cabin', 'embarked'],
                        drop_last=False)
ohe_enc.fit(X_train)

ohe_enc.encoder_dict_


train_t = ohe_enc.transform(X_train)
test_t = ohe_enc.transform(X_train)
test_t.head()


# ### Dropping the last category for linear models


ohe_enc = OneHotEncoder(top_categories=None,
                        variables=['pclass', 'cabin', 'embarked'],
                        drop_last=True)

ohe_enc.fit(X_train)

ohe_enc.encoder_dict_


train_t = ohe_enc.transform(X_train)
test_t = ohe_enc.transform(X_train)

test_t.head()


# ### Automatically select categorical variables
#
# This encoder selects all the categorical variables, if None is passed to the variable argument when calling the encoder.


ohe_enc = OneHotEncoder(top_categories=None,
                        drop_last=True)

ohe_enc.fit(X_train)


# the parameter variables is None
ohe_enc.variables


# but the attribute variables_ has the categorical variables 
# that will be encoded

ohe_enc.variables_


# and we can also find which variables from those
# are binary

ohe_enc.variables_binary_


train_t = ohe_enc.transform(X_train)
test_t = ohe_enc.transform(X_train)

test_t.head()


# ### Automatically create 1 dummy from binary variables (sex)
#
# We can encode categorical variables that have more than 2 categories into k dummies, and, at the same time, encode categorical variables that have 2 categories only in 1 dummy. The second 1 is completely redundant.
#
# We do so as follows:


ohe_enc = OneHotEncoder(top_categories=None,
                        drop_last=False,
                        drop_last_binary=True,
                        )

ohe_enc.fit(X_train)


# the encoder dictionary
ohe_enc.encoder_dict_


# and we can also find which variables from those
# are binary

ohe_enc.variables_binary_


train_t = ohe_enc.transform(X_train)
test_t = ohe_enc.transform(X_train)

test_t.head()



// ---------------------------------------------------

// WoEEncoder.py
// encoding/WoEEncoder.py
# Generated from: WoEEncoder.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# ## WoEEncoder (weight of evidence)
#
# This encoder replaces the labels by the weight of evidence 
# #### It only works for binary classification.
#
# The weight of evidence is given by: log( p(1) / p(0) )


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from feature_engine.encoding import WoEEncoder

from feature_engine.encoding import RareLabelEncoder #to reduce cardinality


# Load titanic dataset from OpenML

def load_titanic(filepath='titanic.csv'):
    # data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    # data.drop(labels=['boat', 'body', 'home.dest', 'name', 'ticket'], axis=1, inplace=True)
    return data


# data = load_titanic("../data/titanic.csv")
data = load_titanic("../data/titanic-2/Titanic-Dataset.csv")
data.head()


X = data.drop(['survived', 'name', 'ticket'], axis=1)
y = data.survived


# we will encode the below variables, they have no missing values
X[['cabin', 'pclass', 'embarked']].isnull().sum()


''' Make sure that the variables are type (object).
if not, cast it as object , otherwise the transformer will either send an error (if we pass it as argument) 
or not pick it up (if we leave variables=None). '''

X[['cabin', 'pclass', 'embarked']].dtypes


# let's separate into training and testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

X_train.shape, X_test.shape


## Rare value encoder first to reduce the cardinality
# see RareLabelEncoder jupyter notebook for more details on this encoder
rare_encoder = RareLabelEncoder(tol=0.03,
                                n_categories=2, 
                                variables=['cabin', 'pclass', 'embarked'])

rare_encoder.fit(X_train)

# transform
train_t = rare_encoder.transform(X_train)
test_t = rare_encoder.transform(X_test)


# The WoERatioEncoder() replaces categories by the weight of evidence
# or by the ratio between the probability of the target = 1 and the probability
# of the  target = 0.
#
# The weight of evidence is given by: log(P(X=x<sub>j</sub>|Y = 1)/P(X=x<sub>j</sub>|Y=0))
#
#
# Note: This categorical encoding is exclusive for binary classification.
#
# For example in the variable colour, if the mean of the target = 1 for blue
# is 0.8 and the mean of the target = 0  is 0.2, blue will be replaced by:
# np.log(0.8/0.2) = 1.386
# #### Note: 
# The division by 0 is not defined and the log(0) is not defined.
# Thus, if p(0) = 0 or p(1) = 0 for
# woe , in any of the variables, the encoder will return an error.
#
# The encoder will encode only categorical variables (type 'object'). A list
# of variables can be passed as an argument. If no variables are passed as 
# argument, the encoder will find and encode all categorical variables
# (object type).<br>
#
# For details on the calculation of the weight of evidence visit:<br>
# https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html


# ### Weight of evidence


woe_enc = WoEEncoder(variables=['cabin', 'pclass', 'embarked'])

# to fit you need to pass the target y
woe_enc.fit(train_t, y_train)


woe_enc.encoder_dict_


# transform and visualise the data

train_t = woe_enc.transform(train_t)
test_t = woe_enc.transform(test_t)

test_t.sample(5)


''' The WoEEncoder has the characteristic that return monotonic
 variables, that is, encoded variables which values increase as the target increases'''

# let's explore the monotonic relationship
plt.figure(figsize=(7,5))
pd.concat([test_t,y_test], axis=1).groupby("pclass")["survived"].mean().plot()
#plt.xticks([0,1,2])
plt.yticks(np.arange(0,1.1,0.1))
plt.title("Relationship between pclass and target")
plt.xlabel("Pclass")
plt.ylabel("Mean of target")
plt.show()


# ### Automatically select the variables
#
# This encoder will select all categorical variables to encode, when no variables are specified when calling the encoder.


ratio_enc = WoEEncoder()

# to fit we need to pass the target y
ratio_enc.fit(train_t, y_train)


# transform and visualise the data

train_t = ratio_enc.transform(train_t)
test_t = ratio_enc.transform(test_t)

test_t.head()



// ---------------------------------------------------

// DecisionTreeEncoder.py
// encoding/DecisionTreeEncoder.py
# Generated from: DecisionTreeEncoder.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # DecisionTreeEncoder
#
# The DecisionTreeEncoder() encodes categorical variables with predictions of a decision tree model.


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from feature_engine.encoding import DecisionTreeEncoder


# Load titanic dataset from OpenML

def load_titanic(filepath='titanic.csv'):
    # data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    # data.drop(labels=['boat', 'body', 'home.dest', 'name', 'ticket'], axis=1, inplace=True)
    return data


# data = load_titanic("../data/titanic.csv")
data = load_titanic("../data/titanic-2/Titanic-Dataset.csv")
data.head()


X = data.drop(['survived', 'name', 'ticket'], axis=1)
y = data.survived


# we will encode the below variables, they have no missing values
X[['cabin', 'pclass', 'embarked']].isnull().sum()


''' Make sure that the variables are type (object).
if not, cast it as object , otherwise the transformer will either send an error (if we pass it as argument) 
or not pick it up (if we leave variables=None). '''

X[['cabin', 'pclass', 'embarked']].dtypes


# let's separate into training and testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

X_train.shape, X_test.shape


# The categorical variable will be first encoded into integers with the
# OrdinalEncoder(). The integers can be assigned arbitrarily to the
# categories or following the mean value of the target in each category.
#
# Then a decision tree will be fit using the resulting numerical variable to predict
# the target  variable. Finally, the original categorical variable values will be
# replaced by the predictions of the decision tree.


'''
Parameters
    ----------

    encoding_method: str, default='arbitrary'
        The categorical encoding method that will be used to encode the original
        categories to numerical values.

        'ordered': the categories are numbered in ascending order according to
        the target mean value per category.

        'arbitrary' : categories are numbered arbitrarily.

    cv : int, default=3
        Desired number of cross-validation fold to be used to fit the decision
        tree.

    scoring: str, default='neg_mean_squared_error'
        Desired metric to optimise the performance for the tree. Comes from
        sklearn metrics. See the DecisionTreeRegressor or DecisionTreeClassifier
        model evaluation documentation for more options:
        https://scikit-learn.org/stable/modules/model_evaluation.html

    regression : boolean, default=True
        Indicates whether the encoder should train a regression or a classification
        decision tree.

    param_grid : dictionary, default=None
        The list of parameters over which the decision tree should be optimised
        during the grid search. The param_grid can contain any of the permitted
        parameters for Scikit-learn's DecisionTreeRegressor() or
        DecisionTreeClassifier().

        If None, then param_grid = {'max_depth': [1, 2, 3, 4]}.

    random_state : int, default=None
        The random_state to initialise the training of the decision tree. It is one
        of the parameters of the Scikit-learn's DecisionTreeRegressor() or
        DecisionTreeClassifier(). For reproducibility it is recommended to set
        the random_state to an integer.

    variables : list, default=None
        The list of categorical variables that will be encoded. If None, the
        encoder will find and select all object type variables.
'''


tree_enc = DecisionTreeEncoder(encoding_method='arbitrary',
                               cv=3,
                               scoring = 'roc_auc',
                               param_grid = {'max_depth': [1, 2, 3, 4]},
                               regression = False,
                               variables=['cabin', 'pclass', 'embarked']
                              )

tree_enc.fit(X_train,y_train) # to fit you need to pass the target y


tree_enc.encoder_


# transform and visualise the data

train_t = tree_enc.transform(X_train)
test_t = tree_enc.transform(X_test)

test_t.sample(5)


# ### Automatically select the variables
#
# This encoder will select all categorical variables to encode, when no variables are specified when calling the encoder.


tree_enc = DecisionTreeEncoder(encoding_method='arbitrary',
                               cv=3,
                               scoring = 'roc_auc',
                               param_grid = {'max_depth': [1, 2, 3, 4]},
                               regression = False,
                              )

tree_enc.fit(X_train,y_train) # to fit you need to pass the target y


tree_enc.encoder_


# transform and visualise the data

train_t = tree_enc.transform(X_train)
test_t = tree_enc.transform(X_test)

test_t.sample(5)



// ---------------------------------------------------

// CountFrequencyEncoder.py
// encoding/CountFrequencyEncoder.py
# Generated from: CountFrequencyEncoder.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # CountFrequencyEncoder
# <p>The CountFrequencyEncoder() replaces categories by the count of
# observations per category or by the percentage of observations per category.<br>
# For example in the variable colour, if 10 observations are blue, blue will
# be replaced by 10. Alternatively, if 10% of the observations are blue, blue
# will be replaced by 0.1.</p>


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from feature_engine.encoding import CountFrequencyEncoder


# Load titanic dataset from OpenML

def load_titanic(filepath='titanic.csv'):
    # data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    # data.drop(labels=['boat', 'body', 'home.dest', 'name', 'ticket'], axis=1, inplace=True)
    return data


# data = load_titanic("../data/titanic.csv")
data = load_titanic("../data/titanic-2/Titanic-Dataset.csv")
data.head()


X = data.drop(['survived', 'name', 'ticket'], axis=1)
y = data.survived


# we will encode the below variables, they have no missing values
X[['cabin', 'pclass', 'embarked']].isnull().sum()


''' Make sure that the variables are type (object).
if not, cast it as object , otherwise the transformer will either send an error (if we pass it as argument) 
or not pick it up (if we leave variables=None). '''

X[['cabin', 'pclass', 'embarked']].dtypes


# let's separate into training and testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

X_train.shape, X_test.shape


# The CountFrequencyEncoder(), replaces the categories by the count or frequency of the observations in the train set for that category. 
#
# If we select "count" in the encoding_method, then for the variable colour, if there are 10 observations in the train set that show colour blue, blue will be replaced by 10.<br><br> Alternatively, if we select "frequency" in the encoding_method, if 10% of the observations in the train set show blue colour, then blue will be replaced by 0.1.


# ### Frequency
#
# Labels are replaced by the percentage of the observations that show that label in the train set.


'''
Parameters
----------

encoding_method : str, default='count' 
                Desired method of encoding.

        'count': number of observations per category
        
        'frequency': percentage of observations per category

variables : list
          The list of categorical variables that will be encoded. If None, the 
          encoder will find and transform all object type variables.
'''
count_encoder = CountFrequencyEncoder(encoding_method='frequency',
                                      variables=['cabin', 'pclass', 'embarked'])

count_encoder.fit(X_train)


# we can explore the encoder_dict_ to find out the category replacements.
count_encoder.encoder_dict_


# transform the data: see the change in the head view
train_t = count_encoder.transform(X_train)
test_t = count_encoder.transform(X_test)
test_t.head()


test_t['pclass'].value_counts().plot.bar()
plt.show()


test_orig = count_encoder.inverse_transform(test_t)
test_orig.head()


# ### Count
#
# Labels are replaced by the number of the observations that show that label in the train set.


# this time we encode only 1 variable

count_enc = CountFrequencyEncoder(encoding_method='count',
                                                variables='cabin')

count_enc.fit(X_train)


# we can find the mappings in the encoder_dict_ attribute.

count_enc.encoder_dict_


# transform the data: see the change in the head view for Cabin

train_t = count_enc.transform(X_train)
test_t = count_enc.transform(X_test)

test_t.head()


# ### Select categorical variables automatically
#
# If we don't indicate which variables we want to encode, the encoder will find all categorical variables


# this time we ommit the argument for variable
count_enc = CountFrequencyEncoder(encoding_method = 'count')

count_enc.fit(X_train)


# we can see that the encoder selected automatically all the categorical variables

count_enc.variables


# transform the data: see the change in the head view

train_t = count_enc.transform(X_train)
test_t = count_enc.transform(X_test)

test_t.head()


# ### Note
# if there are labels in the test set that were not present in the train set, the transformer will introduce NaN, and raise a warning.



// ---------------------------------------------------

// PRatioEncoder.py
// encoding/PRatioEncoder.py
# Generated from: PRatioEncoder.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # PRatioEncoder
#
# The PRatioEncoder() replaces categories by the ratio of the probability of the
# target = 1 and the probability of the target = 0.<br>
#
# The target probability ratio is given by: p(1) / p(0).
#
# The log of the target probability ratio is: np.log( p(1) / p(0) )
# #### It only works for binary classification.


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from feature_engine.encoding import PRatioEncoder

from feature_engine.encoding import RareLabelEncoder #to reduce cardinality


# Load titanic dataset from OpenML

def load_titanic(filepath='titanic.csv'):
    # data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    # data.drop(labels=['boat', 'body', 'home.dest', 'name', 'ticket'], axis=1, inplace=True)
    return data


# data = load_titanic("../data/titanic.csv")
data = load_titanic("../data/titanic-2/Titanic-Dataset.csv")
data.head()


X = data.drop(['survived', 'name', 'ticket'], axis=1)
y = data.survived


# we will encode the below variables, they have no missing values
X[['cabin', 'pclass', 'embarked']].isnull().sum()


''' Make sure that the variables are type (object).
if not, cast it as object , otherwise the transformer will either send an error (if we pass it as argument) 
or not pick it up (if we leave variables=None). '''

X[['cabin', 'pclass', 'embarked']].dtypes


# let's separate into training and testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

X_train.shape, X_test.shape


## Rare value encoder first to reduce the cardinality
# see RareLabelEncoder jupyter notebook for more details on this encoder
rare_encoder = RareLabelEncoder(tol=0.03,
                                n_categories=2, 
                                variables=['cabin', 'pclass', 'embarked'])

rare_encoder.fit(X_train)

# transform
train_t = rare_encoder.transform(X_train)
test_t = rare_encoder.transform(X_test)


# The PRatioEncoder() replaces categories by the ratio of the probability of the
# target = 1 and the probability of the target = 0.
#
# The target probability ratio is given by: p(1) / p(0)
#
# The log of the target probability ratio is: np.log( p(1) / p(0) )
#
# Note: This categorical encoding is exclusive for binary classification.
#
# For example in the variable colour, if the mean of the target = 1 for blue
# is 0.8 and the mean of the target = 0  is 0.2, blue will be replaced by:
# 0.8 / 0.2 = 4 if ratio is selected, or log(0.8/0.2) = 1.386 if log_ratio
# is selected.
#
# Note: the division by 0 is not defined and the log(0) is not defined.
# Thus, if p(0) = 0 for the ratio encoder, or either p(0) = 0 or p(1) = 0 for
# log_ratio, in any of the variables, the encoder will return an error.
#
# The encoder will encode only categorical variables (type 'object'). A list
# of variables can be passed as an argument. If no variables are passed as
# argument, the encoder will find and encode all categorical variables
# (object type).


# ### Ratio


'''
Parameters
----------

encoding_method : str, default=woe
    Desired method of encoding.

    'ratio' : probability ratio

    'log_ratio' : log probability ratio

variables : list, default=None
    The list of categorical variables that will be encoded. If None, the
    encoder will find and select all object type variables.
'''
Ratio_enc = PRatioEncoder(encoding_method='ratio',
                           variables=['cabin', 'pclass', 'embarked'])

# to fit you need to pass the target y
Ratio_enc.fit(train_t, y_train)


Ratio_enc.encoder_dict_


# transform and visualise the data

train_t = Ratio_enc.transform(train_t)
test_t = Ratio_enc.transform(test_t)

test_t.sample(5)


# ### log ratio


train_t = rare_encoder.transform(X_train)
test_t = rare_encoder.transform(X_test)

logRatio_enc = PRatioEncoder(encoding_method='log_ratio',
                           variables=['cabin', 'pclass', 'embarked'])

# to fit you need to pass the target y
logRatio_enc.fit(train_t, y_train)


logRatio_enc.encoder_dict_


# transform and visualise the data

train_t = logRatio_enc.transform(train_t)
test_t = logRatio_enc.transform(test_t)

test_t.sample(5)


''' The PRatioEncoder(encoding_method='ratio' or 'log_ratio') has the characteristic that return monotonic
 variables, that is, encoded variables which values increase as the target increases'''

# let's explore the monotonic relationship
plt.figure(figsize=(7,5))
pd.concat([test_t,y_test], axis=1).groupby("pclass")["survived"].mean().plot()
#plt.xticks([0,1,2])
plt.yticks(np.arange(0,1.1,0.1))
plt.title("Relationship between pclass and target")
plt.xlabel("Pclass")
plt.ylabel("Mean of target")
plt.show()


# ### Automatically select the variables
#
# This encoder will select all categorical variables to encode, when no variables are specified when calling the encoder.


train_t = rare_encoder.transform(X_train)
test_t = rare_encoder.transform(X_test)

logRatio_enc = PRatioEncoder(encoding_method='log_ratio')

# to fit you need to pass the target y
logRatio_enc.fit(train_t, y_train)


# transform and visualise the data

train_t = logRatio_enc.transform(train_t)
test_t = logRatio_enc.transform(test_t)

test_t.sample(5)



// ---------------------------------------------------

// MeanEncoder.py
// encoding/MeanEncoder.py
# Generated from: MeanEncoder.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # MeanEncoder
#
# The MeanEncoder() replaces the labels of the variables by the mean value of the target for that label. <br>For example, in the variable colour, if the mean value of the binary target is 0.5 for the label blue, then blue is replaced by 0.5


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from feature_engine.encoding import MeanEncoder


# Load titanic dataset from OpenML

def load_titanic(filepath='titanic.csv'):
    # data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    # data.drop(labels=['boat', 'body', 'home.dest', 'name', 'ticket'], axis=1, inplace=True)
    return data


# data = load_titanic("../data/titanic.csv")
data = load_titanic("../data/titanic-2/Titanic-Dataset.csv")
data.head()


X = data.drop(['survived', 'name', 'ticket'], axis=1)
y = data.survived


# we will encode the below variables, they have no missing values
X[['cabin', 'pclass', 'embarked']].isnull().sum()


''' Make sure that the variables are type (object).
if not, cast it as object , otherwise the transformer will either send an error (if we pass it as argument) 
or not pick it up (if we leave variables=None). '''

X[['cabin', 'pclass', 'embarked']].dtypes


# let's separate into training and testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

X_train.shape, X_test.shape


# The MeanEncoder() replaces categories by the mean value of the
# target for each category.<br><br>
# For example in the variable colour, if the mean of the target for blue, red
# and grey is 0.5, 0.8 and 0.1 respectively, blue is replaced by 0.5, red by 0.8
# and grey by 0.1.<br><br>
# The encoder will encode only categorical variables (type 'object'). A list
# of variables can be passed as an argument. If no variables are passed as 
# argument, the encoder will find and encode all categorical variables
# (object type).


# we will transform 3 variables
'''
Parameters
----------  
variables : list, default=None
    The list of categorical variables that will be encoded. If None, the 
    encoder will find and select all object type variables.
'''

mean_enc = MeanEncoder(variables=['cabin', 'pclass', 'embarked'])

# Note: the MeanCategoricalEncoder needs the target to fit
mean_enc.fit(X_train, y_train)


# see the dictionary with the mappings per variable

mean_enc.encoder_dict_


# we can see the transformed variables in the head view

train_t = mean_enc.transform(X_train)
test_t = mean_enc.transform(X_test)

test_t.head()


''' The MeanEncoder has the characteristic that return monotonic
 variables, that is, encoded variables which values increase as the target increases'''

# let's explore the monotonic relationship
plt.figure(figsize=(7,5))
pd.concat([test_t,y_test], axis=1).groupby("pclass")["survived"].mean().plot()
#plt.xticks([0,1,2])
plt.yticks(np.arange(0,1.1,0.1))
plt.title("Relationship between pclass and target")
plt.xlabel("Pclass")
plt.ylabel("Mean of target")
plt.show()


# ### Automatically select the variables
#
# This encoder will select all categorical variables to encode, when no variables are specified when calling the encoder.


mean_enc = MeanEncoder()

mean_enc.fit(X_train, y_train)


mean_enc.variables


# we can see the transformed variables in the head view

train_t = mean_enc.transform(X_train)
test_t = mean_enc.transform(X_test)

test_t.head()



// ---------------------------------------------------

// OutlierTrimmer.py
// outliers/OutlierTrimmer.py
# Generated from: OutlierTrimmer.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # OutlierTrimmer
# The OutlierTrimmer() removes observations with outliers from the dataset.
#
# It works only with numerical variables. A list of variables can be indicated.
# Alternatively, the OutlierTrimmer() will select all numerical variables.
#
# The OutlierTrimmer() first calculates the maximum and /or minimum values
# beyond which a value will be considered an outlier, and thus removed.
#
# Limits are determined using:
#
# - a Gaussian approximation
# - the inter-quantile range proximity rule
# - percentiles.
#
# ### Example:


# importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from feature_engine.outliers import OutlierTrimmer


# Load titanic dataset from OpenML

def load_titanic(filepath='../data/titanic.csv'):
    # data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['embarked'].fillna('C', inplace=True)
    data['fare'] = data['fare'].astype('float')
    data['fare'].fillna(data['fare'].median(), inplace=True)
    data['age'] = data['age'].astype('float')
    data['age'].fillna(data['age'].median(), inplace=True)
    data.drop(['name', 'ticket'], axis=1, inplace=True)
    return data

# To plot histogram of given numerical feature
def plot_hist(data, col):
    plt.figure(figsize=(8, 5))
    plt.hist(data[col], bins=30)
    plt.title("Distribution of " + col)
    return plt.show()


# Loading titanic dataset
data = load_titanic()
data.sample(5)


# let's separate into training and testing set

X_train, X_test, y_train, y_test = train_test_split(data.drop('survived', axis=1),
                                                    data['survived'],
                                                    test_size=0.3,
                                                    random_state=0)

print("train data shape before removing outliers:", X_train.shape)
print("test data shape before removing outliers:", X_test.shape)


# let's find out the maximum Age and maximum Fare in the titanic

print("Max age:", data.age.max())
print("Max fare:", data.fare.max())

print("Min age:", data.age.min())
print("Min fare:", data.fare.min())


# Histogram of age feature before capping outliers
plot_hist(data, 'age')


# Histogram of fare feature before capping outliers
plot_hist(data, 'fare')


# ### Outlier trimming using Gaussian limits:
# The transformer will find the maximum and / or minimum values to
#     trim the variables using the Gaussian approximation.
#
#
# - right tail: mean + 3* std
# - left tail: mean - 3* std


'''Parameters
----------

capping_method : str, default=gaussian
    Desired capping method. Can take 'gaussian', 'iqr' or 'quantiles'.
    
tail : str, default=right
    Whether to cap outliers on the right, left or both tails of the distribution.
    Can take 'left', 'right' or 'both'.

fold: int or float, default=3
    How far out to to place the capping values. The number that will multiply
    the std or IQR to calculate the capping values.

variables : list, default=None

missing_values: string, default='raise'
    Indicates if missing values should be ignored or raised.'''

# removing outliers based on right tail of age and fare columns using gaussian capping method
trimmer = OutlierTrimmer(
    capping_method='gaussian', tail='right', fold=3, variables=['age', 'fare'])

# fitting trimmer object to training data
trimmer.fit(X_train)


# here we can find the maximum caps allowed
trimmer.right_tail_caps_


# this dictionary is empty, because we selected only right tail
trimmer.left_tail_caps_


# transforming the training and testing data
train_t = trimmer.transform(X_train)
test_t = trimmer.transform(X_test)

# let's check the new maximum Age and maximum Fare in the titanic
print("Max age:", train_t.age.max())
print("Max fare:", train_t.fare.max())


print("train data shape after removing outliers:", train_t.shape)
print(f"{X_train.shape[0] - train_t.shape[0]} observations are removed\n")

print("test data shape after removing outliers:", test_t.shape)
print(f"{X_test.shape[0] - test_t.shape[0]} observations are removed")


# ### Gaussian approximation trimming, both tails


# Trimming the outliers at both tails using gaussian  method
trimmer = OutlierTrimmer(
    capping_method='gaussian', tail='both', fold=2, variables=['fare', 'age'])
trimmer.fit(X_train)


print("Minimum caps :", trimmer.left_tail_caps_)

print("Maximum caps :", trimmer.right_tail_caps_)


# transforming the training and testing data
train_t = trimmer.transform(X_train)
test_t = trimmer.transform(X_test)

print("train data shape after removing outliers:", train_t.shape)
print(f"{X_train.shape[0] - train_t.shape[0]} observations are removed\n")

print("test data shape after removing outliers:", test_t.shape)
print(f"{X_test.shape[0] - test_t.shape[0]} observations are removed")


# ### Inter Quartile Range, both tails
# The transformer will find the boundaries using the IQR proximity rule.
# **IQR limits:**
#
# - right tail: 75th quantile + 3* IQR
# - left tail:  25th quantile - 3* IQR
#
# where IQR is the inter-quartile range: 75th quantile - 25th quantile.


# trimming at both tails using iqr capping method
trimmer = OutlierTrimmer(
    capping_method='iqr', tail='both', variables=['age', 'fare'])

trimmer.fit(X_train)


print("Minimum caps :", trimmer.left_tail_caps_)

print("Maximum caps :", trimmer.right_tail_caps_)


# transforming the training and testing data
train_t = trimmer.transform(X_train)
test_t = trimmer.transform(X_test)

print("train data shape after removing outliers:", train_t.shape)
print(f"{X_train.shape[0] - train_t.shape[0]} observations are removed\n")

print("test data shape after removing outliers:", test_t.shape)
print(f"{X_test.shape[0] - test_t.shape[0]} observations are removed")


# ### percentiles or quantiles:
# The limits are given by the percentiles.
# - right tail: 98th percentile
# - left tail:  2nd percentile


# trimming at both tails using quantiles capping method
trimmer = OutlierTrimmer(capping_method='quantiles',
                         tail='both', fold=0.02, variables=['age', 'fare'])

trimmer.fit(X_train)


print("Minimum caps :", trimmer.left_tail_caps_)

print("Maximum caps :", trimmer.right_tail_caps_)


# transforming the training and testing data
train_t = trimmer.transform(X_train)
test_t = trimmer.transform(X_test)

print("train data shape after removing outliers:", train_t.shape)
print(f"{X_train.shape[0] - train_t.shape[0]} observations are removed\n")

print("test data shape after removing outliers:", test_t.shape)
print(f"{X_test.shape[0] - test_t.shape[0]} observations are removed")


# Histogram of age feature after removing outliers
plot_hist(train_t, 'age')


# Histogram of fare feature after removing outliers
plot_hist(train_t, 'fare')



// ---------------------------------------------------

// Winsorizer.py
// outliers/Winsorizer.py
# Generated from: Winsorizer.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Winsorizer
# Winzorizer finds maximum and minimum values following a Gaussian or skewed distribution as indicated. It can also cap the right, left or both ends of the distribution.
#
# The Winsorizer() caps maximum and / or minimum values of a variable.
#
# The Winsorizer() works only with numerical variables. A list of variables can
# be indicated. Alternatively, the Winsorizer() will select all numerical
# variables in the train set.
#
# The Winsorizer() first calculates the capping values at the end of the
# distribution. The values are determined using:
#
# - a Gaussian approximation,
# - the inter-quantile range proximity rule (IQR)
# - percentiles.
#
#
# ### Example


# importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from feature_engine.outliers import Winsorizer


# Load titanic dataset from OpenML

def load_titanic(filepath='../data/titanic.csv'):
    # data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['embarked'].fillna('C', inplace=True)
    data['fare'] = data['fare'].astype('float')
    data['fare'].fillna(data['fare'].median(), inplace=True)
    data['age'] = data['age'].astype('float')
    data['age'].fillna(data['age'].median(), inplace=True)
    data.drop(['name', 'ticket'], axis=1, inplace=True)
    return data

# To plot histogram of given numerical feature
def plot_hist(data, col):
    plt.figure(figsize=(8, 5))
    plt.hist(data[col], bins=30)
    plt.title("Distribution of " + col)
    return plt.show()


# Loading titanic dataset
data = load_titanic()
data.sample(5)


# let's separate into training and testing set

X_train, X_test, y_train, y_test = train_test_split(data.drop('survived', axis=1),
                                                    data['survived'],
                                                    test_size=0.3,
                                                    random_state=0)

print("train data:", X_train.shape)
print("test data:", X_test.shape)


# let's find out the maximum Age and maximum Fare in the titanic

print("Max age:", data.age.max())
print("Max fare:", data.fare.max())


# Histogram of age feature before capping outliers
plot_hist(data, 'age')


# Histogram of fare feature before capping outliers
plot_hist(data, 'fare')


# ### Capping : Gaussian
#
# Gaussian limits:
# + right tail: mean + 3* std
# + left tail: mean - 3* std


'''Parameters
----------
capping_method : str, default=gaussian

    Desired capping method. Can take 'gaussian', 'iqr' or 'quantiles'.

tail : str, default=right

    Whether to cap outliers on the right, left or both tails of the distribution.
    Can take 'left', 'right' or 'both'.

fold: int or float, default=3

    How far out to to place the capping values. The number that will multiply
    the std or IQR to calculate the capping values. Recommended values, 2
    or 3 for the gaussian approximation, or 1.5 or 3 for the IQR proximity
    rule.

variables: list, default=None
  
missing_values: string, default='raise'

    Indicates if missing values should be ignored or raised.
'''
# capping at right tail using gaussian capping method
capper = Winsorizer(
    capping_method='gaussian', tail='right', fold=3, variables=['age', 'fare'])

# fitting winsorizer object to training data
capper.fit(X_train)


# here we can find the maximum caps allowed
capper.right_tail_caps_


# this dictionary is empty, because we selected only right tail
capper.left_tail_caps_


# # Histogram of age feature after capping outliers
plot_hist(capper.transform(X_train), 'age')


# transforming the training and testing data
train_t = capper.transform(X_train)
test_t = capper.transform(X_test)

# let's check the new maximum Age and maximum Fare in the titanic
train_t.age.max(), train_t.fare.max()


# ### Gaussian approximation capping, both tails


# Capping the outliers at both tails using gaussian capping method

winsor = Winsorizer(capping_method='gaussian',
                    tail='both', fold=2, variables='fare')
winsor.fit(X_train)


print("Minimum caps :", winsor.left_tail_caps_)

print("Maximum caps :", winsor.right_tail_caps_)


# Histogram of fare feature after capping outliers
plot_hist(winsor.transform(X_train), 'fare')


# transforming the training and testing data
train_t = winsor.transform(X_train)
test_t = winsor.transform(X_test)

print("Max fare:", train_t.fare.max())
print("Min fare:", train_t.fare.min())


# ### Inter Quartile Range, both tails
# **IQR limits:**
#
# - right tail: 75th quantile + 3* IQR
# - left tail:  25th quantile - 3* IQR
#
# where IQR is the inter-quartile range: 75th quantile - 25th quantile.


# capping at both tails using iqr capping method
winsor = Winsorizer(capping_method='iqr', tail='both',
                    variables=['age', 'fare'])

winsor.fit(X_train)


winsor.left_tail_caps_


winsor.right_tail_caps_


# transforming the training and testing data

train_t = winsor.transform(X_train)
test_t = winsor.transform(X_test)

print("Max fare:", train_t.fare.max())
print("Min fare", train_t.fare.min())


# ### percentiles or quantiles:
#
# - right tail: 98th percentile
# - left tail:  2nd percentile


# capping at both tails using quantiles capping method
winsor = Winsorizer(capping_method='quantiles', tail='both',
                    fold=0.02, variables=['age', 'fare'])

winsor.fit(X_train)


print("Minimum caps :", winsor.left_tail_caps_)

print("Maximum caps :", winsor.right_tail_caps_)


# transforming the training and testing data
train_t = winsor.transform(X_train)
test_t = winsor.transform(X_test)

print("Max age:", train_t.age.max())
print("Min age", train_t.age.min())


# Histogram of age feature after capping outliers
plot_hist(train_t, 'age')



// ---------------------------------------------------

// ArbitraryOutlierCapper.py
// outliers/ArbitraryOutlierCapper.py
# Generated from: ArbitraryOutlierCapper.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # ArbitraryOutlierCapper
# The ArbitraryOutlierCapper() caps the maximum or minimum values of a variable
# at an arbitrary value indicated by the user.
#
# The user must provide the maximum or minimum values that will be used <br>
# to cap each variable in a dictionary {feature : capping_value}


# ### Example


# importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from feature_engine.outliers import ArbitraryOutlierCapper


# Load titanic dataset from OpenML

def load_titanic(filepath='../data/titanic.csv'):
    # data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['embarked'].fillna('C', inplace=True)
    data['fare'] = data['fare'].astype('float')
    data['fare'].fillna(data['fare'].median(), inplace=True)
    data['age'] = data['age'].astype('float')
    data['age'].fillna(data['age'].median(), inplace=True)
    data.drop(['name', 'ticket'], axis=1, inplace=True)
    return data

# To plot histogram of given numerical feature
def plot_hist(data, col):
    plt.figure(figsize=(8, 5))
    plt.hist(data[col], bins=30)
    plt.title("Distribution of " + col)
    return plt.show()


data = load_titanic()
data.sample(5)


# let's separate into training and testing set

X_train, X_test, y_train, y_test = train_test_split(data.drop('survived', axis=1),
                                                    data['survived'],
                                                    test_size=0.3,
                                                    random_state=0)

print("train data:", X_train.shape)
print("test data:", X_test.shape)


# Histogram of age feature before capping outliers

plot_hist(data, 'age')


# Histogram of fare feature before capping outliers

plot_hist(data, 'fare')


# let's find out the maximum&minimum Age and maximum Fare in the titanic
print("Max age:", data.age.max())
print("Max fare:", data.fare.max())

print("Min age:", data.age.min())
print("Min fare:", data.fare.min())


# ### Maximum capping


'''Parameters
----------
max_capping_dict : dictionary, default=None
    Dictionary containing the user specified capping values for the right tail of
    the distribution of each variable (maximum values).

min_capping_dict : dictionary, default=None
    Dictionary containing user specified capping values for the eft tail of the
    distribution of each variable (minimum values).

missing_values : string, default='raise'
    Indicates if missing values should be ignored or raised. If
    `missing_values='raise'` the transformer will return an error if the
    training or the datasets to transform contain missing values.
'''

# capping of age and fare features at right tail
capper = ArbitraryOutlierCapper(
    max_capping_dict={'age': 50, 'fare': 150}, min_capping_dict=None)

capper.fit(X_train)


# here we can find the maximum caps allowed
print("Maximum caps:", capper.right_tail_caps_)


# this dictionary is empty, because we selected only right tail
capper.left_tail_caps_


# transforming train and test data
train_t = capper.transform(X_train)
test_t = capper.transform(X_test)

#check max age and max fare after capping
print("Max age after capping:", train_t.age.max())
print("Max fare after capping:", train_t.fare.max())


# ### Minimum capping


# capping outliers at left tail
capper = ArbitraryOutlierCapper(
    max_capping_dict=None, min_capping_dict={'age': 10, 'fare': 100})

capper.fit(X_train)


# this dictionary is empty, because we selected only right tail
capper.right_tail_caps_


# here we can find the minimum caps allowed
capper.left_tail_caps_


# transforming train and test set
train_t = capper.transform(X_train)
test_t = capper.transform(X_test)

# After capping
print("Min age:", train_t.age.min())
print("Min fare:", train_t.fare.min())


# ### Both ends capping


# capping outliers at both tails
capper = ArbitraryOutlierCapper(
    min_capping_dict={'age': 5, 'fare': 5},
    max_capping_dict={'age': 60, 'fare': 150})
capper.fit(X_train)


# here we can find the maximum caps allowed
capper.right_tail_caps_


# here we can find the minimum caps allowed
capper.left_tail_caps_


# transforming train and test data
train_t = capper.transform(X_train)
test_t = capper.transform(X_test)

# After capping outliers
print("Max age:", train_t.age.max())
print("Max fare:", train_t.fare.max())

print("Min age:", train_t.age.min())
print("Min fare:", train_t.fare.min())


# Histogram of age feature after capping outliers
plot_hist(train_t, 'age')


# Histogram of fare feature after capping outliers
plot_hist(train_t, 'fare')



// ---------------------------------------------------

// Select-by-Single-Feature-Performance.py
// selection/Select-by-Single-Feature-Performance.py
# Generated from: Select-by-Single-Feature-Performance.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# ## Univariate Single Performance
#
# - Train a ML model per every single feature
# - Determine the performance of the models
# - Select features if model performance is above a certain threshold


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import roc_auc_score, mean_squared_error

from feature_engine.selection import SelectBySingleFeaturePerformance


# ## Classification


# data.shape


data.head()


# **Important**
#
# In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit.


# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['target'], axis=1),
    data['target'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape


# set up a machine learning model
rf = RandomForestClassifier(
    n_estimators=10, random_state=1, n_jobs=4)

# set up the selector
sel = SelectBySingleFeaturePerformance(
    variables=None,
    estimator=rf,
    scoring="roc_auc",
    cv=3,
    threshold=0.5)

# find predictive features
sel.fit(X_train, y_train)


#  the transformer stores a dictionary of feature:metric pairs
# in this case is the roc_auc of each individual model

sel.feature_performance_


# we can plot feature importance sorted by importance

pd.Series(sel.feature_performance_).sort_values(ascending=False).plot.bar(figsize=(20, 5))
plt.title('Performance of ML models trained with individual features')
plt.ylabel('roc-auc')


# the features that will be removed

len(sel.features_to_drop_)


# remove non-prective features

X_train = sel.transform(X_train)
X_test = sel.transform(X_test)

X_train.shape, X_test.shape


# ## Regression
#
# ### with r2 and user specified threshold


# load dataset

data = pd.read_csv('../houseprice.csv')

data.shape


# I will use only numerical variables
# select numerical columns:

numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numerical_vars = list(data.select_dtypes(include=numerics).columns)
data = data[numerical_vars]
data.shape


data.head()


# fill missing values
data.fillna(0, inplace=True)


# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['Id','SalePrice'], axis=1),
    data['SalePrice'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape


# set up the machine learning model
rf = RandomForestRegressor(
    n_estimators=10, max_depth=2, random_state=1, n_jobs=4)

# set up the selector
sel = SelectBySingleFeaturePerformance(
    variables=None,
    estimator=rf,
    scoring="r2",
    cv=3,
    threshold=0.5)

# find predictive features
sel.fit(X_train, y_train)


# the transformer stores a dictionary of feature:metric pairs
# notice that the r2 can be positive or negative.
# the selector selects based on the absolute value

sel.feature_performance_


pd.Series(sel.feature_performance_).sort_values(ascending=False).plot.bar(figsize=(20, 5))
plt.title('Performance of ML models trained with individual features')
plt.ylabel('r2')


# same plot but taking the absolute value of the r2

np.abs(pd.Series(sel.feature_performance_)).sort_values(ascending=False).plot.bar(figsize=(20, 5))
plt.title('Performance of ML models trained with individual features')
plt.ylabel('r2 - absolute value')


# the features that will be removed

len(sel.features_to_drop_)


# select features in the dataframes

X_train = sel.transform(X_train)
X_test = sel.transform(X_test)

X_train.shape, X_test.shape


# ### Automatically select threshold
#
# If we leave the threshold to None, the threshold will be automatically specified as the mean of performance of all features.


# separate train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    data.drop(labels=['Id','SalePrice'], axis=1),
    data['SalePrice'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape


# set up the machine learning model
rf = RandomForestRegressor(
    n_estimators=10, max_depth=2, random_state=1, n_jobs=4)

# set up the selector
sel = SelectBySingleFeaturePerformance(
    variables=None,
    estimator=rf,
    scoring="neg_mean_squared_error",
    cv=3,
    threshold=None)

# find predictive features
sel.fit(X_train, y_train)


# the transformer stores a dictionary of feature:metric pairs
# the selector will select those features with neg mean squared error
# bigger than the mean of the neg squared error of all features

sel.feature_performance_


pd.Series(sel.feature_performance_).sort_values(ascending=False).plot.bar(figsize=(20, 5))
plt.title('Performance of ML models trained with individual features')
plt.ylabel('Negative mean Squared Error')


# the features that will be dropped
sel.features_to_drop_


# note that these features have the biggest negative mean squared error
pd.Series(sel.feature_performance_)[sel.features_to_drop_].sort_values(ascending=False).plot.bar(figsize=(20, 5))



// ---------------------------------------------------

// Drop-Constant-and-QuasiConstant-Features.py
// selection/Drop-Constant-and-QuasiConstant-Features.py
# Generated from: Drop-Constant-and-QuasiConstant-Features.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.


// ---------------------------------------------------

// Select-by-MinimumRedundance-MaximumRelevante.py
// selection/Select-by-MinimumRedundance-MaximumRelevante.py
# Generated from: Select-by-MinimumRedundance-MaximumRelevante.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# MRMR Feature Selection by Maykon Schots & Matheus Rugollo


# <h1> Numerical Feature Selection by MRMR </h1>
# <hr></hr>
#
# Experimenting fast is key to success in Data Science. When experimenting we're going to bump with huge datasets that require special attention when feature selecting and engineering. In a profit driven context, it's important to quickly test the potential value of an idea rather than exploring the best way to use your data or parametrize a machine learning model. It not only takes time that we usually can't afford but also increase financial costs. 
#
# Herewit we describe an efficient solution to reduce dimensionality of your dataset, by identifying and creating clusters of redundant features and selecting the most relevant one. This has potential to speed up your experimentation process and reduce costs.</p>
#
# <hr></hr>
# <h5>Case</h5>
#
# You might be wondering how this applies to a real use case and why we had to come up with such technique. Hear this story:
# Consider a project in a financial company that we try to understand how likely a client is to buy a product through Machine Learning. Other then profile features, we usually end up with many financial transactions history features of the clients. With that in mind we can assume that probably many of them are highly correlated, e.g in order to buy something of x value, the client probably received a value > x in the past, and since we're going to extract aggregation features from such events we're going to end up with a lot of correlation between them. 
#
#
# The solution was to come up with an efficient "automatic" way to wipe redundant features from the training set, that can vary from time to time, maintaining our model performance. With this we can always consider at the start of our pipeline all of our "raw" features and select the most relevant of them that are not highly correlated in given moment.
#
# Based on a published [article](https://arxiv.org/abs/1908.05376) we developed an implementation using [feature_engine](https://github.com/feature-engine/feature_engine) and [sklearn](https://scikit-learn.org/stable/). Follow the step-by-step to understand our approach.


# <h3> Classification Example </h3>
# <hr>
#
# In order to demonstrate, use the [make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) helper function from sklearn  to create a set of features making sure that some of them are redundant. Convert both X and y returned by it to be pandas DataFrames for further compatibility with sklearn api.


import warnings

import pandas as pd
from sklearn.datasets import make_classification

warnings.filterwarnings('ignore')

X, y = make_classification(
    n_samples=5000,
    n_features=30,
    n_redundant=15,
    n_clusters_per_class=1,
    weights=[0.50],
    class_sep=2,
    random_state=42
)

cols = []
for i in range(len(X[0])):
   cols.append(f"feat_{i}")
X = pd.DataFrame(X, columns=cols)
y = pd.DataFrame({"y": y})



X.head()


# <h4>Get Redundant Clusters</h4>
# <hr></hr>
#
# Now that we have our master table example set up, we can start by taking advantage of [SmartCorrelatedSelection](https://feature-engine.readthedocs.io/en/1.0.x/selection/SmartCorrelatedSelection.html) implementation by feature_egine. Let's check it's parameters:
#
# <h5>Correlation Threshold </h5>
# This can be a hot topic of discussion for each case, in order to keep as much useful data as possible the correlation threshold set was very conservative .97. 
# p.s: This demonstration will only have one value set, but a good way of improving this pipeline would be to attempt multiple iterations lowering the threshold, then you could measure performance of given model with different sets of selected features.
#
# <h5>Method</h5>
# The best option here was spearman, identifying both linear and non-linear numerical features correlated clusters to make it less redundant as possible through rank correlation threshold.
#
# <h5>Selection Method</h5>
# This is not relevant for this implementation, because we're not going to use features selected by the SmartCorrelatedSelection. Use variance , it's faster.
#
#
# <hr></hr>
# <h6>Quick Comment</h6>
# You might be wondering why we don't just use feature_engine methods, and we definitely considered and tried it, finally it inspired us to come up with some tweaks for our process. It's a very similar idea, but instead of variance we use mutual information to select one feature out of each cluster, it's also the ground work for optimal parametrization and further development of the pipeline for ad hoc usage.


from feature_engine.selection import SmartCorrelatedSelection


MODEL_TYPE = "classifier" ## Or "regressor"
CORRELATION_THRESHOLD = .97

# Setup Smart Selector /// Tks feature_engine
feature_selector = SmartCorrelatedSelection(
    variables=None,
    method="spearman",
    threshold=CORRELATION_THRESHOLD,
    missing_values="ignore",
    selection_method="variance",
    estimator=None,
)


feature_selector.fit_transform(X)

### Setup a list of correlated clusters as lists and a list of uncorrelated features
correlated_sets = feature_selector.correlated_feature_sets_

correlated_clusters = [list(feature) for feature in correlated_sets]

correlated_features = [feature for features in correlated_clusters for feature in features]

uncorrelated_features = [feature for feature in X if feature not in correlated_features]



# <h4>Wiping Redundancy considering Relevance</h4>
#
# Now we're going to extract the best feature from each correlated cluster using [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) from sklearn.feature_selection. Here we use [mutual_info_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif) implementation as our score_func for this classifier example, there are other options like mutual_info_regression be sure to select it according to your use case.
#
# The relevance of each selected feature is considered when we use mutual info of the samples against the target Y, this will be important so we do not lose any predictive power of our features.
#
# <hr></hr>
#
# We end up with a set of selected features that considering our correlation threshold of .97, probably will have similar performance. In a context where you want to prioritize reduction of dimensionality, you can check how the selection will perform to make a good decision about it.
#
# I don't want to believe, I want to know.


from sklearn.feature_selection import (
    SelectKBest,
    mutual_info_classif,
    mutual_info_regression,
)


mutual_info = {
    "classifier": mutual_info_classif,
    "regressor": mutual_info_regression,
}

top_features_cluster = []
for cluster in correlated_clusters:
            selector = SelectKBest(score_func=mutual_info[MODEL_TYPE], k=1)  # selects the top feature (k=1) regarding target mutual information
            selector = selector.fit(X[cluster], y)
            top_features_cluster.append(
                list(selector.get_feature_names_out())[0]
            )

selected_features = top_features_cluster + uncorrelated_features


# <h4>Evaluating the set of features</h4>
#
# Now that we have our set it's time to decide if we're going with it or not. In this demonstration, the idea was to use a GridSearch to find the best hyperparameters for a RandomForestClassifier providing us with the best possible estimator. 
#
# If we attempt to fit many grid searches in a robust way, it would take too long and be very costy. Since we're just experimenting, initally we can use basic cross_validate with the chosen estimator, and we can quickly discard "gone wrong" selections, specially when we lower down our correlation threshold for the clusters.
#
# It's an efficient way to approach experimenation with this method, although I highly recommend going for a more robust evaluation with grid searches or other approaches, and a deep discussion on the impact of the performance threshold for your use cause, sometimes 1% can be a lot of $.


import os
import multiprocessing

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold, cross_validate


cv = StratifiedKFold(shuffle=True, random_state=42)

baseline_raw = cross_validate(
    RandomForestClassifier(
        max_samples=1.0,
        n_jobs=int(os.getenv("N_CORES", 0.50 * multiprocessing.cpu_count())), # simplifica isso aqui pro artigo, bota -1.
        random_state=42
    ),
    X,
    y,
    cv=cv,
    scoring="f1", # or any other metric that you want.
    groups=None
)

baseline_selected_features = cross_validate(
            RandomForestClassifier(),
            X[selected_features],
            y,
            cv=cv,
            scoring="f1",
            groups=None,
            error_score="raise",
        )

score_raw = baseline_raw["test_score"].mean()
score_baseline = baseline_selected_features["test_score"].mean()

# Define a threshold to decide whether to reduce or not the dimensionality for your test case
dif = round(((score_raw - score_baseline) / score_raw), 3)

# 5% is our limit (ponder how it will impact your product $)
performance_threshold = -0.050

if dif >= performance_threshold:
    print(f"It's worth to go with the selected set =D")
elif dif < performance_threshold:
    print(f"The performance reduction is not acceptable!!!! >.<")



# <h2> Make it better ! </h2>
#
# <p> Going Further on implementing a robust feature selection with MRMR , we can use the process explained above to iterate over a range of threshold and choose what's best for our needs instead of a simple score performance evaluation! </p>


# Repeat df from example.

import warnings

import pandas as pd
from sklearn.datasets import make_classification

warnings.filterwarnings('ignore')

X, y = make_classification(
    n_samples=5000,
    n_features=30,
    n_redundant=15,
    n_clusters_per_class=1,
    weights=[0.50],
    class_sep=2,
    random_state=42
)

cols = []
for i in range(len(X[0])):
   cols.append(f"feat_{i}")
X = pd.DataFrame(X, columns=cols)
y = pd.DataFrame({"y": y})



# Functions to iterate over accepted threshold
from sklearn.feature_selection import (
    SelectKBest,
    mutual_info_classif,
    mutual_info_regression,
)
import os
import multiprocessing

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold, cross_validate

import pandas as pd
from feature_engine.selection import SmartCorrelatedSelection


def select_features_clf(X: pd.DataFrame, y: pd.DataFrame, corr_threshold: float) -> list:
    """ Function will select a set of features with minimum redundance and maximum relevante based on the set correlation threshold """
    # Setup Smart Selector /// Tks feature_engine
    feature_selector = SmartCorrelatedSelection(
        variables=None,
        method="spearman",
        threshold=corr_threshold,
        missing_values="ignore",
        selection_method="variance",
        estimator=None,
    )
    feature_selector.fit_transform(X)
    ### Setup a list of correlated clusters as lists and a list of uncorrelated features
    correlated_sets = feature_selector.correlated_feature_sets_
    correlated_clusters = [list(feature) for feature in correlated_sets]
    correlated_features = [feature for features in correlated_clusters for feature in features]
    uncorrelated_features = [feature for feature in X if feature not in correlated_features]
    top_features_cluster = []
    for cluster in correlated_clusters:
                selector = SelectKBest(score_func=mutual_info_classif, k=1)  # selects the top feature (k=1) regarding target mutual information
                selector = selector.fit(X[cluster], y)
                top_features_cluster.append(
                    list(selector.get_feature_names_out())[0]
                )
    return top_features_cluster + uncorrelated_features

def get_clf_model_scores(X: pd.DataFrame, y: pd.DataFrame, scoring: str, selected_features:list):
    """ """
    cv = StratifiedKFold(shuffle=True, random_state=42) 
    model_result = cross_validate(
        RandomForestClassifier(),
        X[selected_features],
        y,
        cv=cv,
        scoring=scoring,
        groups=None,
        error_score="raise",
    )
    return model_result["test_score"].mean(), model_result["fit_time"].mean(), model_result["score_time"].mean()

def evaluate_clf_feature_selection_range(X: pd.DataFrame, y: pd.DataFrame, scoring:str, corr_range: int, corr_starting_point: float = .98) -> pd.DataFrame:
    """ Evaluates feature selection for every .01 on corr threshold """
    evaluation_data = {
        "corr_threshold": [],
        scoring: [],
        "n_features": [],
        "fit_time": [],
        "score_time": []
    }
    for i in range(corr_range):
        current_corr_threshold = corr_starting_point - (i / 100) ## Reduces .01 on corr_threshold for every iteration
        selected_features = select_features_clf(X, y, corr_threshold=current_corr_threshold)
        score, fit_time, score_time = get_clf_model_scores(X, y, scoring, selected_features)
        evaluation_data["corr_threshold"].append(current_corr_threshold)
        evaluation_data[scoring].append(score)
        evaluation_data["n_features"].append(len(selected_features))
        evaluation_data["fit_time"].append(fit_time)
        evaluation_data["score_time"].append(score_time)
        
    return pd.DataFrame(evaluation_data)



evaluation_df = evaluate_clf_feature_selection_range(X, y, "f1", 15)


%pip install hiplot


import hiplot
from IPython.display import HTML

# html = hiplot.Experiment.from_dataframe(evaluation_df).to_html()
# displayHTML(html)

exp = hiplot.Experiment.from_dataframe(evaluation_df)
HTML(exp.to_html())



// ---------------------------------------------------

// Recursive-Feature-Elimination.py
// selection/Recursive-Feature-Elimination.py
# Generated from: Recursive-Feature-Elimination.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.


// ---------------------------------------------------

// Drop-Correlated-Features.py
// selection/Drop-Correlated-Features.py
# Generated from: Drop-Correlated-Features.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# ## Custom methods in `DropCorrelatedFeatures`
#
# In this tutorial we show how to pass a custom method to `DropCorrelatedFeatures` using the association measure [Distance Correlation](https://m-clark.github.io/docs/CorrelationComparison.pdf) from the python package [dcor](https://dcor.readthedocs.io/en/latest/index.html).


%pip install dcor


import dcor
import pandas as pd
import warnings

from sklearn.datasets import make_classification
from feature_engine.selection import DropCorrelatedFeatures

warnings.filterwarnings('ignore')


X, _ = make_classification(
    n_samples=1000,
    n_features=12,
    n_redundant=6,
    n_clusters_per_class=1,
    weights=[0.50],
    class_sep=2,
    random_state=1,
)

colnames = ["var_" + str(i) for i in range(12)]
X = pd.DataFrame(X, columns=colnames)

X


dcor_tr = DropCorrelatedFeatures(
    variables=None, method=dcor.distance_correlation, threshold=0.8
)

X_dcor = dcor_tr.fit_transform(X)

X_dcor


# In the next example, we use the function [sklearn.feature_selection.mutual_info_regression](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression) to calculate the Mutual Information between two numerical variables, dropping any features with a score below 0.8.
#
# Remember that the callable should take as input two 1d ndarrays and output a float value, we define a custom function calling the sklearn method.


from sklearn.feature_selection import mutual_info_regression

def custom_mi(x, y):
    x = x.reshape(-1, 1)
    y = y.reshape(-1, 1)
    return mutual_info_regression(x, y)[0] # should return a float value


mi_tr = DropCorrelatedFeatures(
    variables=None, method=custom_mi, threshold=0.8
)

X_mi = mi_tr.fit_transform(X)
X_mi



// ---------------------------------------------------

// Smart-Correlation-Selection.py
// selection/Smart-Correlation-Selection.py
# Generated from: Smart-Correlation-Selection.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# ## Custom methods in `SmartCorrelatedSelection`
#
# In this tutorial we show how to pass a custom method to `SmartCorrelatedSelection` using the association measure [Distance Correlation](https://m-clark.github.io/docs/CorrelationComparison.pdf) from the python package [dcor](https://dcor.readthedocs.io/en/latest/index.html). Install `dcor` before starting the tutorial
#
# ```
# !pip install dcor
# ```


import pandas as pd
import dcor
import warnings

from sklearn.datasets import make_classification
from feature_engine.selection import SmartCorrelatedSelection

warnings.filterwarnings('ignore')


X, _ = make_classification(
    n_samples=1000,
    n_features=12,
    n_redundant=6,
    n_clusters_per_class=1,
    weights=[0.50],
    class_sep=2,
    random_state=1
)

colnames = ['var_'+str(i) for i in range(12)]
X = pd.DataFrame(X, columns=colnames)


dcor_tr = SmartCorrelatedSelection(
    variables=None,
    method=dcor.distance_correlation,
    threshold=0.75,
    missing_values="raise",
    selection_method="variance",
    estimator=None,
)

X_dcor = dcor_tr.fit_transform(X)
X_dcor


# In the next example, we use the function [sklearn.feature_selection.mutual_info_regression](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression) to calculate the Mutual Information between two numerical variables.
#
# As the callable should take as input two 1d ndarrays and output a float value, we define a custom function calling the sklearn method.


from sklearn.feature_selection import mutual_info_regression

def custom_mi(x, y):
    x = x.reshape(-1, 1)
    y = y.reshape(-1, 1)
    return mutual_info_regression(x, y)[0] # should return a float value


mi_tr = SmartCorrelatedSelection(
    variables=None,
    method=custom_mi,
    threshold=0.75,
    missing_values="raise",
    selection_method="variance",
    estimator=None,
)

X_mi = mi_tr.fit_transform(X)
X_mi



// ---------------------------------------------------

// Select-by-Feature-Shuffling.py
// selection/Select-by-Feature-Shuffling.py
# Generated from: Select-by-Feature-Shuffling.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.


// ---------------------------------------------------

// Drop-High-PSI-Features.py
// selection/Drop-High-PSI-Features.py
# Generated from: Drop-High-PSI-Features.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Drop Features with High PSI Value
#
# The **DropHighPSIFeatures** selects features based on the Population Stability Index (PSI). The higher this value, the more unstable a feature. Unstable in this case means that there is a significant change in the distribution of the feature in the groups being compared.
#
# To determine the PSI of a feature, the DropHighPSIFeatures takes a dataframe and splits it in 2 based on a reference variable. This reference variable can be numerical, categorical or date. If the variable is numerical, the split ensures a certain proportion of observations in each sub-dataframe. If the variable is categorical, we can split the data based on the categories. And if the variable is a date, we can split the data based on dates.
#
# **In this notebook, we showcase many possible ways in which the DropHighPSIFeatures can be used to select features based on their PSI value.**
#
# ### Dataset
#
# We use the Credit Approval data set from the UCI Machine Learning Repository.
#
# To download the Credit Approval dataset from the UCI Machine Learning Repository visit [this website](http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/) and click on crx.data to download data. Save crx.data to the parent folder to this notebook folder.
#
# **Citation:**
#
# Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.
#
# # Data preparation
#
# We will edit some of the original variables and add some additional features to simulate different scenarios.


from datetime import date

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split

from feature_engine.selection import DropHighPSIFeatures


# ## Load the data


# load data
data = pd.read_csv('../data/credit+approval/crx.data', header=None)

# add variable names according to UCI Machine Learning
# Repo information
data.columns = ['A'+str(s) for s in range(1,17)]

# replace ? by np.nan
data = data.replace('?', np.nan)

# re-cast some variables to the correct types 
data['A2'] = data['A2'].astype('float')
data['A14'] = data['A14'].astype('float')

# encode target as binary
data['A16'] = data['A16'].map({'+':1, '-':0})

data.head()


# ## Edit and add features


# simulate customers from different portfolios.
data['A13'] = data['A13'].map({'g':'portfolio_1', 's':'portfolio_2', 'p':'portfolio_3'})
data['A13'].fillna('Unknown', inplace=True)

# simulate customers from different channels
data['A12'] = data['A12'].map({'f':'wholesale', 't':'retail'})
data['A12'].fillna('Missing', inplace=True)


# simulate customers from different age groups

data['A6'].fillna('Missing', inplace=True)

labels = {
'w': '20-25',
'q': '25-30',
'm': '30-35',
'r': '35-40',
'cc': '40-45',
'k': '45-50',
'c': '50-55',
'd': '55-60',
'x': '60-65',
'i': '65-70',
'e': '70-75',
'aa': '75-80',
'ff': '85-90',
'j': 'Unknown',
'Missing': 'Missing',
}
    
data['A6'] = data['A6'].map(labels)


# add a datetime variable

data['date'] = pd.date_range(start='1/1/2018', periods=len(data))

data.head()


# ## Data Analysis
#
# We will plot the distributions of numerical and categorical variables.


# categorical variables

vars_cat = data.select_dtypes(include='O').columns.to_list()

vars_cat


for var in vars_cat:
    data[var].value_counts(normalize=True).plot.bar()
    plt.title(var)
    plt.ylabel('% observations')
    plt.show()


# numerical variables

vars_num = data.select_dtypes(exclude='O').columns.to_list()

vars_num.remove('A16')

vars_num.remove('date')

vars_num


for var in vars_num:
    data[var].hist(bins=50)
    plt.title(var)
    plt.ylabel('Number observations')
    plt.show()


# # PSI feature selection
#
# ## Split data based on proportions
#
# DropHighPSIFeatures splits the dataset in 2, a base dataset and a comparison dataset. The comparison dataset is compared against the base dataset to determine the PSI.
#
# We may want to divide the dataset just based on **proportion of observations**. We want to have, say, 60% of observations in the base dataset. We can use the **dataframe index** to guide the split.
#
# **NOTE** that for the split, the transformer orders the variable, here the index, and then smaller values of the variable will be in the base dataset, and bigger values of the variable will go to the test dataset. In other words, this is not a random split.


# First, we split the data into a train and a test set

X_train, X_test, y_train, y_test = train_test_split(
    data[vars_cat+vars_num],
    data['A16'],
    test_size=0.1,
    random_state=42,
)


# Now we set up the DropHighPSIFeatures
# to split based on fraction of observations

transformer = DropHighPSIFeatures(
    split_frac=0.6, # the proportion of obs in the base dataset
    split_col=None, # If None, it uses the index
    strategy = 'equal_frequency', # whether to create the bins of equal frequency
    threshold=0.1, # the PSI threshold to drop variables
    variables=vars_num, # the variables to analyse
    missing_values='ignore',
)


# Now we fit the transformer to the train set.

# Here, the transformer will split the data, 
# determine the PSI of each feature and identify
# those that will be removed.

transformer.fit(X_train)


# the value in the index that determines the separation
# into base and comparison datasets. 

# Observations whose index value is smaller than 
# the cut_off will be in the base dataset. 
# The remaining ones in the test data.

transformer.cut_off_


# the PSI threshold above which variables 
# will be removed.

# We can change this when we initialize the transformer

transformer.threshold


# During fit() the transformer determines the PSI
# values for each variable and stores it.

transformer.psi_values_


# The variables that will be dropped:
# those whose PSI is biggher than the threshold.

transformer.features_to_drop_


# To understand what the DropHighPSIFeatures is doing, let's split the train set manually, in the same what that the transformer is doing. Then, let's plot the distribution of the variables in each of the sub-dataframes.


# Let's plot the variables distribution
# in each of the dataset portions

# create series to flag if an observation belongs to
# the base or test dataframe.

# Note how we use the cut_off identified by the
# transformer:
tmp = X_train.index <= transformer.cut_off_

# plot
sns.ecdfplot(data=X_train, x='A8', hue=tmp)
plt.title('A8 - moderate PSI')


# We observe a difference in the cumulative distribution of A8 between dataframes.


# For comparison, let's plot a variable with low PSI

sns.ecdfplot(data=X_train, x='A2', hue=tmp)
plt.title('A2 - low PSI')


# We see that the cumulative distribution of A8 is different in both datasets and this is why it is flagged for removal. On the other hand, the cumulative distribution of A2 is not different in the sub-datasets.
#
# Now we can go ahead and drop the features from the train and test sets. We use the transform() method.


# print shape before dropping variables

X_train.shape, X_test.shape


X_train = transformer.transform(X_train)
X_test = transformer.transform(X_test)

# print shape **after** dropping variables

X_train.shape, X_test.shape


# The datasets have now 2 variables less, those that had higher PSI values.
#
# ## Split data based on categorical values
#
# In the previous example, we sorted the observations based on a numerical variable, the index, and then we assigned the top 60% of the observations to the base dataframe. 
#
# Now, we will sort the observations based on a categorical variable, and assign the top 50% to the base dataframe.
#
# **Note** when splitting based on categorical variables the proportions achieved after the split may not match exactly the one specified.
#
# ### When is this split useful?
#
# This way of splitting the data is useful when, for example, we have a variable with the customer's ID. The ID's normally increase in time, with smaller values corresponding to older customers and bigger ID values corresponding to newly acquired customers.
#
# **Our example**
#
# In our data, we have customers from different age groups. We want to know if the variable distribution in younger age groups differ from older age groups. This is a suitable case to split based on a categorical value without specifically specifying the cut_off.
#
# The transformer will sort the categories of the variable and then those with smaller category values will be in the base dataframe, and the remaining in the comparison dataset.


# First, we split the data into a train and a test set

X_train, X_test, y_train, y_test = train_test_split(
    data[vars_cat+vars_num],
    data['A16'],
    test_size=0.1,
    random_state=42,
)


# Now, we set up the transformer

# Note that if we do not specify which variables to analyse, 
# the transformer will find the numerical variables automatically

transformer = DropHighPSIFeatures(
    split_frac=0.5, # percentage of observations in base df
    split_col='A6', # the categorical variable with the age groups
    strategy = 'equal_frequency',
    bins=8, # the number of bins into which the observations should be sorted
    threshold=0.1,
    variables=None, # When None, finds numerical variables automatically
    missing_values='ignore',
)


# Now we fit the transformer to the train set.

# Here, the transformer will split the data, 
# determine the PSI of each feature and identify
# those that will be removed.

transformer.fit(X_train)


# The transformer identified the numerical variables

transformer.variables_


# the age group under which observations will be
# in the base df.

transformer.cut_off_


# The PSI values determined for each feature

transformer.psi_values_


# The variables that will be dropped.

transformer.features_to_drop_


# There is no significant shift in the distribution of the variables between younger and older customers. Thus, no variables will be dropped.
#
# To understand what the DropHighPSIFeatures is doing, let's split the train set manually, in the same what that the transformer is doing. Then, let's plot the distribution of the variables in each of the sub-dataframes.


# Let's plot the variables distribution
# in each of the dataset portions

# create series to flag if an observation belongs to
# the base or comparison dataframe.

# Note how we use the cut_off identified by the
# transformer
tmp = X_train['A6'] <= transformer.cut_off_

# plot
sns.ecdfplot(data=X_train, x='A8', hue=tmp)
plt.title('A8 - low PSI')


# Let's plot another variable with low PSI

sns.ecdfplot(data=X_train, x='A15', hue=tmp)
plt.title('A15 - low PSI')


# As we can see, the distributions of the variables in both dataframes is quite similar.
#
# Now, let's identify which observations were assigned to each sub-dataframe by the transformer.


# The observations belonging to these age groups
# were assigned to the base df.

X_train[tmp]['A6'].unique()


# The number of age groups in the base df

X_train[tmp]['A6'].nunique()


# Proportion of observations in the base df

len(X_train[tmp]['A6']) / len(X_train)


# Note that we aimed for 50% of observations in the base reference, but based on this categorical variable, the closer we could get is 41%.


# The observations belonging to these age groups
# were assigned to the comparison df.

X_train[~tmp]['A6'].unique()


# The number of age groups in the comparison df

X_train[~tmp]['A6'].nunique()


# Proportion of observations in the comparison df

len(X_train[~tmp]['A6']) / len(X_train)


# Note that we have more age groups in the comparison df, but these groups have fewer observations, so the proportion of observations in the base and test dfs is the closest possible to what we wanted: 50%.
#
# Now we can go ahead and drop the features from the train and test sets.
#
# In this case, we would be dropping None.


# print shape before dropping variables

X_train.shape, X_test.shape


X_train = transformer.transform(X_train)
X_test = transformer.transform(X_test)

# print shape **after** dropping variables

X_train.shape, X_test.shape


# ## Split data based on distinct values
#
# In the previous example, we split the data using a categorical variable as guide, but ultimately, the split was done based on proportion of observations.
#
# In the extreme example where 50% of our customers belong to the age group 20-25 and the remaining 50% belong to older age groups, we would have only 1 age group in the base dataframe and all the remaining in the comparison dataframe if we split as we did in our previous example. This may result in a biased comparison.
#
# If we want to ensure that we have 50% of the possible age groups in each base and comparison dataframe, we can do so with the parameter `split_distinct`.


# First, we split the data into a train and a test set

X_train, X_test, y_train, y_test = train_test_split(
    data[vars_cat+vars_num],
    data['A16'],
    test_size=0.1,
    random_state=42,
)


transformer = DropHighPSIFeatures(
    split_frac=0.5, # proportion of (unique) categories in the base df
    split_distinct=True, # we split based on unique categories
    split_col='A6', # the categorical variable guiding the split
    strategy = 'equal_frequency',
    bins=5,
    threshold=0.1,
    missing_values='ignore',
)


# Now we fit the transformer to the train set
# Here, the transformer will split the data, 
# determine the PSI of each feature and identify
# those that will be removed.

transformer.fit(X_train)


# the age group under which, observations will be
# in the base df.

transformer.cut_off_


# Note that this cut_off is different from the one we obtained previously.


# The PSI values determined for each feature

transformer.psi_values_


# The variables that will be dropped.

transformer.features_to_drop_


# To understand what the DropHighPSIFeatures is doing, let's split the train set manually, in the same what that the transformer is doing. Then, let's plot the distribution of the variables in each of the sub-dataframes.


# Let's plot the variables distribution
# in each of the dataset portions

# create series to flag if an observation belongs to
# the base or comparison dataframe.

# Note how we use the cut_off identified by the
# transformer
tmp = X_train['A6'] <= transformer.cut_off_

# plot
sns.ecdfplot(data=X_train, x='A8', hue=tmp)
plt.title('A8 - high PSI')


# There is a mild difference in the variable distribution.


# For comparison, let's plot a variable with low PSI

sns.ecdfplot(data=X_train, x='A15', hue=tmp)
plt.title('A15 - low PSI')


# Now, let's identify which observations were assigned to each sub-dataframe by the transformer.


# The observations belonging to these age groups
# were assigned to the base df.

X_train[tmp]['A6'].unique()


# The number of age groups in the base df

X_train[tmp]['A6'].nunique()


# Proportion of observations in the base df

len(X_train[tmp]['A6']) / len(X_train)


# The observations belonging to these age groups
# were assigned to the comparison df.

X_train[~tmp]['A6'].unique()


# The number of age groups in the comparison df

X_train[~tmp]['A6'].nunique()


# Proportion of observations in the comparison df

len(X_train[~tmp]['A6']) / len(X_train)


# Now, we have a similar proportion of age groups in the base and comparison dfs. But the proportion of observations is different.
#
# Now we can go ahead and drop the features from the train and test sets.


# print shape before dropping variables

X_train.shape, X_test.shape


X_train = transformer.transform(X_train)
X_test = transformer.transform(X_test)

# print shape **after** dropping variables

X_train.shape, X_test.shape


# ## Split based on specific categories
#
# In the previous example, the categories had an intrinsic order. What if, we want to split based on category values which do not have an intrinsic order?
#
# We can do so by specifying which category values should go to the base dataframe.
#
# This way of splitting the data is useful if we want to compare features across customers coming from different portfolios, or different sales channels.


# First, we split the data into a train and a test set

X_train, X_test, y_train, y_test = train_test_split(
    data[vars_cat+vars_num],
    data['A16'],
    test_size=0.1,
    random_state=42,
)


# Set up the transformer 

transformer = DropHighPSIFeatures(
    cut_off=['portfolio_2', 'portfolio_3'], # the categories that should be in the base df
    split_col='A13', # the categorical variable with the portfolios
    strategy = 'equal_width', # the intervals are equidistant
    bins=5, # the number of intervals into which to sort the numerical values
    threshold=0.1,
    variables=vars_num,
    missing_values='ignore',
)


# Now we fit the transformer to the train set
# Here, the transformer will split the data, 
# determine the PSI of each feature and identify
# those that will be removed.

transformer.fit(X_train)


# We specified the cut_off, so we should see
# the portfolios here

transformer.cut_off_


# The transformer stores the PSI values of the variables

transformer.psi_values_


# The variables that will be dropped.

transformer.features_to_drop_


# It looks like all variables will be dropped.
#
# To understand what the DropHighPSIFeatures is doing, let's split the train set manually, in the same what that the transformer is doing. Then, let's plot the distribution of the variables in each of the sub-dataframes.


# Let's plot the variables distribution
# in each of the dataset portions

# create series to flag if an observation belongs to
# the base or comparison dataframe.

# Note how we use the cut_off identified by the
# transformer
tmp = X_train['A13'].isin(transformer.cut_off_)

sns.ecdfplot(data=X_train, x='A3', hue=tmp)
plt.title('A3 - high PSI')


# Let's plot another variable with high PSI

sns.ecdfplot(data=X_train, x='A11', hue=tmp)
plt.title('A11 - high PSI')


# Let's plot a variable with lower PSI

sns.ecdfplot(data=X_train, x='A2', hue=tmp)
plt.title('A2 - high PSI')


# Now we can go ahead and drop the features from the train and test sets.


# print shape before dropping variables

X_train.shape, X_test.shape


X_train = transformer.transform(X_train)
X_test = transformer.transform(X_test)

# print shape **after** dropping variables

X_train.shape, X_test.shape


# ## Split based on Date
#
# If our data had a valid timestamp, we could want to compare the distributions before and after a time point.


# Let's find out which are the minimum
# and maximum dates in our dataset

data['date'].agg(['min', 'max'])


# Now, we split the data into a train and a test set

X_train, X_test, y_train, y_test = train_test_split(
    data[vars_cat+vars_num+['date']],
    data['A16'],
    test_size=0.1,
    random_state=42,
)


# And we specify a transformer to split based
# on dates

transformer = DropHighPSIFeatures(
    cut_off =  pd.to_datetime('2018-12-14'), # the cut_off date
    split_col='date', # the date variable
    strategy = 'equal_frequency',
    bins=8,
    threshold=0.1,
    missing_values='ignore',
)


# Now we fit the transformer to the train set.

# Here, the transformer will split the data, 
# determine the PSI of each feature and identify
# those that will be removed.

transformer.fit(X_train)


# We specified the cut_off, so we should see
# our value here

transformer.cut_off_


# The transformer stores the PSI values of the variables

transformer.psi_values_


# The variables that will be dropped.

transformer.features_to_drop_


# To understand what the DropHighPSIFeatures is doing, let's split the train set manually, in the same what that the transformer is doing. Then, let's plot the distribution of the variables in each of the sub-dataframes.


# Let's plot the variables distribution
# in each of the dataset portions

# create series to flag if an observation belongs to
# the base or comparison dataframe.

# Note how we use the cut_off identified by the
# transformer
tmp = X_train['date'] <= transformer.cut_off_

# plot
sns.ecdfplot(data=X_train, x='A3', hue=tmp)
plt.title('A3 - moderate PSI')


# For comparison, let's plot a variable with low PSI

sns.ecdfplot(data=X_train, x='A14', hue=tmp)


# Now we can go ahead and drop the features from the train and test sets.


# print shape before dropping variables

X_train.shape, X_test.shape


X_train = transformer.transform(X_train)
X_test = transformer.transform(X_test)

# print shape **after** dropping variables

X_train.shape, X_test.shape


# That is all!
#
# I hope I gave you a good idea about how we can use this transformer to select features based on the Population Stability Index.



// ---------------------------------------------------

// Drop-Duplicated-Features.py
// selection/Drop-Duplicated-Features.py
# Generated from: Drop-Duplicated-Features.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.


// ---------------------------------------------------

// Drop-Arbitrary-Features.py
// selection/Drop-Arbitrary-Features.py
# Generated from: Drop-Arbitrary-Features.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.


// ---------------------------------------------------

// Recursive-Feature-Addition.py
// selection/Recursive-Feature-Addition.py
# Generated from: Recursive-Feature-Addition.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.


// ---------------------------------------------------

// Select-by-Target-Mean-Encoding.py
// selection/Select-by-Target-Mean-Encoding.py
# Generated from: Select-by-Target-Mean-Encoding.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# ## Select with Target Mean as Performance Proxy
#
# **Method used in a KDD 2009 competition**
#
# This feature selection approach was used by data scientists at the University of Melbourne in the [KDD 2009](http://www.kdd.org/kdd-cup/view/kdd-cup-2009) data science competition. The task consisted in predicting churn based on a dataset with a huge number of features.
#
# The authors describe this procedure as an aggressive non-parametric feature selection procedure that is based in contemplating the relationship between the feature and the target.
#
#
# **The procedure consists in the following steps**:
#
# For each categorical variable:
#
#     1) Separate into train and test
#
#     2) Determine the mean value of the target within each label of the categorical variable using the train set
#
#     3) Use that mean target value per label as the prediction (using the test set) and calculate the roc-auc.
#
# For each numerical variable:
#
#     1) Separate into train and test
#
#     2) Divide the variable intervals
#
#     3) Calculate the mean target within each interval using the training set 
#
#     4) Use that mean target value / bin as the prediction (using the test set) and calculate the roc-auc
#
#
# The authors quote the following advantages of the method:
#
# - Speed: computing mean and quantiles is direct and efficient
# - Stability respect to scale: extreme values for continuous variables do not skew the predictions
# - Comparable between categorical and numerical variables
# - Accommodation of non-linearities
#
# **Important**
# The authors here use the roc-auc, but in principle, we could use any metric, including those valid for regression.
#
# The authors sort continuous variables into percentiles, but Feature-engine gives the option to sort into equal-frequency or equal-width intervals.
#
# **Reference**:
# [Predicting customer behaviour: The University of Melbourne's KDD Cup Report. Miller et al. JMLR Workshop and Conference Proceedings 7:45-55](http://www.mtome.com/Publications/CiML/CiML-v3-book.pdf)


import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

from feature_engine.selection import SelectByTargetMeanPerformance


# load the titanic dataset
data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')

# remove unwanted variables
data.drop(labels = ['name','boat', 'ticket','body', 'home.dest'], axis=1, inplace=True)

# replace ? by Nan
data = data.replace('?', np.nan)

# missing values
data.dropna(subset=['embarked', 'fare'], inplace=True)

data['age'] = data['age'].astype('float')
data['age'] = data['age'].fillna(data['age'].mean())

data['fare'] = data['fare'].astype('float')

def get_first_cabin(row):
    try:
        return row.split()[0]
    except:
        return 'N' 
    
data['cabin'] = data['cabin'].apply(get_first_cabin)


data.head()


# Variable preprocessing:

# then I will narrow down the different cabins by selecting only the
# first letter, which represents the deck in which the cabin was located

# captures first letter of string (the letter of the cabin)
data['cabin'] = data['cabin'].str[0]

# now we will rename those cabin letters that appear only 1 or 2 in the
# dataset by N

# replace rare cabins by N
data['cabin'] = np.where(data['cabin'].isin(['T', 'G']), 'N', data['cabin'])

data['cabin'].unique()


data.dtypes


# number of passengers per value
data['parch'].value_counts()


# cap variable at 3, the rest of the values are
# shown by too few observations

data['parch'] = np.where(data['parch']>3,3,data['parch'])


data['sibsp'].value_counts()


# cap variable at 3, the rest of the values are
# shown by too few observations

data['sibsp'] = np.where(data['sibsp']>3,3,data['sibsp'])


# cast discrete variables as categorical

# feature-engine considers categorical variables all those of type
# object. So in order to work with numerical variables as if they
# were categorical, we  need to cast them as object

data[['pclass','sibsp','parch']] = data[['pclass','sibsp','parch']].astype('O')


# check absence of missing data

data.isnull().sum()


# **Important**
#
# In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit.


# separate train and test sets

X_train, X_test, y_train, y_test = train_test_split(
    data.drop(['survived'], axis=1),
    data['survived'],
    test_size=0.3,
    random_state=0)

X_train.shape, X_test.shape


# feautre engine automates the selection for both
# categorical and numerical variables

sel = SelectByTargetMeanPerformance(
    variables=None, # automatically finds categorical and numerical variables
    scoring="roc_auc_score", # the metric to evaluate performance
    threshold=0.6, # the threshold for feature selection, 
    bins=3, # the number of intervals to discretise the numerical variables
    strategy="equal_frequency", # whether the intervals should be of equal size or equal number of observations
    cv=2,# cross validation
    random_state=1, #seed for reproducibility
)

sel.fit(X_train, y_train)


# after fitting, we can find the categorical variables
# using this attribute

sel.variables_categorical_


# and here we find the numerical variables

sel.variables_numerical_


# here the selector stores the roc-auc per feature

sel.feature_performance_


# and these are the features that will be dropped

sel.features_to_drop_


X_train = sel.transform(X_train)
X_test = sel.transform(X_test)

X_train.shape, X_test.shape


# That is all for this lecture, I hope you enjoyed it and see you in the next one!



// ---------------------------------------------------

// MathematicalCombination.py
// creation/MathematicalCombination.py
# Generated from: MathematicalCombination.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# ### Feature Creation: MathematicalCombination
# The MathematicalCombination() applies basic mathematical operations **[‘sum’, ‘prod’, ‘mean’, ‘std’, ‘max’, ‘min’]** to multiple features, returning one or more additional features as a result.
#
# For this demonstration, we use the UCI Wine Quality Dataset.
#
# The data is publicly available on **[UCI repository](https://archive.ics.uci.edu/ml/datasets/Wine+Quality)**
#
# P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.
# Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    roc_curve,
    roc_auc_score,
    classification_report,
    confusion_matrix,
)
from sklearn.pipeline import Pipeline as pipe
from sklearn.preprocessing import StandardScaler

from feature_engine.creation import MathFeatures
from feature_engine.imputation import MeanMedianImputer

pd.set_option('display.max_columns', None)


# Read data
data = pd.read_csv('../data/winequality-red.csv', sep=';')

data.head()


# **This Data contains 11 features, all numerical, with no missing values.**


# Let's transform the Target, i.e Wine Quality into a binary classification problem:

bins = [0,5,10]

labels = [0, 1] # 'low'=0, 'high'=1

data['quality_range']= pd.cut(x=data['quality'], bins=bins, labels=labels)

data[['quality_range','quality']].head(5)


# drop original target

data.drop('quality', axis=1, inplace = True) 


# ### Sum and Mean Combinators:
# Let's create two new variables:
# - avg_acidity = mean(fixed acidity, volatile acidity)
# - total_minerals = sum(Total sulfure dioxide, sulphates)


# Create the Combinators

math_combinator_mean = MathFeatures(
    variables=['fixed acidity', 'volatile acidity'],
    func = ['mean'],
    new_variables_names = ['avg_acidity']
)

math_combinator_sum = MathFeatures(
    variables=['total sulfur dioxide', 'sulphates'],
    func = ['sum'],
    new_variables_names = ['total_minerals']
)

# Fit the Mean Combinator on training data
math_combinator_mean.fit(data)

# Transform the data
data_t = math_combinator_mean.transform(data)

# We can combine both steps in a single call with ".fit_transform()" methode
data_t = math_combinator_sum.fit_transform(data_t)


data_t.head()


# You can check the mappings between each new variable and the operation it's created with in the **combination_dict_**


# math_combinator_mean.feature_names_in_


math_combinator_mean.variables_


# ### Combine with more than 1 operation
#
# We can also combine the variables with more than 1 mathematical operation. And the transformer has the option to create variable names automatically.


# Create the Combinators

multiple_combinator = MathFeatures(
    variables=['fixed acidity', 'volatile acidity'],
    func = ['mean', 'sum'],
    new_variables_names = None
)


# Fit the Combinator to the training data
multiple_combinator.fit(data)

# Transform the data
data_t = multiple_combinator.transform(data)


# Note the 2 additional variables at the end of the dataframe
data_t.head()


multiple_combinator._get_new_features_name()


# # and here the variable names and the operation that was
# # applied to create that variable

# multiple_combinator.combination_dict_

# # {'mean(fixed acidity-volatile acidity)': 'mean',
# #  'sum(fixed acidity-volatile acidity)': 'sum'}



# ### Pipeline Example


# We can put all these transformations into single pipeline:
#
# 1. Create new variables
# 2. Scale features
# 3. Train a Logistic Regression model to predict wine quality
#
# See more on how to use Feature-engine within Scikit-learn Pipelines in these **[examples](https://github.com/solegalli/feature_engine/tree/master/examples/Pipelines)**


X = data.drop(['quality_range'], axis=1)

y = data.quality_range

X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.1,
                                                    random_state=0,
                                                    shuffle=True,
                                                    stratify=y
                                                    )
X_train.shape, X_test.shape


value_pipe = pipe([
    ('math_combinator_mean', MathFeatures(variables=['fixed acidity', 'volatile acidity'],
                                          func=['mean'],
                                          new_variables_names=['avg_acidity'])),
    ('math_combinator_sum', MathFeatures(variables=['total sulfur dioxide', 'sulphates'],
                                         func=['sum'],
                                         new_variables_names=['total_minerals'])),
    ('scaler', StandardScaler()),
    ('LogisticRegression', LogisticRegression())
])


value_pipe.fit(X_train, y_train)


pred_train = value_pipe.predict(X_train)
pred_test = value_pipe.predict(X_test)


print('Logistic Regression Model train accuracy score: {}'.format(
    accuracy_score(y_train, pred_train)))
print()
print('Logistic Regression Model test accuracy score: {}'.format(
    accuracy_score(y_test, pred_test)))


print('Logistic Regression Model test classification report: \n\n {}'.format(
    classification_report(y_test, pred_test)))


score = round(accuracy_score(y_test, pred_test), 3)
cm = confusion_matrix(y_test, pred_test)

sns.heatmap(cm, annot=True, fmt=".0f")
plt.xlabel('Predicted Values')
plt.ylabel('Actual Values')
plt.title('Accuracy Score: {0}'.format(score), size=15)
plt.show()


# Predict probabilities for the test data
probs = value_pipe.predict_proba(X_test)[:, 1]

# Get the ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, probs)

# Plot ROC curve
plt.figure(figsize=(8, 5))
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate = 1 - Specificity Score')
plt.ylabel('True Positive Rate  = Recall Score')
plt.title('ROC Curve')
plt.show()



// ---------------------------------------------------

// CombineWithReferenceFeature.py
// creation/CombineWithReferenceFeature.py
# Generated from: CombineWithReferenceFeature.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

#
# ## Feature Creation: Combine with reference feature
#
# The CombineWithReferenceFeature() applies combines a group of variables with a group of reference variables utilising mathematical operations ['sub', 'div','add','mul'], returning one or more additional features as a result.
#
# For this demonstration, we use the UCI Wine Quality Dataset.
#
# The data is publicly available on [UCI repository](https://archive.ics.uci.edu/ml/datasets/Wine+Quality)
#
# P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    roc_curve,
    roc_auc_score,
    classification_report,
    confusion_matrix,
)
from sklearn.pipeline import Pipeline as pipe
from sklearn.preprocessing import StandardScaler

from feature_engine.creation import RelativeFeatures
from feature_engine.creation import MathFeatures

from feature_engine.imputation import MeanMedianImputer

pd.set_option('display.max_columns', None)


# Read data
data = pd.read_csv('../data/winequality-red.csv', sep=';')

data.head()


# **This Data contains 11 features, all numerical, with no missing values.**


# Let's transform the Target, i.e Wine Quality into a binary classification problem:

bins = [0,5,10]

labels = [0, 1] # 'low'=0, 'high'=1

data['quality_range']= pd.cut(x=data['quality'], bins=bins, labels=labels)

data[['quality_range','quality']].head(5)


data.shape


# drop original target

data.drop('quality', axis=1, inplace = True)


data.shape


# ### Sub and Div Combinators:
#
# Let's create two new variables:
#
# - non_free_sulfur_dioxide = total sulfur dioxide - free sulfur dioxide
# - percentage_free_sulfur = free sulfur dioxide / total sulfur dioxide


import operator


def binary_add(x):
    return x.iloc[0] + x.iloc[1]


def binary_sub(x):
    return x.iloc[0] - x.iloc[1]


def binary_div(x):
    return x.iloc[0] / x.iloc[1]


def binary_mul(x):
    return x.iloc[0] * x.iloc[1]



# this transformer substracts free sulfur from total sulfur
sub_with_reference_feature = RelativeFeatures(
    variables=['total sulfur dioxide'],
    reference=['free sulfur dioxide'],
    func=['sub'],
)

# this transformer divides free sulfur by total sulfur
div_with_reference_feature = RelativeFeatures(
    variables=['free sulfur dioxide'],
    reference=['total sulfur dioxide'],
    func=['div'],
)



# # Create the Combinators

# sub_with_reference_feature = MathFeatures(
#     variables=['total sulfur dioxide', 'free sulfur dioxide'],
#     func=operator.sub,
#     new_variables_names=['non_free_sulfur_dioxide']
# )

# div_with_reference_feature = MathFeatures(
#     variables=['free sulfur dioxide', 'total sulfur dioxide'],
#     func=operator.truediv,
#     new_variables_names=['percentage_free_sulfur']
# )


# Fit the Sub Combinator on training data
sub_with_reference_feature.fit(data)


# perform the substraction
data_t = sub_with_reference_feature.transform(data)


# perform division
# We can combine both steps in a single call with ".fit_transform()" method
data_t = div_with_reference_feature.fit_transform(data_t)


# Note the additional variables at the end of the dataframe

data_t.head()


# #### Combine with more than 1 operation
#
# We can also combine the variables with more than 1 mathematical operation. And the transformer has the option to create variable names automatically.
#
# Here we will create the following variables:
#
# - ratio_fixed_to_volatile_acidity = fixed acidity / volatile acidity
# - total_acidity = fixed acidity + volatile acidity


# Create the Combinator

multiple_combinator = RelativeFeatures(
    variables=['fixed acidity'],
    reference=['volatile acidity'],
    func=['div', 'add'],
)


# multiple_combinator = MathFeatures(
#     variables=['fixed acidity', 'volatile acidity'],
#     func=[binary_div, binary_add],
#     new_variables_names=['ratio_fixed_to_volatile', 'total_acidity']
# )


# Fit the Combinator to the training data

multiple_combinator.fit(data_t)


# Transform the data

data_t = multiple_combinator.transform(data_t)


# Note the additional variables at the end of the dataframe

data_t.head()


# ### Pipeline Example
#
# We can put all these transformations into single pipeline:
#
# Create new variables scale features and train a Logistic Regression model to predict the wine quality range.
#
# See more on how to use Feature-engine within Scikit-learn Pipelines in these [examples](https://github.com/solegalli/feature_engine/tree/master/examples/Pipelines)


X = data.drop(['quality_range'], axis=1)

y = data.quality_range

X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.1,
                                                    random_state=0,
                                                    shuffle=True,
                                                    stratify=y
                                                    )
X_train.shape, X_test.shape


value_pipe = pipe([
    
    # Create new features
    ('subtraction', RelativeFeatures(
        variables=['total sulfur dioxide'],
        reference=['free sulfur dioxide'],
        func=['sub'],
    )
    ),

    ('ratio', RelativeFeatures(
        variables=['free sulfur dioxide'],
        reference=['total sulfur dioxide'],
        func=['div'],
    )
    ),

    ('acidity', RelativeFeatures(
        variables=['fixed acidity'],
        reference=['volatile acidity'],
        func=['div', 'add'],
    )
    ),

    # scale features
    ('scaler', StandardScaler()),

    # Logistic Regression
    ('LogisticRegression', LogisticRegression())
])


# value_pipe = pipe([

#     # Create new features
#     ('subtraction', MathFeatures(
#         variables=['total sulfur dioxide', 'free sulfur dioxide'],
#         func=binary_sub,
#         new_variables_names=['non_free_sulfur_dioxide']
#     )
#     ),

#     ('ratio', MathFeatures(
#         variables=['free sulfur dioxide', 'total sulfur dioxide'],
#         func=binary_div,
#         new_variables_names=['percentage_free_sulfur']
#     )
#     ),

#     ('acidity', MathFeatures(
#         variables=['fixed acidity', 'volatile acidity'],
#         func=[binary_div, binary_add],
#         new_variables_names=['ratio_fixed_to_volatile', 'total_acidity']
#     )
#     ),

#     # scale features
#     ('scaler', StandardScaler()),

#     # Logistic Regression
#     ('LogisticRegression', LogisticRegression())
# ])


value_pipe.fit(X_train, y_train)


pred_train = value_pipe.predict(X_train)
pred_test = value_pipe.predict(X_test)


print('Logistic Regression Model train accuracy score: {}'.format(
    accuracy_score(y_train, pred_train)))

print()

print('Logistic Regression Model test accuracy score: {}'.format(
    accuracy_score(y_test, pred_test)))


print('Logistic Regression Model test classification report: \n\n {}'.format(
    classification_report(y_test, pred_test)))


score = round(accuracy_score(y_test, pred_test), 3)
cm = confusion_matrix(y_test, pred_test)

sns.heatmap(cm, annot=True, fmt=".0f")
plt.xlabel('Predicted Values')
plt.ylabel('Actual Values')
plt.title('Accuracy Score: {0}'.format(score), size=15)
plt.show()


# Predict probabilities for the test data

probs = value_pipe.predict_proba(X_test)[:, 1]

# Get the ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, probs)

# Plot ROC curve
plt.figure(figsize=(8, 5))
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate = 1 - Specificity Score')
plt.ylabel('True Positive Rate  = Recall Score')
plt.title('ROC Curve')
plt.show()



// ---------------------------------------------------

// Sklearn-wrapper-plus-SimpleImputer.py
// wrappers/Sklearn-wrapper-plus-SimpleImputer.py
# Generated from: Sklearn-wrapper-plus-SimpleImputer.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

from feature_engine.wrappers import SklearnTransformerWrapper


# # load house prices data set from Kaggle

# data = pd.read_csv('houseprice.csv')
# data.head()

# # let's separate into training and testing set

# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1),
#     data['SalePrice'],
#     test_size=0.3,
#     random_state=0)

# X_train.shape, X_test.shape



# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


X_train[['LotFrontage', 'MasVnrArea']].isnull().mean()


# ## SimpleImputer
#
# ### Mean imputation


imputer = SklearnTransformerWrapper(
    transformer = SimpleImputer(strategy='mean'),
    variables = ['LotFrontage', 'MasVnrArea'],
)

imputer.fit(X_train)


# we can find the mean values within the parameters of the
# simple imputer

imputer.transformer_.statistics_


# remove NA

X_train = imputer.transform(X_train)
X_test = imputer.transform(X_test)

X_train[['LotFrontage', 'MasVnrArea']].isnull().mean()


# ### Frequent category imputation


cols = [c for c in train_df.columns if train_df[c].dtypes=='O' and train_df[c].isnull().sum()>0]
train_df[cols].head()


imputer = SklearnTransformerWrapper(
    transformer=SimpleImputer(strategy='most_frequent'),
    variables=cols,
)

# find the most frequent category
imputer.fit(X_train)


# we can find the most frequent values within the parameters of the
# simple imputer

imputer.transformer_.statistics_


# remove NA

X_train = imputer.transform(X_train)
X_test = imputer.transform(X_test)

X_train[cols].isnull().mean()


X_test[cols].head()



// ---------------------------------------------------

// Sklearn-wrapper-plus-Categorical-Encoding.py
// wrappers/Sklearn-wrapper-plus-Categorical-Encoding.py
# Generated from: Sklearn-wrapper-plus-Categorical-Encoding.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder

from feature_engine.wrappers import SklearnTransformerWrapper
from feature_engine.encoding import RareLabelEncoder


# # load the dataset from Kaggle

# data = pd.read_csv('houseprice.csv')
# data.head()

# # let's separate into training and testing set

# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1),
#     data['SalePrice'],
#     test_size=0.3,
#     random_state=0,
# )

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# ## OrdinalEncoder


cols = ['Alley',
        'MasVnrType',
        'BsmtQual',
        'BsmtCond',
        'BsmtExposure',
        'BsmtFinType1',
        'BsmtFinType2',
        'Electrical',
        'FireplaceQu',
        'GarageType',
        'GarageFinish',
        'GarageQual',
        ]


# let's remove rare labels to avoid errors when encoding

rare_label_enc = RareLabelEncoder(n_categories=2, variables=cols)

X_train = rare_label_enc.fit_transform(X_train.fillna('Missing'))
X_test = rare_label_enc.transform(X_test.fillna('Missing'))


# now let's replace categories by integers

encoder = SklearnTransformerWrapper(
    transformer = OrdinalEncoder(),
    variables = cols,
)

encoder.fit(X_train)


# we can navigate to the parameters of the sklearn transformer
# like this:

encoder.transformer_.categories_


# encode categories

X_train = encoder.transform(X_train)
X_test = encoder.transform(X_test)

X_train[cols].isnull().mean()


X_test[cols].head()



// ---------------------------------------------------

// Sklearn-wrapper-plus-scalers.py
// wrappers/Sklearn-wrapper-plus-scalers.py
# Generated from: Sklearn-wrapper-plus-scalers.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from feature_engine.wrappers import SklearnTransformerWrapper


# data = pd.read_csv('houseprice.csv')
# data.head()

# # let's separate into training and testing set

# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1), data['SalePrice'], test_size=0.3, random_state=0)

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# ## Scaling


cols = [var for var in X_train.columns if X_train[var].dtypes !='O']

cols


# let's apply the standard scaler on the above variables

scaler = SklearnTransformerWrapper(transformer = StandardScaler(),
                                    variables = cols)

scaler.fit(X_train.fillna(0))


X_train = scaler.transform(X_train.fillna(0))
X_test = scaler.transform(X_test.fillna(0))


# mean values, learnt by the StandardScaler
scaler.transformer_.mean_


# std values, learnt by the StandardScaler
scaler.transformer_.scale_


# the mean of the scaled variables is 0
X_train[cols].mean()


# the std of the scaled variables is ~1

X_train[cols].std()



// ---------------------------------------------------

// Sklearn-wrapper-plus-KBinsDiscretizer.py
// wrappers/Sklearn-wrapper-plus-KBinsDiscretizer.py
# Generated from: Sklearn-wrapper-plus-KBinsDiscretizer.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import KBinsDiscretizer

from feature_engine.wrappers import SklearnTransformerWrapper


# # load house prices data set from Kaggle

# data = pd.read_csv('houseprice.csv')
# data.head()

# # let's separate into training and testing set

# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1),
#     data['SalePrice'],
#     test_size=0.3,
#     random_state=0)

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


cols = [var for var in X_train.columns if X_train[var].dtypes !='O']

cols


X_train[cols].hist(bins=50, figsize=(15,15))
plt.show()


# ## KBinsDiscretizer
#
# ### Equal-frequency discretization


variables = ['GrLivArea','GarageArea']

X_train[variables].isnull().mean()


# # at the moment it only works if the encoding in kbinsdiscretizer
# # is set to 'ordinal'

# discretizer = SklearnTransformerWrapper(
#     transformer = KBinsDiscretizer(
#         n_bins=5, strategy='quantile', encode='ordinal'),
#     variables = variables,
# )

# discretizer.fit(X_train)

from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

# Create pipeline with imputer and discretizer
discretizer = Pipeline([
    ('imputer', SklearnTransformerWrapper(
        transformer=SimpleImputer(strategy='median'),
        variables=variables
    )),
    ('discretizer', SklearnTransformerWrapper(
        transformer=KBinsDiscretizer(n_bins=5, strategy='quantile', encode='ordinal'),
        variables=variables
    ))
])

# Fit and transform
discretizer.fit(X_train)



# discretizer.variables_


# discretizer.transformer_


# we can find the mean values within the parameters of the
# simple imputer

# discretizer.transformer_.bin_edges_


# remove NA

X_train = discretizer.transform(X_train)
X_test = discretizer.transform(X_test)


X_test['GrLivArea'].value_counts(normalize=True)


X_test['GarageArea'].value_counts(normalize=True)


X_test[variables].hist()
plt.show()



// ---------------------------------------------------

// Sklearn-wrapper-plus-feature-selection.py
// wrappers/Sklearn-wrapper-plus-feature-selection.py
# Generated from: Sklearn-wrapper-plus-feature-selection.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import (
    f_regression,
    SelectKBest,
    SelectFromModel,
)

from sklearn.linear_model import Lasso

from feature_engine.wrappers import SklearnTransformerWrapper


# # load dataset

# data = pd.read_csv('houseprice.csv')
# data.head()

# # let's separate into training and testing set

# X_train, X_test, y_train, y_test = train_test_split(
#     data.drop(['Id', 'SalePrice'], axis=1),
#     data['SalePrice'],
#     test_size=0.3,
#     random_state=0,
# )

# X_train.shape, X_test.shape


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# ## Select K Best


# variables to evaluate:

cols = [var for var in X_train.columns if X_train[var].dtypes !='O']

cols


# let's use select K best to select the best k variables

selector = SklearnTransformerWrapper(
    transformer = SelectKBest(f_regression, k=5),
    variables = cols)

selector.fit(X_train.fillna(0), y_train)


selector.transformer_.get_support(indices=True)


# selecteed features

X_train.columns[selector.transformer_.get_support(indices=True)]


# the transformer returns the selected variables from the list
# we passed to the transformer PLUS the remaining variables 
# in the dataframe that were not examined

X_train_t = selector.transform(X_train.fillna(0))
X_test_t = selector.transform(X_test.fillna(0))


X_test_t.head()


# ## SelectFromModel


# let's select the best variables according to Lasso

lasso = Lasso(alpha=10000, random_state=0)

sfm = SelectFromModel(lasso, prefit=False)

selector = SklearnTransformerWrapper(
    transformer = sfm,
    variables = cols)

selector.fit(X_train.fillna(0), y_train)


selector.transformer_.get_support(indices=True)


len(selector.transformer_.get_support(indices=True))


len(cols)


# the transformer returns the selected variables from the list
# we passed to the transformer PLUS the remaining variables 
# in the dataframe that were not examined

X_train_t = selector.transform(X_train.fillna(0))
X_test_t = selector.transform(X_test.fillna(0))


X_test_t.head()



// ---------------------------------------------------

// DatetimeFeatures.py
// datetime/DatetimeFeatures.py
# Generated from: DatetimeFeatures.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Datetime variable transformation
#
# The **DatetimeFeatures()** transformer is able to extract many different datetime features from existing datetime variables present in a dataframe. Some of these features are numerical, such as month, year, day of the week, week of the year, etc. and some are binary, such as whether that day was a weekend day or was the last day of its correspondent month. All features are cast to integer before adding them to the dataframe. <br>
# DatetimeFeatures() converts datetime variables whose dtype is originally object or categorical to a datetime format, but it does not work with variables whose original dtype is numerical. <br>
#
# For this demonstration, we use the Metro Interstate Traffic Volume Data Set, which is publicly available at https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume


#for starters, we import the relevant modules and the DatetimeFeatures class
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from feature_engine.datetime import DatetimeFeatures


#load and inspect the dataset
data = pd.read_csv('../data/Metro_Interstate_Traffic_Volume.csv')

data.head()


data.shape


# Inspect the columns typing and check for potentially missing values
pd.DataFrame({"type":data.dtypes, "nan count":data.isna().sum()})


# As it seems, this dataset only contains one datetime variable (named, indeed, _date\_time_). <br>
# Let's say we wanted to extract the _day of the month_ and the _hour_ features from it.
# Since _date\_time_ happens to be the only datetime variable in this dataset, we can do either of the following
# - let the transformer search for all datetime variables by initializing it with variables=None (which is the default option anyway)
# - specify which variables are going to be processed, which in this case would be setting variables="date_time"


dtfs = DatetimeFeatures(
    variables=None,
    features_to_extract=["day_of_month", "hour"]
)

# as per scikit-learn and feature-engine convention, we call the fit and transform method
# to process the data (even though this particular transformer does not learn any parameters)
dtfs.fit(data)


# check which variables have been picked up as datetime during fit
dtfs.variables_


data_transf = dtfs.transform(data)
data_transf.head()


# Notably, the dataframe identified that the object-like _date\_time_ variable could be cast to datetime and acquired the two columns _date\_time\_dotm_ and _date\_time\_hour_ corresponding to the features we required through the _features\_to\_extract_ argument. <br>
# **Note**: the original _date\_time_ column was removed from the dataframe in the process, as per default behaviour. If we want to keep it, we need to initialize the transformer passing drop_original=False.


# this time we specify what variable(s) we want the features extracted from
# we also want to keep the original datetime variable(s).
dtfs = DatetimeFeatures(
    variables="date_time",
    features_to_extract=["day_of_month", "hour"],
    drop_original=False
)

data_transf = dtfs.fit_transform(data)


data_transf.head()


# There are many more datetime features that DatetimeFeatures() can extract; see the docs for a full list. <br>
# The argument _features\_to\_extract_ has a default option aswell. Let's quickly see what it does.


dtfs = DatetimeFeatures(features_to_extract=None)

data_transf = dtfs.fit_transform(data)


# only show columns that were extracted from date_time
data_transf.filter(regex="date_time*").head()


# As shown above, DatetimeFeatures() extracts _month_, _year_, _day of the week_, _day of the month_, _hour_, _minute_ and _second_ by default. <br>
# **Note**: when a variable only contains date information all the time features default to 00:00:00 time; conversely, when a variable only contains time information, date features default to today's date at the time of calling the transform method.
#
# If we really want to extract _all_ of the available features we can set _features\_to\_extract_ to the special value "all". Beware, though, as your feature space might grow significantly and most of the extracted features are most likely not going to be too relevant.


dtfs = DatetimeFeatures(features_to_extract="all")

data_transf = dtfs.fit_transform(data)


data_transf.filter(regex="date_time*").head()


# Another thing to keep in mind is that oftentimes most of these features are going to be quasi-constant if not constant altogether. This can be for several reason, most likely due to the particular time window in which the data was collected. <br>
# We can thus combine the DatetimeFeatures() and DropConstantFeatures() transformers from feature_engine in a scikit-learn pipeline to automatically get rid of features we deem irrelevant to our analysis.


# data.drop('holiday', axis=1, inplace = True)
data['holiday'] = data['holiday'].replace({pd.NA: None, pd.NaT: None, np.nan: None})
data_for_pipe = data.drop('holiday', axis=1)


from sklearn.pipeline import Pipeline
from feature_engine.selection import DropConstantFeatures

pipe = Pipeline([
    ('datetime_extraction', DatetimeFeatures(features_to_extract=["year", "day_of_month", "minute", "second"])),
    ('drop_constants', DropConstantFeatures())
])



# print(data.isnull().sum()[data.isnull().sum() > 0])


data_transf = pipe.fit_transform(data_for_pipe)


data_transf.head()


# Since all data was gathered with only hour-precision, the _minute_ and _second_ features we had requested were extracted by DatetimeFeatures() but subsequently dropped by DropConstantFeatures(). This way we can avoid our feature space to become overly cluttered with useless information even when we are not being particularly diligent with the features we request to extract.



// ---------------------------------------------------

// EqualWidthDiscretiser.py
// discretisation/EqualWidthDiscretiser.py
# Generated from: EqualWidthDiscretiser.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # EqualWidthDiscretiser
#
# The EqualWidthDiscretiser() divides continuous numerical variables into
# intervals of the same width, that is, equidistant intervals. Note that the
# proportion of observations per interval may vary.
#
# The number of intervals
# in which the variable should be divided must be indicated by the user.
#
# **Note**
#
# For this demonstration, we use the Ames House Prices dataset produced by Professor Dean De Cock:
#
# Dean De Cock (2011) Ames, Iowa: Alternative to the Boston Housing
# Data as an End of Semester Regression Project, Journal of Statistics Education, Vol.19, No. 3
#
# http://jse.amstat.org/v19n3/decock.pdf
#
# https://www.tandfonline.com/doi/abs/10.1080/10691898.2011.11889627
#
# The version of the dataset used in this notebook can be obtained from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from feature_engine.discretisation import EqualWidthDiscretiser

plt.rcParams["figure.figsize"] = [15,5]


# data = pd.read_csv('../data/housing.csv')
# data.head()

# # let's separate into training and testing set
# X = data.drop(["Id", "SalePrice"], axis=1)
# y = data.SalePrice

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# print("X_train :", X_train.shape)
# print("X_test :", X_test.shape)


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# we will discretise two continuous variables

X_train[["LotArea", 'GrLivArea']].hist(bins=50)
plt.show()


# The EqualWidthDiscretiser() works only with numerical variables.
# A list of variables can be passed as argument. Alternatively, the discretiser
# will automatically select all numerical variables.
#
# The EqualWidthDiscretiser() first finds the boundaries for the intervals for
# each variable, fit.
#
# Then, it transforms the variables, that is, sorts the values into the intervals,
# transform.


'''
Parameters
----------

bins : int, default=10
    Desired number of equal width intervals / bins.

variables : list
    The list of numerical variables to transform. If None, the
    discretiser will automatically select all numerical type variables.

return_object : bool, default=False
    Whether the numbers in the discrete variable should be returned as
    numeric or as object. The decision should be made by the user based on
    whether they would like to proceed the engineering of the variable as
    if it was numerical or categorical.

return_boundaries: bool, default=False
    whether the output should be the interval boundaries. If True, it returns
    the interval boundaries. If False, it returns integers.
'''

ewd = EqualWidthDiscretiser(bins=10, variables=['LotArea', 'GrLivArea'])

ewd.fit(X_train)


# binner_dict contains the boundaries of the different bins
ewd.binner_dict_


train_t = ewd.transform(X_train)
test_t = ewd.transform(X_test)


# the below are the bins into which the observations were sorted
train_t['GrLivArea'].unique()


# here I put side by side the original variable and the transformed variable
tmp = pd.concat([X_train[["LotArea", 'GrLivArea']],
                 train_t[["LotArea", 'GrLivArea']]], axis=1)

tmp.columns = ["LotArea", 'GrLivArea', "LotArea_binned", 'GrLivArea_binned']

tmp.head()


# Note that the bins are not equally distributed
plt.subplot(1, 2, 1)
tmp.groupby('GrLivArea_binned')['GrLivArea'].count().plot.bar()
plt.ylabel('Number of houses')
plt.title('Number of observations per interval')

plt.subplot(1, 2, 2)
tmp.groupby('LotArea_binned')['LotArea'].count().plot.bar()
plt.ylabel('Number of houses')
plt.title('Number of observations per interval')

plt.show()


# ### Now return interval boundaries instead


ewd = EqualWidthDiscretiser(
    bins=10, variables=['LotArea', 'GrLivArea'], return_boundaries=True)

ewd.fit(X_train)


train_t = ewd.transform(X_train)
test_t = ewd.transform(X_test)


# the numbers are the different bins into which the observations
# were sorted
np.sort(np.ravel(train_t['GrLivArea'].unique()))


np.sort(np.ravel(test_t['GrLivArea'].unique()))


#the intervals are more or less of the same length
val = np.sort(np.ravel(train_t['GrLivArea'].unique()))
val


import re

# Extract the upper bounds (except for the last interval which has 'inf')
def extract_upper_bound(interval_str):
    # Extract the number before ']'
    match = re.search(r'([0-9.]+)\]$', interval_str)
    if match:
        return float(match.group(1))
    return None

upper_bounds = [extract_upper_bound(x) for x in val if extract_upper_bound(x) is not None]
upper_bounds.sort()

differences = np.diff(upper_bounds)
print(differences)


def extract_bounds(interval_str):
    # Extract numbers from the interval string
    numbers = re.findall(r'[-+]?\d*\.\d+|\d+', interval_str)
    if len(numbers) == 2:
        return float(numbers[0]), float(numbers[1])
    return None

# Get the bounds and sort by upper bound
bounds = [extract_bounds(x) for x in val if extract_bounds(x) is not None]
bounds.sort(key=lambda x: x[1])  # sort by upper bound

# Calculate interval sizes
interval_sizes = [bounds[i][1] - bounds[i][0] for i in range(len(bounds))]
print(interval_sizes)



// ---------------------------------------------------

// ArbitraryDiscretiser_plus_MeanEncoder.py
// discretisation/ArbitraryDiscretiser_plus_MeanEncoder.py
# Generated from: ArbitraryDiscretiser_plus_MeanEncoder.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # ArbitraryDiscretiser + MeanEncoder
#
# This is very useful for linear models, because by using discretisation + a monotonic encoding, we create monotonic variables with the target, from those that before were not originally. And this tends to help improve the performance of the linear model. 


# ## ArbitraryDiscretiser
#
# The ArbitraryDiscretiser() divides continuous numerical variables into contiguous intervals arbitrarily defined by the user.
#
# The user needs to enter a dictionary with variable names as keys, and a list of the limits of the intervals as values. For example {'var1': [0, 10, 100, 1000],'var2': [5, 10, 15, 20]}.
#
# <b>Note:</b> Check out the ArbitraryDiscretiser notebook to learn more about this transformer.


# ## MeanEncoder
#
# The MeanEncoder() replaces the labels of the variables by the mean value of the target for that label. <br>For example, in the variable colour, if the mean value of the binary target is 0.5 for the label blue, then blue is replaced by 0.5
#
# <b>Note:</b> Read MeanEncoder notebook to know more about this transformer


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline

from feature_engine.discretisation import ArbitraryDiscretiser
from feature_engine.encoding import MeanEncoder

plt.rcParams["figure.figsize"] = [15,5]


# Load titanic dataset from OpenML

def load_titanic(filepath='titanic.csv'):
    # data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    # data.drop(labels=['boat', 'body', 'home.dest', 'name', 'ticket'], axis=1, inplace=True)
    return data


# data = load_titanic("../data/titanic.csv")
data = load_titanic("../data/titanic-2/Titanic-Dataset.csv")
data.head()


# let's separate into training and testing set
X = data.drop(['survived'], axis=1)
y = data.survived

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# we will transform two continuous variables
X_train[["age", 'fare']].hist(bins=30)
plt.show()


# set up the discretiser
arb_disc = ArbitraryDiscretiser(
    binning_dict={'age': [0, 18, 30, 50, 100],
                  'fare': [-1, 20, 40, 60, 80, 600]},
    # returns values as categorical
    return_object=True)

# set up the mean encoder
mean_enc = MeanEncoder(variables=['age', 'fare'])

# set up the pipeline
transformer = Pipeline(steps=[('ArbitraryDiscretiser', arb_disc),
                              ('MeanEncoder', mean_enc),
                              ])
# train the pipeline
transformer.fit(X_train, y_train)


transformer.named_steps['ArbitraryDiscretiser'].binner_dict_


transformer.named_steps['MeanEncoder'].encoder_dict_


train_t = transformer.transform(X_train)
test_t = transformer.transform(X_test)

test_t.head()


# let's explore the monotonic relationship
plt.figure(figsize=(7, 5))
pd.concat([test_t, y_test], axis=1).groupby("fare")["survived"].mean().plot()
plt.title("Relationship between fare and target")
plt.xlabel("fare")
plt.ylabel("Mean of target")
plt.show()


# We can observe an almost linear relationship between the variable "fare" after the transformation and the target.



// ---------------------------------------------------

// GeometricWidthDiscretiser_plus_MeanEncoder.py
// discretisation/GeometricWidthDiscretiser_plus_MeanEncoder.py
# Generated from: GeometricWidthDiscretiser_plus_MeanEncoder.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # GeometricWidthDiscretiser + MeanEncoder
#
# This is very useful for linear models, because by using discretisation + a monotonic encoding, we create monotonic variables with the target, from those that before were not originally. And this tends to help improve the performance of the linear model. 


# ## GeometricWidthDiscretiser
#
# The GeometricWidthDiscretiser() divides continuous numerical variables into
# intervals of increasing width with equal increments. Note that the
# proportion of observations per interval may vary.
#
# The size of the interval will follow geometric progression.


# ## MeanEncoder
#
# This encoder replaces the labels by the target mean.
#
# <b>Note:</b> Check out the MeanEncoder notebook to learn more about this transformer.


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline

from feature_engine.discretisation import GeometricWidthDiscretiser
from feature_engine.encoding import MeanEncoder

plt.rcParams["figure.figsize"] = [15,5]


# Load titanic dataset from OpenML

def load_titanic(filepath='titanic.csv'):
    # data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    # data.drop(labels=['boat', 'body', 'home.dest', 'name', 'ticket'], axis=1, inplace=True)
    return data


# data = load_titanic("../data/titanic.csv")
data = load_titanic("../data/titanic-2/Titanic-Dataset.csv")
data.head()


# let's separate into training and testing set
X = data.drop(['survived'], axis=1)
y = data.survived

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

print("X_train :" ,X_train.shape)
print("X_test :" ,X_test.shape)


# we will use two continuous variables for the transformations
X_train[["age", 'fare']].hist(bins=30)
plt.show()


# set up the discretiser

efd = GeometricWidthDiscretiser(
    bins=5,
    variables=['age', 'fare'],
    # important: return values as categorical
    return_object=True)

# set up the encoder
woe = MeanEncoder(variables=['age', 'fare'])

# pipeline
transformer = Pipeline(
    steps=[
        ('GeometricWidthDiscretiser', efd),
        ('MeanEncoder', woe),
    ]
)

transformer.fit(X_train, y_train)


transformer.named_steps['GeometricWidthDiscretiser'].binner_dict_


transformer.named_steps['MeanEncoder'].encoder_dict_


train_t = transformer.transform(X_train)
test_t = transformer.transform(X_test)

test_t.head()


# let's explore the monotonic relationship
plt.figure(figsize=(7,5))
pd.concat([test_t,y_test], axis=1).groupby("fare")["survived"].mean().plot()
plt.title("Relationship between fare and target")
plt.xlabel("fare")
plt.ylabel("Mean of target")
plt.show()



// ---------------------------------------------------

// Model_Score_Discretisation.py
// discretisation/Model_Score_Discretisation.py
# Generated from: Model_Score_Discretisation.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer


# # Loading the Dataset


X, y = load_breast_cancer(return_X_y=True, as_frame=True)
X.head(3)


np.unique(y)


X.groupby(y).size()


# # Model Probability Discretization
#
# When we want to build a model to rank, we would like to know if the mean of our target variable increases with the model predicted probability. In order to check that, it is common to discretise the model probabilities that is provided by `model.predict_proba(X)[:, 1]`. If the mean target increases monotonically with each bin boundaries, than we can rest assure that our model is doing some sort of ranking.


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.60, random_state=50)


from sklearn.preprocessing import MinMaxScaler
from feature_engine.wrappers import SklearnTransformerWrapper
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

features = X.columns.tolist()

lr_model = Pipeline(steps=[
    ('scaler', SklearnTransformerWrapper(transformer=MinMaxScaler(), variables=features)),
    ('algorithm', LogisticRegression())
])


lr_model.fit(X_train, y_train)

y_proba_train = lr_model.predict_proba(X_train)[:,1]
y_proba_test = lr_model.predict_proba(X_test)[:,1]


from sklearn.metrics import roc_auc_score

print(f"Train ROCAUC: {roc_auc_score(y_train, y_proba_train):.4f}")
print(f"Test ROCAUC: {roc_auc_score(y_test, y_proba_test):.4f}")


# Our model is performing very good! But let's check if it is in fact assigning the examples with the greatest chance to acquire breast cancer with higher probabilities.
#
# To do that, let's use the `EqualFrequencyDiscretiser`.
#
# First, let's build a dataframe with the predicted probabilities and the target.


predictions_df = pd.DataFrame({'model_prob': y_proba_test,'target': y_test})
predictions_df.head()


from feature_engine.discretisation import EqualFrequencyDiscretiser

disc = EqualFrequencyDiscretiser(q=4, variables=['model_prob'], return_boundaries=True)
predictions_df_t = disc.fit_transform(predictions_df)
predictions_df_t.groupby('model_prob')['target'].mean().plot(kind='bar', rot=45);


# Indeed our model is ranking! But the last two groups/bins with greater probabilities are really close to each other. So, maybe after all we just have 3 groups instead of 4. Wouldn't it be nice if we could use a method that finds the optimum number of groups/bins for us? For this, `feature-engine` gotcha you! Let's use the `DecisionTreeDiscretiser`. As said by Soledad Gali [here](https://trainindata.medium.com/variable-discretization-in-machine-learning-7b09009915c2), this discretisation technique consists of using a decision tree to identify the optimal partitions for a continuous variable, that is what our model probability is.


from feature_engine.discretisation import DecisionTreeDiscretiser

disc = DecisionTreeDiscretiser(cv=3, scoring='roc_auc', variables=['model_prob'], regression=False)

predictions_df_t = disc.fit_transform(predictions_df, y_test)

predictions_df_t.groupby('model_prob')['target'].mean().plot(kind='bar');


# Very nice! Our `DecisionTreeDiscretiser` have found three optimum bins/groups/clusters to split the model probability. The first group only contains examples that our model says won't develop breast cancer. The second one has a 0.625 probability chance of develop a breast cancer and the third cluster has the greatest chance of develop breast cancer, 0.978.
#
# Let' check the size of each cluster:


predictions_df_t['model_prob'].value_counts().sort_index()


# It's a common practice to give letters to each cluster in a way that the letter 'A' for example will be used to denote the cluster with less probability:


import string

tree_predictions = np.sort(predictions_df_t['model_prob'].unique())

ratings_map = {tree_prediction: rating for rating, tree_prediction in zip(string.ascii_uppercase, tree_predictions)}
ratings_map


predictions_df_t['cluster'] = predictions_df_t['model_prob'].map(ratings_map)
predictions_df_t.head()


predictions_df_t.groupby('cluster')['target'].mean().plot(kind='bar', rot=0, title="Mean Target by Cluster");


# The same figure as the one above, but now with letters to denote each cluster.
#
# To finish, let's see what are the boundaries of each cluster. With that information, once we apply the model to obtain the probability of develop breast cancer for a new sample, we can classify it into one of the three cluster we created with the `DecisionTreeDiscretiser`.


predictions_df_t['model_probability'] = predictions_df['model_prob']
predictions_df_t.head()


predictions_df_t.groupby('cluster').agg(lower_boundary = ('model_probability', 'min'), upper_boundary=('model_probability', 'max')).round(3)


# So, if a new sample gets a probability of 0.72 it will be assigned to cluster C.


# # References
#
# To learn more about variable discretization tecniques, please go to https://trainindata.medium.com/variable-discretization-in-machine-learning-7b09009915c2.



// ---------------------------------------------------

// ArbitraryDiscretiser.py
// discretisation/ArbitraryDiscretiser.py
# Generated from: ArbitraryDiscretiser.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # ArbitraryDiscretiser
#
# The ArbitraryDiscretiser() divides continuous numerical variables into contiguous intervals are arbitrarily entered by the user.
#
# The user needs to enter a dictionary with variable names as keys, and a list of the limits of the intervals as values. For example {'var1': [0, 10, 100, 1000], 'var2': [5, 10, 15, 20]}.
#
# **Note**
#
# For this demonstration, we use the Ames House Prices dataset produced by Professor Dean De Cock:
#
# Dean De Cock (2011) Ames, Iowa: Alternative to the Boston Housing
# Data as an End of Semester Regression Project, Journal of Statistics Education, Vol.19, No. 3
#
# http://jse.amstat.org/v19n3/decock.pdf
#
# https://www.tandfonline.com/doi/abs/10.1080/10691898.2011.11889627
#
# The version of the dataset used in this notebook can be obtained from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from feature_engine.discretisation import ArbitraryDiscretiser
plt.rcParams["figure.figsize"] = [15,5]


# data = pd.read_csv('../data/housing.csv')   # ~ rename from train.csv
# data.head()

# # let's separate into training and testing set
# X = data.drop(["Id", "SalePrice"], axis=1)
# y = data.SalePrice

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# print("X_train :", X_train.shape)   # (1022, 79)
# print("X_test :", X_test.shape)     # (438, 79)


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# we will discretise two continuous variables

X_train[["LotArea", 'GrLivArea']].hist(bins=50)
plt.show()


# The ArbitraryDiscretiser() works only with numerical variables. The discretiser will
# check if the dictionary entered by the user contains variables present in the
# training set, and if these variables are cast as numerical, before doing any
# transformation.
#
# Then it transforms the variables, that is, it sorts the values into the intervals,
# transform.


'''
Parameters
----------

binning_dict : dict
    The dictionary with the variable : interval limits pairs, provided by the user.
    A valid dictionary looks like this:

     binning_dict = {'var1':[0, 10, 100, 1000], 'var2':[5, 10, 15, 20]}.

return_object : bool, default=False
    Whether the numbers in the discrete variable should be returned as
    numeric or as object. The decision is made by the user based on
    whether they would like to proceed the engineering of the variable as
    if it was numerical or categorical.

return_boundaries: bool, default=False
    whether the output should be the interval boundaries. If True, it returns
    the interval boundaries. If False, it returns integers.
'''

atd = ArbitraryDiscretiser(binning_dict={"LotArea":[-np.inf,4000,8000,12000,16000,20000,np.inf],
                                        "GrLivArea":[-np.inf,500,1000,1500,2000,2500,np.inf]})

atd.fit(X_train)


# binner_dict contains the boundaries of the different bins
atd.binner_dict_


train_t = atd.transform(X_train)
test_t = atd.transform(X_test)


# the below are the bins into which the observations were sorted
print(train_t['GrLivArea'].unique())
print(train_t['LotArea'].unique())


# here I put side by side the original variable and the transformed variable
tmp = pd.concat([X_train[["LotArea", 'GrLivArea']], train_t[["LotArea", 'GrLivArea']]], axis=1)
tmp.columns = ["LotArea", 'GrLivArea',"LotArea_binned", 'GrLivArea_binned']
tmp.head()


plt.subplot(1,2,1)
tmp.groupby('GrLivArea_binned')['GrLivArea'].count().plot.bar()
plt.ylabel('Number of houses')
plt.title('Number of observations per bin')
plt.subplot(1,2,2)
tmp.groupby('LotArea_binned')['LotArea'].count().plot.bar()
plt.ylabel('Number of houses')
plt.title('Number of observations per bin')

plt.show()


# ### Now return interval boundaries instead


atd = ArbitraryDiscretiser(binning_dict={"LotArea": [-np.inf, 4000, 8000, 12000, 16000, 20000, np.inf],
                                         "GrLivArea": [-np.inf, 500, 1000, 1500, 2000, 2500, np.inf]},
                           # to return the boundary limits
                           return_boundaries=True)

atd.fit(X_train)


train_t = atd.transform(X_train)
test_t = atd.transform(X_test)


# the numbers are the different bins into which the observations
# were sorted
np.sort(np.ravel(train_t['GrLivArea'].unique()))


np.sort(np.ravel(test_t['GrLivArea'].unique()))


# bar plot to show the intervals returned by the transformer
test_t.LotArea.value_counts(sort=False).plot.bar(figsize=(6,4))
plt.ylabel('Number of houses')
plt.title('Number of houses per interval')
plt.show()



// ---------------------------------------------------

// DecisionTreeDiscretiser.py
// discretisation/DecisionTreeDiscretiser.py
# Generated from: DecisionTreeDiscretiser.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # DecisionTreeDiscretiser
#
# The DecisionTreeDiscretiser() divides continuous numerical variables into discrete, finite, values estimated by a decision tree.
#
# The methods is inspired by the following article from the winners of the KDD 2009 competition:
# http://www.mtome.com/Publications/CiML/CiML-v3-book.pdf
#
# **Note**
#
# For this demonstration, we use the Ames House Prices dataset produced by Professor Dean De Cock:
#
# Dean De Cock (2011) Ames, Iowa: Alternative to the Boston Housing
# Data as an End of Semester Regression Project, Journal of Statistics Education, Vol.19, No. 3
#
# http://jse.amstat.org/v19n3/decock.pdf
#
# https://www.tandfonline.com/doi/abs/10.1080/10691898.2011.11889627
#
# The version of the dataset used in this notebook can be obtained from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from feature_engine.discretisation import DecisionTreeDiscretiser

plt.rcParams["figure.figsize"] = [15,5]


# ## DecisionTreeDiscretiser with Regression


# data = pd.read_csv('../data/housing.csv')
# data.head()

# # let's separate into training and testing set
# X = data.drop(["Id", "SalePrice"], axis=1)
# y = data.SalePrice

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# print("X_train :", X_train.shape)
# print("X_test :", X_test.shape)


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# we will discretise two continuous variables

X_train[["LotArea", 'GrLivArea']].hist(bins=50)
plt.show()


# The DecisionTreeDiscretiser() works only with numerical variables.
# A list of variables can be passed as an argument. Alternatively, the
# discretiser will automatically select and transform all numerical variables.
#
# The DecisionTreeDiscretiser() first trains a decision tree for each variable,
# fit.
#
# The DecisionTreeDiscretiser() then transforms the variables, that is,
# makes predictions based on the variable values, using the trained decision
# tree, transform.


'''
Parameters
----------

cv : int, default=3
    Desired number of cross-validation fold to be used to fit the decision
    tree.

scoring: str, default='neg_mean_squared_error'
    Desired metric to optimise the performance for the tree. Comes from
    sklearn metrics. See DecisionTreeRegressor or DecisionTreeClassifier
    model evaluation documentation for more options:
    https://scikit-learn.org/stable/modules/model_evaluation.html

variables : list
    The list of numerical variables that will be transformed. If None, the
    discretiser will automatically select all numerical type variables.

regression : boolean, default=True
    Indicates whether the discretiser should train a regression or a classification
    decision tree.

param_grid : dictionary, default=None
    The list of parameters over which the decision tree should be optimised
    during the grid search. The param_grid can contain any of the permitted
    parameters for Scikit-learn's DecisionTreeRegressor() or
    DecisionTreeClassifier().

    If None, then param_grid = {'max_depth': [1, 2, 3, 4]}

random_state : int, default=None
    The random_state to initialise the training of the decision tree. It is one
    of the parameters of the Scikit-learn's DecisionTreeRegressor() or
    DecisionTreeClassifier(). For reproducibility it is recommended to set
    the random_state to an integer.
'''

treeDisc = DecisionTreeDiscretiser(cv=3,
                                   scoring='neg_mean_squared_error',
                                   variables=['LotArea', 'GrLivArea'],
                                   regression=True,
                                   random_state=29)

# the DecisionTreeDiscretiser needs the target for fitting
treeDisc.fit(X_train, y_train)


# the binner_dict_ contains the best decision tree for each variable
treeDisc.binner_dict_


train_t = treeDisc.transform(X_train)
test_t = treeDisc.transform(X_test)


# the below account for the best obtained bins, aka, the tree predictions

train_t['GrLivArea'].unique()


# the below account for the best obtained bins, aka, the tree predictions

train_t['LotArea'].unique()


# here I put side by side the original variable and the transformed variable

tmp = pd.concat([X_train[["LotArea", 'GrLivArea']],
                 train_t[["LotArea", 'GrLivArea']]], axis=1)

tmp.columns = ["LotArea", 'GrLivArea', "LotArea_binned", 'GrLivArea_binned']

tmp.head()


# in  equal frequency discretisation, we obtain the same amount of observations
# in each one of the bins.

plt.subplot(1,2,1)
tmp.groupby('GrLivArea_binned')['GrLivArea'].count().plot.bar()
plt.ylabel('Number of houses')
plt.title('Number of houses per discrete value')

plt.subplot(1,2,2)
tmp.groupby('LotArea_binned')['LotArea'].count().plot.bar()
plt.ylabel('Number of houses')
plt.ylabel('Number of houses')

plt.show()


# ## DecisionTreeDiscretiser with binary classification


# Load titanic dataset from OpenML

def load_titanic(filepath='titanic.csv'):
    # data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    # data.drop(labels=['boat', 'body', 'home.dest', 'name', 'ticket'], axis=1, inplace=True)
    return data


# data = load_titanic("../data/titanic.csv")
data = load_titanic("../data/titanic-2/Titanic-Dataset.csv")
data.head()


# let's separate into training and testing set

X_train, X_test, y_train, y_test = train_test_split(data.drop(['survived'], axis=1),
                                                    data['survived'],
                                                    test_size=0.3, 
                                                    random_state=0)

print(X_train.shape)
print(X_test.shape)


#this discretiser transforms the numerical variables
X_train[['fare', 'age']].dtypes


treeDisc = DecisionTreeDiscretiser(cv=3,
                                   scoring='roc_auc',
                                   variables=['fare', 'age'],
                                   regression=False,
                                   param_grid={'max_depth': [1, 2]},
                                   random_state=29,
                                   )

treeDisc.fit(X_train, y_train)


treeDisc.binner_dict_


train_t = treeDisc.transform(X_train)
test_t = treeDisc.transform(X_test)


# the below account for the best obtained bins
# in this case, the tree has found that dividing the data in 6 bins is enough
train_t['age'].unique()


# the below account for the best obtained bins
# in this case, the tree has found that dividing the data in 8 bins is enough
train_t['fare'].unique()


# here I put side by side the original variable and the transformed variable

tmp = pd.concat([X_train[["fare", 'age']], train_t[["fare", 'age']]], axis=1)

tmp.columns = ["fare", 'age', "fare_binned", 'age_binned']

tmp.head()


plt.subplot(1,2,1)
tmp.groupby('fare_binned')['fare'].count().plot.bar()
plt.ylabel('Number of houses')
plt.title('Number of houses per discrete value')

plt.subplot(1,2,2)
tmp.groupby('age_binned')['age'].count().plot.bar()
plt.ylabel('Number of houses')
plt.title('Number of houses per discrete value')

plt.show()


# The DecisionTreeDiscretiser() returns values which show
# a monotonic relationship with target

pd.concat([test_t, y_test], axis=1).groupby(
    'age')['survived'].mean().plot(figsize=(6, 4))

plt.ylabel("Mean of target")
plt.title("Relationship between fare and target")
plt.show()


# The DecisionTreeDiscretiser() returns values which show
# a monotonic relationship with target

pd.concat([test_t, y_test], axis=1).groupby(
    'fare')['survived'].mean().plot(figsize=(6, 4))

plt.ylabel("Mean of target")
plt.title("Relationship between fare and target")
plt.show()


# ## DecisionTreeDiscretiser for Multi-class classification


# Load iris dataset from sklearn
from sklearn.datasets import load_iris

data = pd.DataFrame(load_iris().data, 
                    columns=load_iris().feature_names).join(
    pd.Series(load_iris().target, name='type'))

data.head()


data.type.unique() # 3 - class classification


# let's separate into training and testing set

X_train, X_test, y_train, y_test = train_test_split(data.drop('type', axis=1),
                                                    data['type'],
                                                    test_size=0.3,
                                                    random_state=0)

print(X_train.shape)
print(X_test.shape)


#selected two numerical variables
X_train[['sepal length (cm)', 'sepal width (cm)']].dtypes


treeDisc = DecisionTreeDiscretiser(cv=3,
                                   scoring='accuracy',
                                   variables=[
                                       'sepal length (cm)', 'sepal width (cm)'],
                                   regression=False,
                                   random_state=29,
                                   )

treeDisc.fit(X_train, y_train)


treeDisc.binner_dict_


train_t = treeDisc.transform(X_train)
test_t = treeDisc.transform(X_test)


# here I put side by side the original variable and the transformed variable
tmp = pd.concat([X_train[['sepal length (cm)', 'sepal width (cm)']],
                 train_t[['sepal length (cm)', 'sepal width (cm)']]], axis=1)

tmp.columns = ['sepal length (cm)', 'sepal width (cm)',
               'sepalLen_binned', 'sepalWid_binned']

tmp.head()


plt.subplot(1, 2, 1)
tmp.groupby('sepalLen_binned')['sepal length (cm)'].count().plot.bar()
plt.ylabel('Number of species')
plt.title('Number of observations per discrete value')

plt.subplot(1, 2, 2)
tmp.groupby('sepalWid_binned')['sepal width (cm)'].count().plot.bar()
plt.ylabel('Number of species')
plt.title('Number of observations per discrete value')

plt.show()



// ---------------------------------------------------

// GeometricWidthDiscretiser.py
// discretisation/GeometricWidthDiscretiser.py
# Generated from: GeometricWidthDiscretiser.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from feature_engine.discretisation import GeometricWidthDiscretiser


data = pd.read_csv('../data/housing.csv')   # ~ rename from train.csv
# data.head()

# # let's separate into training and testing set
# X = data.drop(["Id", "SalePrice"], axis=1)
# y = data.SalePrice

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# print("X_train :", X_train.shape)   # (1022, 79)
# print("X_test :", X_test.shape)     # (438, 79)


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# set up the discretisation transformer
disc = GeometricWidthDiscretiser(bins=10, variables=['LotArea', 'GrLivArea'])

# fit the transformer
disc.fit(X_train)


# transform the data
train_t= disc.transform(X_train)
test_t= disc.transform(X_test)


disc.binner_dict_


fig, ax = plt.subplots(1, 2)
X_train['LotArea'].hist(ax=ax[0], bins=10);
train_t['LotArea'].hist(ax=ax[1], bins=10);



// ---------------------------------------------------

// EqualFrequencyDiscretiser.py
// discretisation/EqualFrequencyDiscretiser.py
# Generated from: EqualFrequencyDiscretiser.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # EqualFrequencyDiscretiser
#
# The EqualFrequencyDiscretiser() divides continuous numerical variables
# into contiguous equal frequency intervals, that is, intervals that contain
# approximately the same proportion of observations.
#
# The interval limits are determined by the quantiles. The number of intervals,
# i.e., the number of quantiles in which the variable should be divided is
# determined by the user.
#
# **Note**
#
# For this demonstration, we use the Ames House Prices dataset produced by Professor Dean De Cock:
#
# Dean De Cock (2011) Ames, Iowa: Alternative to the Boston Housing
# Data as an End of Semester Regression Project, Journal of Statistics Education, Vol.19, No. 3
#
# http://jse.amstat.org/v19n3/decock.pdf
#
# https://www.tandfonline.com/doi/abs/10.1080/10691898.2011.11889627
#
# The version of the dataset used in this notebook can be obtained from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from feature_engine.discretisation import EqualFrequencyDiscretiser

plt.rcParams["figure.figsize"] = [15,5]


# data = pd.read_csv('../data/housing.csv')
# data.head()

# # let's separate into training and testing set
# X = data.drop(["Id", "SalePrice"], axis=1)
# y = data.SalePrice

# X_train, X_test, y_train, y_test = train_test_split(
#     X, y, test_size=0.3, random_state=0)

# print("X_train :", X_train.shape)
# print("X_test :", X_test.shape)


# Read the separate files
train_df = pd.read_csv('../data/house-prices/train.csv')
test_df = pd.read_csv('../data/house-prices/test.csv')

# Separate features and target in training data
X_train = train_df.drop(['Id', 'SalePrice'], axis=1)
y_train = train_df['SalePrice']

# For test data, you might not have the target variable
X_test = test_df.drop(['Id'], axis=1)  # Note: test data might not have SalePrice column

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# we will use two continuous variables for transformation

X_train[["LotArea", 'GrLivArea']].hist(bins=50)

plt.show()


# The EqualFrequencyDiscretiser() works only with numerical variables.
# A list of variables can be passed as argument. Alternatively, the discretiser
# will automatically select and transform all numerical variables.
#
# The EqualFrequencyDiscretiser() first finds the boundaries for the intervals or
# quantiles for each variable, fit.
#
# Then it transforms the variables, that is, it sorts the values into the intervals,
# transform.


'''
Parameters
----------

q : int, default=10
    Desired number of equal frequency intervals / bins. In other words the
    number of quantiles in which the variables should be divided.

variables : list
    The list of numerical variables that will be discretised. If None, the
    EqualFrequencyDiscretiser() will select all numerical variables.

return_object : bool, default=False
    Whether the numbers in the discrete variable should be returned as
    numeric or as object. The decision is made by the user based on
    whether they would like to proceed the engineering of the variable as
    if it was numerical or categorical.

return_boundaries: bool, default=False
    whether the output should be the interval boundaries. If True, it returns
    the interval boundaries. If False, it returns integers.
'''

efd = EqualFrequencyDiscretiser(q=10, variables=['LotArea', 'GrLivArea'])

efd.fit(X_train)


# binner_dict contains the boundaries of the different bins
efd.binner_dict_


train_t = efd.transform(X_train)
test_t = efd.transform(X_test)


# the numbers are the different bins into which the observations
# were sorted
train_t['GrLivArea'].unique()


# the numbers are the different bins into which the observations
# were sorted
train_t['LotArea'].unique()


# here I put side by side the original variable and the transformed variable
tmp = pd.concat([X_train[["LotArea", 'GrLivArea']], train_t[["LotArea", 'GrLivArea']]], axis=1)
tmp.columns = ["LotArea", 'GrLivArea',"LotArea_binned", 'GrLivArea_binned']
tmp.head()


# in  equal frequency discretisation, we obtain the same amount of observations
# in each one of the bins.
plt.subplot(1,2,1)
tmp.groupby('GrLivArea_binned')['GrLivArea'].count().plot.bar()
plt.ylabel('Number of houses')
plt.title('Number of observations per interval')

plt.subplot(1,2,2)
tmp.groupby('LotArea_binned')['LotArea'].count().plot.bar()
plt.ylabel('Number of houses')
plt.title('Number of observations per interval')

plt.show()


# ### Return interval limits instead


# Now, let's return bin boundaries instead

efd = EqualFrequencyDiscretiser(
    q=10, variables=['LotArea', 'GrLivArea'], return_boundaries=True)

efd.fit(X_train)


train_t = efd.transform(X_train)
test_t = efd.transform(X_test)


# the numbers are the different bins into which the observations
# were sorted
np.sort(np.ravel(train_t['GrLivArea'].unique()))


np.sort(np.ravel(test_t['GrLivArea'].unique()))



// ---------------------------------------------------

// EqualFrequencyDiscretiser_plus_WoEEncoder.py
// discretisation/EqualFrequencyDiscretiser_plus_WoEEncoder.py
# Generated from: EqualFrequencyDiscretiser_plus_WoEEncoder.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # EqualFrequencyDiscretiser + WoEEncoder
#
# This is very useful for linear models, because by using discretisation + a monotonic encoding, we create monotonic variables with the target, from those that before were not originally. And this tends to help improve the performance of the linear model. 


# ## EqualFrequencyDiscretiser
#
# The EqualFrequencyDiscretiser() divides continuous numerical variables
# into contiguous equal frequency intervals, that is, intervals that contain
# approximately the same proportion of observations.
#
# The interval limits are determined by the quantiles. The number of intervals,
# i.e., the number of quantiles in which the variable should be divided is
# determined by the user.
#
# <b>Note</b>: Check out the EqualFrequencyDiscretiser notebook to larn more about this transformer.


# ## WoEEncoder
#
# This encoder replaces the labels by the weight of evidence.
#
# **It only works for binary classification.**
#
# <b>Note:</b> Check out the WoEEncoder notebook to learn more about this transformer.


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline

from feature_engine.discretisation import EqualFrequencyDiscretiser
from feature_engine.encoding import WoEEncoder

plt.rcParams["figure.figsize"] = [15,5]


# Load titanic dataset from OpenML

def load_titanic(filepath='titanic.csv'):
    # data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    # data.drop(labels=['boat', 'body', 'home.dest', 'name', 'ticket'], axis=1, inplace=True)
    return data


# data = load_titanic("../data/titanic.csv")
data = load_titanic("../data/titanic-2/Titanic-Dataset.csv")
data.head()


# let's separate into training and testing set
X = data.drop(['survived'], axis=1)
y = data.survived

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

print("X_train :" ,X_train.shape)
print("X_test :" ,X_test.shape)


# we will use two continuous variables for the transformations
X_train[["age", 'fare']].hist(bins=30)
plt.show()


# set up the discretiser

efd = EqualFrequencyDiscretiser(
    q=4,
    variables=['age', 'fare'],
    # important: return values as categorical
    return_object=True)

# set up the encoder
woe = WoEEncoder(variables=['age', 'fare'])

# pipeline
transformer = Pipeline(steps=[('EqualFrequencyDiscretiser', efd),
                              ('WoEEncoder', woe),
                              ])

transformer.fit(X_train, y_train)


transformer.named_steps['EqualFrequencyDiscretiser'].binner_dict_


transformer.named_steps['WoEEncoder'].encoder_dict_


train_t = transformer.transform(X_train)
test_t = transformer.transform(X_test)

test_t.head()


# let's explore the monotonic relationship
plt.figure(figsize=(7,5))
pd.concat([test_t,y_test], axis=1).groupby("fare")["survived"].mean().plot()
plt.title("Relationship between fare and target")
plt.xlabel("fare")
plt.ylabel("Mean of target")
plt.show()



// ---------------------------------------------------

// EqualWidthDiscretiser_plus_OrdinalEncoder.py
// discretisation/EqualWidthDiscretiser_plus_OrdinalEncoder.py
# Generated from: EqualWidthDiscretiser_plus_OrdinalEncoder.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # EqualWidthDiscretiser + OrdinalEncoder
#
#
# This is very useful for linear models, because by using discretisation + a monotonic encoding, we create monotonic variables with the target, from those that before were not originally. And this tends to help improve the performance of the linear model. 


# ## EqualWidthDiscretiser
#
# The EqualWidthDiscretiser() divides continuous numerical variables into
# intervals of the same width, that is, equidistant intervals. Note that the
# proportion of observations per interval may vary.
#
# The number of intervals
# in which the variable should be divided must be indicated by the user.
#
# <b>Note:</b> Check out the EqualWidthDiscretiser notebook to learn more about this transformer.


# ## OrdinalEncoder
# The OrdinalEncoder() will replace the variable labels by digits, from 1 to the number of different labels. 
#
# If we select "arbitrary", then the encoder will assign numbers as the labels appear in the variable (first come first served).
#
# If we select "ordered", the encoder will assign numbers following the mean of the target value for that label. So labels for which the mean of the target is higher will get the number 1, and those where the mean of the target is smallest will get the number n.
#
# <b>Note:</b> Check out the OrdinalEncoder notebook to know more about this transformer.


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline


from feature_engine.discretisation import EqualWidthDiscretiser
from feature_engine.encoding import OrdinalEncoder

plt.rcParams["figure.figsize"] = [15,5]


# Load titanic dataset from OpenML

def load_titanic(filepath='titanic.csv'):
    # data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
    data = pd.read_csv(filepath)
    data = data.replace('?', np.nan)
    data['cabin'] = data['cabin'].astype(str).str[0]
    data['pclass'] = data['pclass'].astype('O')
    data['age'] = data['age'].astype('float').fillna(data.age.median())
    data['fare'] = data['fare'].astype('float').fillna(data.fare.median())
    data['embarked'].fillna('C', inplace=True)
    # data.drop(labels=['boat', 'body', 'home.dest', 'name', 'ticket'], axis=1, inplace=True)
    return data


# data = load_titanic("../data/titanic.csv")
data = load_titanic("../data/titanic-2/Titanic-Dataset.csv")
data.head()


# let's separate into training and testing set
X = data.drop(['survived'], axis=1)
y = data.survived

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)

print("X_train :", X_train.shape)
print("X_test :", X_test.shape)


# we will use two continuous variables for the transformations

X_train[["age", 'fare']].hist(bins=30)
plt.show()


# set up the discretiser
ewd = EqualWidthDiscretiser(
    bins=5,
    variables=['age', 'fare'],
    # important: return values as categorical
    return_object=True)

# set up the encoder
oe = OrdinalEncoder(variables=['age', 'fare'])

# pipeline
transformer = Pipeline(steps=[('EqualWidthDiscretiser', ewd),
                              ('OrdinalEncoder', oe),
                              ])

transformer.fit(X_train, y_train)


transformer.named_steps['EqualWidthDiscretiser'].binner_dict_


transformer.named_steps['OrdinalEncoder'].encoder_dict_


train_t = transformer.transform(X_train)
test_t = transformer.transform(X_test)

test_t.head()


# let's explore the monotonic relationship
plt.figure(figsize=(7,5))
pd.concat([test_t,y_test], axis=1).groupby("fare")["survived"].mean().plot()
plt.title("Relationship between fare and target")
plt.xlabel("fare")
plt.ylabel("Mean of target")
plt.show()


# Note how the bins are monotonically ordered with the target.



// ---------------------------------------------------

